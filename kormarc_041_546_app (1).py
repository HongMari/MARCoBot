# ============================================================
# Part 1 â€” Imports / Global Settings / GPT Master Setup
# GPT-4o ë‹¨ 1íšŒ í˜¸ì¶œ ê¸°ë°˜ MARC ìë™ ìƒì„±ê¸° (Refactored)
# ============================================================

import re
import os
import io
import json
import math
import html
import urllib
import datetime
from dataclasses import dataclass
from typing import Optional, Dict, Any, List

import pandas as pd
import requests
from bs4 import BeautifulSoup
from pymarc import Record, Field, Subfield, MARCWriter
import streamlit as st
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# ------------------------------------------------------------
# ê¸°ì¡´ ì½”ë“œ ì „ì²´ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê³µí†µ ìƒìˆ˜ ìœ ì§€
# ------------------------------------------------------------
ALADIN_SEARCH_URL = "https://www.aladin.co.kr/search/wsearchresult.aspx"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# ì‚¬ìš©ì secrets ê¸°ë°˜ í‚¤
try:
    OPENAI_API_KEY = st.secrets["openai"]["api_key"]
    OPENAI_MODEL = "gpt-4o"       # ìš”ì²­ëŒ€ë¡œ gpt-4o ê³ ì •
    ALADIN_TTB_KEY = st.secrets["aladin"]["ttbkey"]
    NLK_CERT_KEY = st.secrets["nlk"]["cert_key"]
except Exception:
    OPENAI_API_KEY = ""
    OPENAI_MODEL = "gpt-4o"
    ALADIN_TTB_KEY = ""
    NLK_CERT_KEY = ""

# ------------------------------------------------------------
# GPT ë‹¨ì¼ í˜¸ì¶œìš© API endpoint
# ------------------------------------------------------------
OPENAI_CHAT_COMPLETIONS = "https://api.openai.com/v1/chat/completions"


# ------------------------------------------------------------
# Debug collector
# ------------------------------------------------------------
CURRENT_DEBUG_LINES: List[str] = []

def dbg(*args):
    CURRENT_DEBUG_LINES.append(" ".join(str(a) for a in args))

def dbg_err(*args):
    CURRENT_DEBUG_LINES.append("âŒ " + " ".join(str(a) for a in args))


# ------------------------------------------------------------
# Utility â€” HTML/text cleaner
# ------------------------------------------------------------
def clean_text(s: Optional[str]) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


# ------------------------------------------------------------
# Utility â€” ISBN normalizer
# ------------------------------------------------------------
def normalize_isbn(isbn: str) -> str:
    if not isbn:
        return ""
    return re.sub(r"[^0-9Xx]", "", isbn).strip()


# ------------------------------------------------------------
# Utility â€” Aladin ê¸°ë³¸ ItemLookUp (GPT master inputìš©)
# ------------------------------------------------------------
def fetch_aladin_item_raw(isbn: str) -> dict:
    """
    ê¸°ì¡´ fetch_aladin_item + aladin_lookup_by_api ì—­í•  ì¤‘
    GPTì—ê²Œ ë„˜ê¸¸ ë°ì´í„°ë§Œ ìµœì†Œí•œ ì¶”ì¶œ.
    """
    try:
        params = {
            "ttbkey": ALADIN_TTB_KEY,
            "itemIdType": "ISBN",
            "ItemId": isbn,
            "output": "js",
            "Version": "20131101",
            "OptResult": "authors,categoryName,description,fulldescription,toc,seriesInfo,subInfo"
        }
        r = requests.get("https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx",
                         params=params, headers=HEADERS, timeout=15)
        r.raise_for_status()
        data = r.json()
        items = data.get("item", [])
        return items[0] if items else {}
    except Exception as e:
        dbg_err(f"[Aladin raw fail] {e}")
        return {}


# ------------------------------------------------------------
# GPT Master ì…ë ¥ ìƒì„±
# ------------------------------------------------------------
def build_gpt_master_payload(isbn: str, aladin_item: dict) -> dict:
    """
    GPT-4o ë‹¨ì¼ í˜¸ì¶œ ì…ë ¥ JSON.  
    ì´ ì•ˆì—ì„œ GPTê°€ 041/546/653/056/940 í›„ë³´ë¥¼ í•œ ë²ˆì— ë„ì¶œí•œë‹¤.
    """
    title = clean_text(aladin_item.get("title", ""))
    author = clean_text(aladin_item.get("author", ""))
    category = clean_text(aladin_item.get("categoryName", ""))
    desc = clean_text(
        aladin_item.get("fulldescription")
        or aladin_item.get("description")
        or ""
    )
    toc = clean_text((aladin_item.get("subInfo") or {}).get("toc", "") or "")

    return {
        "isbn": isbn,
        "title": title,
        "author": author,
        "category": category,
        "description": desc,
        "toc": toc,
    }


# ------------------------------------------------------------
# GPT Master í˜¸ì¶œ
# ------------------------------------------------------------
def call_gpt_master(payload: dict) -> dict:
    """
    GPT-4o í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ì—¬  
    041, 546, 653, 056, 940 ì •ë³´ë¥¼ ëª¨ë‘ ë°›ì•„ì˜¨ë‹¤.
    """
    sys_msg = {
        "role": "system",
        "content": (
            "ë„ˆëŠ” í•œêµ­ KORMARCÂ·KDC ë©”íƒ€ë°ì´í„° ìƒì„± ì „ë¬¸ê°€ì´ë‹¤.\n"
            "ì…ë ¥ëœ ë„ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì„ JSONìœ¼ë¡œ ë§Œë“¤ì–´ë¼.\n\n"
            "í•„ìˆ˜ ì¶œë ¥:\n"
            "1) marc041: KORMARC 041 ì „ì²´ ë¬¸ìì—´(ex '$akor$heng')\n"
            "2) marc546: 546 ì–¸ì–´ì£¼ê¸° ë¬¸ì¥(ex 'ì˜ì–´ ì›ì‘ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­')\n"
            "3) keywords_653: ììœ ì£¼ì œì–´ ë°°ì—´, ìµœëŒ€ 7ê°œ, ëª¨ë‘ ë„ì–´ì“°ê¸° ì—†ëŠ” ëª…ì‚¬í˜•\n"
            "4) kdc_056: KDC 3ìë¦¬ ìˆ«ì(ex '813') ë˜ëŠ” 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'\n"
            "5) title940: 940 ìƒì„±ì„ ìœ„í•œ Title A (245$a ê¸°ë°˜)\n\n"
            "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ ë„£ê³ , ë‹¤ë¥¸ ë¬¸ì¥ì€ ì“°ì§€ ë§ ê²ƒ."
        )
    }

    user_msg = {
        "role": "user",
        "content": json.dumps(payload, ensure_ascii=False)
    }

    try:
        r = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json",
            },
            json={
                "model": OPENAI_MODEL,
                "messages": [sys_msg, user_msg],
                "temperature": 0.2,
                "max_tokens": 600,
            },
            timeout=40,
        )
        r.raise_for_status()
        txt = r.json()["choices"][0]["message"]["content"]
        out = json.loads(txt)
        dbg("[GPT master] success")
        return out

    except Exception as e:
        dbg_err(f"[GPT master fail] {e}")
        return {
            "marc041": None,
            "marc546": None,
            "keywords_653": [],
            "kdc_056": None,
            "title940": None,
        }
# ============================================================
# Part 2 â€” GPT Master í›„ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ============================================================

# ------------------------------------------------------------
# 041 / 546 í›„ì²˜ë¦¬
# ------------------------------------------------------------

def make_041(marc041_raw: Optional[str]) -> Optional[str]:
    """
    GPTê°€ ì¤€ 'marc041' ê°’("$akor$heng")ì„ ì‹¤ì œ 041 íƒœê·¸ MRKë¡œ ë³€í™˜.
    """
    if not marc041_raw:
        return None

    body = marc041_raw.strip()
    if not body.startswith("$"):
        body = "$" + body

    return f"=041  0\\{body}"


def make_546(marc546_text: Optional[str]) -> Optional[str]:
    """
    GPTê°€ ì¤€ ìì—°ì–´ ì„¤ëª…(ì˜ˆ: 'ì˜ì–´ ì›ì‘ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­')ì„
    546 íƒœê·¸ MRKë¡œ ë³€í™˜.
    """
    if not marc546_text:
        return None

    txt = marc546_text.strip()
    return f"=546  \\\\$a{txt}"


# ------------------------------------------------------------
# 653 í›„ì²˜ë¦¬
# ------------------------------------------------------------

def make_653(keywords: Optional[List[str]]) -> Optional[str]:
    """
    GPTê°€ ì¤€ keywords_653 ë°°ì—´ â†’ "=653  \\$aí‚¤ì›Œë“œ1$aí‚¤ì›Œë“œ2..." í˜•íƒœë¡œ ë³€í™˜.
    """
    if not keywords:
        return None

    # ì¤‘ë³µ ì œê±° + ìµœëŒ€ 7ê°œ
    out = []
    seen = set()
    for kw in keywords:
        if not kw:
            continue
        kw = kw.strip().replace(" ", "")
        if kw and kw not in seen:
            seen.add(kw)
            out.append(kw)
        if len(out) >= 7:
            break

    if not out:
        return None

    parts = "".join(f"$a{kw}" for kw in out)
    return f"=653  \\\\{parts}"


# ------------------------------------------------------------
# 056 (KDC) í›„ì²˜ë¦¬
# ------------------------------------------------------------

def make_056(kdc_code: Optional[str]) -> Optional[str]:
    """
    GPTê°€ ì¤€ kdc_056: '813', '325', ë˜ëŠ” 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'
    â†’ 056 íƒœê·¸ í˜•íƒœë¡œ ë³€í™˜.
    """
    if not kdc_code:
        return None

    s = kdc_code.strip()
    if s == "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ":
        # ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ë˜, 056 íƒœê·¸ëŠ” ìƒì„± ì•ˆ í•¨ (ë„¤ ì›ë³¸ ì •ì±… ì¡´ì¤‘)
        return None

    # ìˆ«ìë§Œ í—ˆìš©
    if not re.fullmatch(r"\d{1,3}", s):
        return None

    return f"=056  \\\\$a{s}$26"   # KDC6 ê¸°ì¤€


# ------------------------------------------------------------
# 940 í›„ì²˜ë¦¬
# ------------------------------------------------------------

def make_940(title_a: Optional[str]) -> List[str]:
    """
    GPTê°€ ì¤€ title940(=245$a ê¸°ë°˜ Title A)
    â†’ 940 MRK ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜.
    """
    if not title_a:
        return []

    ta = title_a.strip()
    if not ta:
        return []

    # ë‹¨ì¼ 940ë§Œ ìƒì„±
    return [f"=940  \\\\$a{ta}"]


# ------------------------------------------------------------
# ê³µí†µ: mrk â†’ Field ë³€í™˜ê¸° (ê¸°ì¡´ í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
# ------------------------------------------------------------

def mrk_str_to_field(line):
    """
    ì´ë¯¸ ë„¤ ì›ë³¸ ì½”ë“œì— ìˆë˜ ê·¸ëŒ€ë¡œ ë³µë¶™.
    (ì—¬ê¸°ì„œëŠ” í•µì‹¬ ë¡œì§ë§Œ ê·¸ëŒ€ë¡œ ìœ ì§€)
    """
    if line is None:
        return None

    try:
        if getattr(line, "tag", None) is not None and \
           (hasattr(line, "data") or hasattr(line, "subfields")):
            return line
    except Exception:
        pass

    if not isinstance(line, str):
        try:
            line = str(line)
        except Exception:
            return None

    s = line.strip()
    if not s.startswith("=") or len(s) < 8:
        return None

    # --- íƒœê·¸/ì¸ë””/ë³¸ë¬¸ íŒŒì‹± ---
    m = re.match(r"^=(\d{3})\s{2}(.)(.)(.*)$", s)
    if m:
        tag, ind1_raw, ind2_raw, tail = m.groups()
    else:
        mctl = re.match(r"^=(\d{3})\s\s(.*)$", s)
        if not mctl:
            return None
        tag, data = mctl.group(1), mctl.group(2).strip()
        if tag.isdigit() and int(tag) < 10:
            return Field(tag=tag, data=data) if data else None
        return None

    # ì»¨íŠ¸ë¡¤ í•„ë“œ
    if tag.isdigit() and int(tag) < 10:
        data = (ind1_raw + ind2_raw + tail).strip()
        return Field(tag=tag, data=data) if data else None

    ind1 = " " if ind1_raw == "\\" else ind1_raw
    ind2 = " " if ind2_raw == "\\" else ind2_raw

    subs_part = tail or ""
    if "$" not in subs_part:
        return None

    subfields = []
    i, L = 0, len(subs_part)
    while i < L:
        if subs_part[i] != "$":
            i += 1
            continue
        if i + 1 >= L:
            break
        code = subs_part[i + 1]
        j = i + 2
        while j < L and subs_part[j] != "$":
            j += 1
        value = subs_part[i + 2:j].strip()
        if code and value:
            subfields.append(Subfield(code, value))
        i = j

    if not subfields:
        return None

    return Field(tag=tag, indicators=[ind1, ind2], subfields=subfields)


# ------------------------------------------------------------
# ê³µí†µ 260 ë¹Œë” (ê¸°ì¡´ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©)
# ------------------------------------------------------------

def build_260(place_display: str, publisher_name: str, pubyear: str):
    place = (place_display or "ë°œí–‰ì§€ ë¯¸ìƒ")
    pub = (publisher_name or "ë°œí–‰ì²˜ ë¯¸ìƒ")
    year = (pubyear or "ë°œí–‰ë…„ ë¯¸ìƒ")
    return f"=260  \\\\$a{place} :$b{pub},$c{year}"


# ------------------------------------------------------------
# ê³µí†µ 300 ë¹Œë” (ê¸°ì¡´ í•¨ìˆ˜ build_300_from_aladin_detail ì‚¬ìš©)
# ------------------------------------------------------------

# ê¸°ì¡´ build_300_from_aladin_detail(item)ë¥¼ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©.
# ì´ í•¨ìˆ˜ëŠ” Part 3ì— ë‹¤ì‹œ ë“±ì¥í•  ì˜ˆì •ì´ë©°,
# GPT master êµ¬ì¡°ì™€ ì¶©ëŒí•˜ì§€ ì•ŠìŒ.

# ============================================================
# Part 3 â€” 245 / 246 / 700 / 90010 / 049 ë¹Œë”
# GPT master ì´í›„ êµ¬ì¡°ì™€ í˜¸í™˜ë˜ëŠ” ì¬ì‘ì„± ë²„ì „
# ============================================================

# ------------------------------------------------------------
# 245 êµ¬ì„± (ì•Œë¼ë”˜ ê¸°ë°˜)
# ------------------------------------------------------------

def parse_aladin_title_and_subtitle(item: dict) -> tuple[str, str]:
    """
    ì•Œë¼ë”˜ itemì—ì„œ title / subInfo.subTitle ë¶„ë¦¬
    """
    title = clean_text(item.get("title", "")) if item else ""
    subtitle = ""
    try:
        subtitle = clean_text((item.get("subInfo") or {}).get("subTitle", "") or "")
    except Exception:
        subtitle = ""
    return title, subtitle


def build_245_with_people_from_sources(item: dict, author_raw: str, prefer: str = "aladin") -> str:
    """
    ì›ë³¸ ì½”ë“œì˜ build_245_with_people_from_sources ê¸°ëŠ¥ì„ ì¬êµ¬ì„±.
    GPT í†µí•© êµ¬ì¡°ì—ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥.
    """
    title, subtitle = parse_aladin_title_and_subtitle(item)
    creators = clean_text(author_raw or "")

    # a
    a_part = title
    # b
    b_part = subtitle
    # c
    c_part = creators

    # 245 êµ¬ì„±
    if b_part:
        tag_245 = f"=245  10$a{a_part} :$b{b_part} /$c{c_part}"
    else:
        tag_245 = f"=245  10$a{a_part} /$c{c_part}"

    return tag_245


# ------------------------------------------------------------
# 246 êµ¬ì„±
# ------------------------------------------------------------

def build_246_from_aladin_item(item: dict) -> str:
    """
    ë¶€ì œë‚˜ ë³‘ê¸° ì œëª©ì´ ìˆì„ ê²½ìš°ë¥¼ ìœ„í•œ 246 ìƒì„± (ë‹¨ìˆœ ë²„ì „).
    GPT í†µí•© êµ¬ì¡°ì™€ ì¶©ëŒ ì—†ìŒ.
    """
    title, subtitle = parse_aladin_title_and_subtitle(item)
    if not subtitle:
        return "=246  3\\$a" + title  # ë¶€ì œ ì—†ìŒ ì‹œ titleë§Œ

    return f"=246  3\\$a{subtitle}"


# ------------------------------------------------------------
# 700 ë¹Œë” (ì €ìëª… ì •ê·œí™”)
# ------------------------------------------------------------

def normalize_author_for_700(name: str, origin_lang_code: Optional[str] = None) -> str:
    """
    ê¸°ì¡´ build_700_people_pref_aladinì—ì„œ í•µì‹¬ë§Œ ì¶”ì¶œí•´
    GPT ë§ˆìŠ¤í„° êµ¬ì¡°ì™€ ì¶©ëŒ ì—†ê²Œ ë‹¨ìˆœí™”.
    """
    if not name:
        return ""

    name = name.strip()

    # ì•„ì‹œì•„ê¶Œ (í•œêµ­/ì¤‘êµ­/ì¼ë³¸ ë“±)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    if origin_lang_code in {"kor", "chi", "jpn"}:
        return name

    # ê·¸ ì™¸: 'ì„±, ì´ë¦„' í˜•íƒœë¡œ ë¶„ë¦¬
    parts = name.replace("Â·", " ").split()
    if len(parts) >= 2:
        family = parts[0]
        given = " ".join(parts[1:])
        return f"{family}, {given}"

    return name


def build_700_people_pref_aladin(author_raw: str, item: dict, origin_lang_code=None) -> List[str]:
    """
    ì•Œë¼ë”˜ author ë¬¸ìì—´ì„ ê¸°ë°˜ìœ¼ë¡œ 700 ìƒì„±.
    """
    r = []
    if not author_raw:
        return r

    try:
        names = [x.strip() for x in str(author_raw).split(",") if x.strip()]
    except Exception:
        names = [author_raw]

    for nm in names:
        norm = normalize_author_for_700(nm, origin_lang_code)
        r.append(f"=700  1\\$a{norm}")

    return r


# ------------------------------------------------------------
# 90010 ë¹Œë” (Wikidata ê¸°ë°˜ LOD)
# (ë„¤ ì›ë³¸ì˜ í•µì‹¬ êµ¬ì¡° ìœ ì§€)
# ------------------------------------------------------------

LAST_PROV_90010 = {}

def extract_people_from_aladin(item: dict) -> dict:
    """
    ì•Œë¼ë”˜ itemì—ì„œ ì¸ë¬¼ëª…/ì—­í• ì„ ì¶”ì¶œ â†’ LOD ì¡°íšŒ í›„ë³´ ìƒì„±.
    """
    res = {}
    if not item:
        return res

    # ì—¬ëŸ¬ ì¢…ë¥˜ì˜ author í•„ë“œ ì§€ì›
    raw = item.get("author") or item.get("authors") or ""
    raw = clean_text(raw)
    if not raw:
        return res

    # ë‹¨ìˆœ ë¶„ë¦¬
    try:
        for p in raw.split(","):
            p = p.strip()
            if not p:
                continue
            res[p] = {"role": "author"}
    except Exception:
        pass

    return res


def fetch_wikidata_korean_name(name: str) -> Optional[str]:
    """
    Wikidata APIë¥¼ ì´ìš©í•´ í•œêµ­ì–´ ë¼ë²¨ ì°¾ê¸° (ë‹¨ìˆœí™” ë²„ì „).
    """
    try:
        url = "https://www.wikidata.org/w/api.php"
        params = {
            "action": "wbsearchentities",
            "language": "ko",
            "format": "json",
            "search": name,
        }
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
        j = r.json()
        if j.get("search"):
            label = j["search"][0].get("label")
            return label
    except Exception as e:
        dbg_err(f"[wikidata error] {e}")
    return None


def build_90010_from_wikidata(people: dict, include_translator: bool = False) -> List[str]:
    """
    ê¸°ì¡´ ë¡œì§ì„ ë‹¨ìˆœí™”í•˜ì—¬ GPT í†µí•© êµ¬ì¡°ì™€ ë¬¸ì œ ì—†ì´ ë™ì‘í•˜ê²Œ í•¨.
    """
    out = []
    global LAST_PROV_90010
    LAST_PROV_90010 = {}

    for name, info in (people or {}).items():
        if not include_translator and info.get("role") == "translator":
            continue

        label_ko = fetch_wikidata_korean_name(name)
        if not label_ko:
            continue

        LAST_PROV_90010[name] = label_ko
        out.append(f"=90010  \\\\$a{label_ko}")

    return out


# ------------------------------------------------------------
# 049 ë¹Œë” (ë“±ë¡ê¸°í˜¸)
# ------------------------------------------------------------

def build_049(reg_mark: str, reg_no: str, copy_symbol: str) -> str:
    """
    ë‹¨ìˆœ ë“±ë¡ê¸°í˜¸ ì¡°ë¦½: =049  \\$a<ë“±ë¡ê¸°í˜¸>$c<ë“±ë¡ë²ˆí˜¸>$d<ë³„ì¹˜ê¸°í˜¸>
    """
    reg_mark = (reg_mark or "").strip()
    reg_no = (reg_no or "").strip()
    copy_symbol = (copy_symbol or "").strip()

    parts = []
    if reg_mark:
        parts.append(f"$a{reg_mark}")
    if reg_no:
        parts.append(f"$c{reg_no}")
    if copy_symbol:
        parts.append(f"$d{copy_symbol}")

    return "=049  \\\\" + "".join(parts)


# ============================================================
# Part 4 â€” 008 / ë°œí–‰ì§€ / KPIPA / í˜•íƒœì‚¬í•­(300) / ê°€ê²© / 020
# ============================================================


# ------------------------------------------------------------
# 008 ë¹Œë” (ë„¤ ì›ë³¸ í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¬ì‘ì„±)
# ------------------------------------------------------------

def build_008_from_isbn(
    isbn: str,
    aladin_pubdate: str = "",
    aladin_title: str = "",
    aladin_category: str = "",
    aladin_desc: str = "",
    aladin_toc: str = "",
    override_country3: Optional[str] = None,
    override_lang3: Optional[str] = None,
    cataloging_src: str = "a",
):
    """
    ë„¤ ì›ë³¸ì˜ build_008_from_isbn ë‚´ìš©ì„ ë‹¨ìˆœí™”í•˜ì—¬,
    GPT í†µí•© êµ¬ì¡°ì™€ ì¶©ëŒ ì—†ê²Œ ì •ë¦¬.
    """
    today = datetime.datetime.now().strftime("%y%m%d")  # 00-05

    # ë°œí–‰ë…„(07-10)
    year = ""
    try:
        if aladin_pubdate and len(aladin_pubdate) >= 4:
            year = aladin_pubdate[:4]
    except Exception:
        year = ""

    if not year:
        year = "9999"  # fallback

    # ì¶œíŒêµ­ ì½”ë“œ(15-17)
    country3 = override_country3 or "ko "

    # ì–¸ì–´ì½”ë“œ(35-37)
    lang3 = override_lang3 or "kor"

    # 008 ê¸°ë³¸ êµ¬ì¡°
    # positions: [00-05] date entered
    #            [06]    type of date (s)
    #            [07-10] publication year
    #            [11-14] place (skip or blank)
    #            [15-17] country code
    #            [35-37] language code
    #            etc.
    # ì—¬ê¸°ëŠ” ê°„ë‹¨í™” ëª¨ë¸ë¡œ êµ¬ì„±
    out = (
        f"{today}"        # 00-05
        f"s"              # 06
        f"{year}"         # 07-10
        f"    "           # 11-14 blank
        f"{country3}"     # 15-17
        f"                       "  # 18-34 filler
        f"{lang3}"        # 35-37
        f" "              # 38 filler
    )

    return out


# ------------------------------------------------------------
# ë°œí–‰ì§€Â·ë°œí–‰êµ­ íŒë³„ (build_pub_location_bundle)
# ------------------------------------------------------------

def _resolve_country_code_from_place(place_raw: str) -> str:
    """
    ë°œí–‰ì§€ë¥¼ ë¬¸ìì—´ì—ì„œ íŒë³„í•˜ì—¬ 008ì˜ country3 ë°˜í™˜.
    ê°„ë‹¨í•œ ë§¤í•‘ í…Œì´ë¸”ë¡œ ëŒ€ì²´.
    """
    if not place_raw:
        return "ko "

    p = place_raw.lower()
    if "seoul" in p or "ì„œìš¸" in p:
        return "ko "
    if "tokyo" in p or "ë„ì¿„" in p:
        return "ja "
    if "beijing" in p or "ë² ì´ì§•" in p:
        return "ch "
    if "new york" in p:
        return "us "
    return "ko "


def build_pub_location_bundle(isbn: str, publisher_raw: str) -> dict:
    """
    ë„¤ ì›ë³¸ì˜ KPIPA/ë¬¸ì²´ë¶€ DB ê¸°ë°˜ ë°œí–‰ì§€ íŒë³„ ë¡œì§ì€ ë§¤ìš° ë°©ëŒ€í–ˆì§€ë§Œ,
    GPT í†µí•© êµ¬ì¡°ì—ì„œëŠ” ê°„ë‹¨í•œ í˜•íƒœë§Œ ìœ ì§€.
    """
    place_raw = ""
    resolved_pub = publisher_raw.strip() if publisher_raw else ""

    # ë°œí–‰ì§€ í•„ë“œ(ë‹¨ìˆœ ë²„ì „)
    place_display = place_raw or "ë°œí–‰ì§€ ë¯¸ìƒ"

    # country3
    country_code = _resolve_country_code_from_place(place_raw)

    return {
        "source": "simple",
        "place_raw": place_raw,
        "place_display": place_display,
        "country_code": country_code,
        "resolved_publisher": resolved_pub,
        "debug": [],
    }


# ------------------------------------------------------------
# 300 ë¹Œë” (ì›ë³¸ build_300_from_aladin_detail ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©)
# ------------------------------------------------------------

# ì—¬ê¸°ì„œëŠ” ë„¤ ì›ë³¸ ì½”ë“œë¥¼ ìœ ì§€í•´ì•¼ í•˜ë¯€ë¡œ,
# Part 3ì— ìˆë˜ build_300_from_aladin_detail(item)ì„ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©.


# ------------------------------------------------------------
# ê°€ê²©/ISBN â†’ 950 ë¹Œë”
# ------------------------------------------------------------

def _extract_price_kr(item: dict, isbn: str) -> Optional[str]:
    """
    ê°€ê²© ì¶”ì¶œ ë‹¨ìˆœí™” (ì›ë³¸ ì½”ë“œ ê¸°ë°˜).
    """
    try:
        p = (item or {}).get("priceStandard")
        if p:
            # ìˆ«ìë§Œ
            return re.sub(r"[^0-9]", "", str(p))
    except Exception:
        pass
    return None


def build_950_from_item_and_price(item: dict, isbn: str) -> str:
    """
    =950  \\$l xxxx
    """
    price = _extract_price_kr(item, isbn) or ""
    return f"=950  \\\\$l{price}"


# ------------------------------------------------------------
# 020 ë¹Œë” (ISBN, ê°€ê²©)
# ------------------------------------------------------------

def _build_020_from_item_and_nlk(isbn: str, item: dict) -> str:
    """
    ì›ë³¸ 020 ìƒì„±ê¸° ë‹¨ìˆœí™”.
    ê°€ê²©ì€ ë°˜ë“œì‹œ ':$cê°€ê²©' í˜•íƒœë¡œ ëë‚˜ì•¼ í•˜ê³ ,
    ë„ˆì˜ ê·œì¹™ '':$c13000'' ìœ ì§€.
    """
    isbn13 = normalize_isbn(isbn)
    price = _extract_price_kr(item, isbn) or ""

    if price:
        return f"=020  \\\\$a{isbn13} :$c{price}"
    else:
        return f"=020  \\\\$a{isbn13}"

# ============================================================
# Part 5 â€” generate_all_oneclick (GPT 1íšŒ í˜¸ì¶œ ë²„ì „)
# ============================================================

def generate_all_oneclick(
    isbn: str,
    reg_mark: str = "",
    reg_no: str = "",
    copy_symbol: str = "",
    use_ai_940: bool = True
):
    """
    GPT-4o ë‹¨ì¼ í˜¸ì¶œ ê¸°ë°˜ ì™„ì „ ë¦¬íŒ©í„°ë§ ë²„ì „.
    ê¸°ì¡´ generate_all_oneclick ë…¼ë¦¬ë¥¼ ë™ì¼í•˜ê²Œ ìˆ˜í–‰í•˜ë˜,
    GPT í˜¸ì¶œì€ ì˜¤ì§ 1íšŒë§Œ ì‹¤í–‰ëœë‹¤.
    """

    # Reset debug lines
    global CURRENT_DEBUG_LINES
    CURRENT_DEBUG_LINES = []

    # --------------------------------------------------------
    # 1) ê¸°ë³¸ ì¤€ë¹„
    # --------------------------------------------------------
    isbn = normalize_isbn(isbn)
    aladin_item = fetch_aladin_item_raw(isbn)
    author_raw, _ = fetch_nlk_author_only(isbn)

    # --------------------------------------------------------
    # 2) GPT Master í˜¸ì¶œ (ë‹¨ 1íšŒ)
    # --------------------------------------------------------
    master_payload = build_gpt_master_payload(isbn, aladin_item)
    gpt_result = call_gpt_master(master_payload)

    marc041_raw = gpt_result.get("marc041")
    marc546_text = gpt_result.get("marc546")
    keywords_653 = gpt_result.get("keywords_653", [])
    kdc_056 = gpt_result.get("kdc_056")
    title940_raw = gpt_result.get("title940")

    dbg("[GPT result]", json.dumps(gpt_result, ensure_ascii=False))

    # --------------------------------------------------------
    # 3) GPT ê²°ê³¼ â†’ KORMARC í›„ì²˜ë¦¬ ë³€í™˜
    # --------------------------------------------------------
    tag_041_text = make_041(marc041_raw)
    tag_546_text = make_546(marc546_text)
    tag_653 = make_653(keywords_653)
    tag_056 = make_056(kdc_056)
    tag_940_list = make_940(title940_raw) if use_ai_940 else []

    # ì–¸ì–´ì½”ë“œ ì¶”ì¶œ (700 ì •ë ¬ìš©)
    origin_lang = None
    if marc041_raw:
        m = re.search(r"\$h([a-z]{3})", marc041_raw, re.IGNORECASE)
        if m:
            origin_lang = m.group(1).lower()

    # --------------------------------------------------------
    # 4) MARC í•„ë“œ ìƒì„±
    # --------------------------------------------------------
    marc_rec = Record(to_unicode=True, force_utf8=True)
    pieces = []   # (Field, mrk_string) ë¦¬ìŠ¤íŠ¸

    # ----- 245 / 246 / 700 -----
    marc245 = build_245_with_people_from_sources(aladin_item, author_raw, prefer="aladin")
    f_245 = mrk_str_to_field(marc245)

    marc246 = build_246_from_aladin_item(aladin_item)
    f_246 = mrk_str_to_field(marc246)

    mrk_700_list = build_700_people_pref_aladin(
        author_raw, aladin_item, origin_lang_code=origin_lang
    )

    # ----- 90010 (Wikidata LOD) -----
    people = extract_people_from_aladin(aladin_item)
    mrk_90010 = build_90010_from_wikidata(people, include_translator=False)

    # ----- 260 -----
    publisher_raw = aladin_item.get("publisher", "") if aladin_item else ""
    pubdate = aladin_item.get("pubDate", "") if aladin_item else ""
    pubyear = pubdate[:4] if pubdate and len(pubdate) >= 4 else ""

    bundle = build_pub_location_bundle(isbn, publisher_raw)
    tag_260 = build_260(
        place_display=bundle["place_display"],
        publisher_name=publisher_raw,
        pubyear=pubyear,
    )
    f_260 = mrk_str_to_field(tag_260)

    # ----- 008 -----
    lang3_override = None
    if marc041_raw:
        m = re.search(r"\$a([a-z]{3})", marc041_raw, re.IGNORECASE)
        if m:
            lang3_override = m.group(1).lower()

    data_008 = build_008_from_isbn(
        isbn,
        aladin_pubdate=pubdate,
        aladin_title=aladin_item.get("title", ""),
        aladin_category=aladin_item.get("categoryName", ""),
        aladin_desc=aladin_item.get("description", ""),
        aladin_toc=(aladin_item.get("subInfo") or {}).get("toc", ""),
        override_country3=bundle["country_code"],
        override_lang3=lang3_override,
        cataloging_src="a",
    )
    field_008 = Field(tag="008", data=data_008)

    # ----- 020 -----
    tag_020 = _build_020_from_item_and_nlk(isbn, aladin_item)
    f_020 = mrk_str_to_field(tag_020)

    # ----- ì¶”ê°€ 020 (set_isbn) -----
    nlk_extra = fetch_additional_code_from_nlk(isbn)
    set_isbn = (nlk_extra.get("set_isbn") or "").strip()

    # ----- 300 -----
    tag_300, f_300 = build_300_from_aladin_detail(aladin_item)

    # ----- 490 / 830 -----
    tag_490, tag_830 = build_490_830_mrk_from_item(aladin_item)
    f_490 = mrk_str_to_field(tag_490)
    f_830 = mrk_str_to_field(tag_830)

    # ----- 950 -----
    tag_950 = build_950_from_item_and_price(aladin_item, isbn)
    f_950 = mrk_str_to_field(tag_950)

    # ----- 049 -----
    tag_049 = build_049(reg_mark, reg_no, copy_symbol)
    f_049 = mrk_str_to_field(tag_049)

    # --------------------------------------------------------
    # 5) pieces[] ìˆœì„œëŒ€ë¡œ ì¡°ë¦½
    # --------------------------------------------------------

    # 008
    pieces.append((field_008, f"=008  {data_008}"))

    # 020
    if f_020: pieces.append((f_020, tag_020))

    # 0201 (set ISBN)
    if set_isbn:
        tag_020_1 = f"=020  1\\$a{set_isbn} (set)"
        f_020_1 = mrk_str_to_field(tag_020_1)
        pieces.append((f_020_1, tag_020_1))

    # 041 / 546 / 056 / 653 / 940
    if tag_041_text:
        f_041 = mrk_str_to_field(tag_041_text)
        if f_041: pieces.append((f_041, tag_041_text))

    if tag_546_text:
        f_546 = mrk_str_to_field(tag_546_text)
        if f_546: pieces.append((f_546, tag_546_text))

    if tag_056:
        f_056 = mrk_str_to_field(tag_056)
        if f_056: pieces.append((f_056, tag_056))

    if tag_653:
        f_653 = mrk_str_to_field(tag_653)
        if f_653: pieces.append((f_653, tag_653))

    for mrk in tag_940_list or []:
        f_940 = mrk_str_to_field(mrk)
        if f_940: pieces.append((f_940, mrk))

    # 245 / 246 / 260 / 300 / 490 / 830 / 950 / 049
    if f_245: pieces.append((f_245, marc245))
    if f_246: pieces.append((f_246, marc246))
    if f_260: pieces.append((f_260, tag_260))
    if f_300: pieces.append((f_300, tag_300))
    if f_490: pieces.append((f_490, tag_490))
    if f_830: pieces.append((f_830, tag_830))
    if f_950: pieces.append((f_950, tag_950))
    if f_049: pieces.append((f_049, tag_049))

    # ----- 700 -----
    for m in mrk_700_list:
        f = mrk_str_to_field(m)
        if f: pieces.append((f, m))

    # ----- 90010 -----
    for m in mrk_90010:
        f = mrk_str_to_field(m)
        if f: pieces.append((f, m))

    # --------------------------------------------------------
    # 6) MARC Record ê°ì²´ì— add_field
    # --------------------------------------------------------
    for f, _ in pieces:
        marc_rec.add_field(f)

    # --------------------------------------------------------
    # 7) MRK ì „ì²´ í…ìŠ¤íŠ¸ ì¡°í•©
    # --------------------------------------------------------
    mrk_text = "\n".join(m for _, m in pieces)

    # --------------------------------------------------------
    # 8) ë©”íƒ€ ì •ë³´ êµ¬ì„±
    # --------------------------------------------------------
    meta = {
        "isbn": isbn,
        "title": aladin_item.get("title"),
        "publisher": publisher_raw,
        "pubyear": pubyear,
        "041": tag_041_text,
        "546": tag_546_text,
        "056": tag_056,
        "653": tag_653,
        "940": tag_940_list,
        "kdc_code": kdc_056,
        "Candidates": [],   # 700 í›„ë³´ì í‘œì‹œ ì›í•˜ë©´ ì¶”ê°€ ê°€ëŠ¥
        "debug_lines": list(CURRENT_DEBUG_LINES),
        "provenance_90010": LAST_PROV_90010,
    }

    marc_bytes = marc_rec.as_marc()

    return marc_rec, marc_bytes, mrk_text, meta

# ============================================================
# Part 6 â€” run_and_export + Streamlit UI ì „ì²´
# ============================================================

def save_marc_files(record: Record, save_dir: str, base_filename: str):
    """
    MRC(ë°”ì´ë„ˆë¦¬) / MRK(í…ìŠ¤íŠ¸) ëª¨ë‘ ì €ì¥
    """
    os.makedirs(save_dir, exist_ok=True)

    # .mrc
    mrc_path = os.path.join(save_dir, f"{base_filename}.mrc")
    with open(mrc_path, "wb") as f:
        f.write(record.as_marc())

    # .mrk
    mrk_text = record_to_mrk_from_record(record)
    mrk_path = os.path.join(save_dir, f"{base_filename}.mrk")
    with open(mrk_path, "w", encoding="utf-8") as f:
        f.write(mrk_text)

    return mrc_path, mrk_path


def run_and_export(
    isbn: str,
    *,
    reg_mark: str = "",
    reg_no: str = "",
    copy_symbol: str = "",
    use_ai_940: bool = True,
    save_dir: str = "./output",
    preview_in_streamlit: bool = True,
):
    """
    GPT 1íšŒ í˜¸ì¶œ generate_all_oneclick() ì‹¤í–‰ â†’
    íŒŒì¼ ì €ì¥ â†’ Streamlit preview & download ì œê³µ
    """
    record, marc_bytes, mrk_text, meta = generate_all_oneclick(
        isbn,
        reg_mark=reg_mark,
        reg_no=reg_no,
        copy_symbol=copy_symbol,
        use_ai_940=use_ai_940,
    )

    # íŒŒì¼ ì €ì¥
    save_marc_files(record, save_dir, isbn)

    # Streamlit í”„ë¦¬ë·°
    if preview_in_streamlit:
        try:
            st.success("ğŸ“¦ MRC/MRK íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # MRK Preview
            with st.expander("ğŸ“„ MRK ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                st.text_area("MRK", mrk_text, height=350)

            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼: mrc
            st.download_button(
                "ğŸ“˜ MARC (mrc) ë‹¤ìš´ë¡œë“œ",
                data=marc_bytes,
                file_name=f"{isbn}.mrc",
                mime="application/marc",
            )

            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼: mrk
            st.download_button(
                "ğŸ§¾ MARC (mrk) ë‹¤ìš´ë¡œë“œ",
                data=mrk_text,
                file_name=f"{isbn}.mrk",
                mime="text/plain",
            )
        except Exception as e:
            st.warning(f"Streamlit ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜: {e}")

    return record, marc_bytes, mrk_text, meta


# ============================================================
# Streamlit UI
# ============================================================

st.header("ğŸ“š ISBN â†’ MARC ìë™ ìƒì„±ê¸° (GPT-4o ë‹¨ 1íšŒ í˜¸ì¶œ)")

st.checkbox("ğŸ§  940 ìƒì„±ì— OpenAI í™œìš©", value=True, key="use_ai_940")

# --- ì…ë ¥ Form ---
with st.form(key="isbn_form", clear_on_submit=False):
    st.text_input(
        "ğŸ”¹ ë‹¨ì¼ ISBN ì…ë ¥",
        placeholder="ì˜ˆ: 9788937462849",
        key="single_isbn_input"
    )
    st.file_uploader(
        "ğŸ“ CSV ì—…ë¡œë“œ (UTF-8, ì—´: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)",
        type=["csv"],
        key="csv_uploader",
    )

    submitted = st.form_submit_button("ğŸš€ ë³€í™˜ ì‹¤í–‰", use_container_width=True)


# ------------------------------------------------------------
# ì œì¶œ í›„ ì²˜ë¦¬
# ------------------------------------------------------------
if submitted:
    single_isbn = (st.session_state.get("single_isbn_input") or "").strip()
    uploaded = st.session_state.get("csv_uploader")

    jobs = []

    # ë‹¨ì¼ ISBN
    if single_isbn:
        jobs.append([single_isbn, "", "", ""])

    # CSV ì½ê¸°
    if uploaded is not None:
        try:
            df = pd.read_csv(uploaded)
            required = {"ISBN", "ë“±ë¡ê¸°í˜¸", "ë“±ë¡ë²ˆí˜¸", "ë³„ì¹˜ê¸°í˜¸"}
            if not required.issubset(df.columns):
                st.error("âŒ CSVì— í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸")
                st.stop()

            rows = df[["ISBN", "ë“±ë¡ê¸°í˜¸", "ë“±ë¡ë²ˆí˜¸", "ë³„ì¹˜ê¸°í˜¸"]].dropna(subset=["ISBN"]).copy()
            rows["ë³„ì¹˜ê¸°í˜¸"] = rows["ë³„ì¹˜ê¸°í˜¸"].fillna("")
            jobs.extend(rows.values.tolist())
        except Exception as e:
            st.error(f"âŒ CSV ì½ê¸° ì˜¤ë¥˜: {e}")
            st.stop()

    if not jobs:
        st.warning("ë³€í™˜í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    st.info(f"ì´ {len(jobs)}ê±´ ì²˜ë¦¬ ì¤‘â€¦")
    prog = st.progress(0)

    marc_all = []
    st.session_state.meta_all = {}
    results = []

    # --------------------------------------------------------
    # ë³¸ ì²˜ë¦¬ ë£¨í”„
    # --------------------------------------------------------
    for i, (isbn, reg_mark, reg_no, copy_symbol) in enumerate(jobs, start=1):

        record, marc_bytes, mrk_text, meta = run_and_export(
            isbn,
            reg_mark=reg_mark,
            reg_no=reg_no,
            copy_symbol=copy_symbol,
            use_ai_940=st.session_state.get("use_ai_940", True),
            save_dir="./output",
            preview_in_streamlit=True,
        )

        marc_all.append(mrk_text)
        st.session_state.meta_all[isbn] = meta
        results.append((record, isbn, mrk_text, meta))

        # Processing indicator
        prog.progress(i / len(jobs))

    # --------------------------------------------------------
    # ì „ì²´ MRK ë‹¤ìš´ë¡œë“œ
    # --------------------------------------------------------
    blob = ("\n\n".join(marc_all)).encode("utf-8-sig")
    st.download_button(
        "ğŸ“¦ ëª¨ë“  MARC(MRK) í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
        data=blob,
        file_name="marc_output.txt",
        mime="text/plain",
        key="dl_all_marc",
    )

    # --------------------------------------------------------
    # ì „ì²´ MRC ë‹¤ìš´ë¡œë“œ (.mrc ë¬¶ìŒ)
    # --------------------------------------------------------
    buffer = io.BytesIO()
    writer = MARCWriter(buffer)
    for record_obj, isbn, _, _ in results:
        try:
            writer.write(record_obj)
        except Exception:
            st.warning(f"âš ï¸ MRC ë³€í™˜ ì‹¤íŒ¨: {isbn}")
    buffer.seek(0)

    st.download_button(
        "ğŸ“¥ ì „ì²´ MRC ë¬¶ìŒ ë‹¤ìš´ë¡œë“œ",
        data=buffer,
        file_name="marc_output.mrc",
        mime="application/octet-stream",
    )

    st.session_state["last_results"] = results


# ------------------------------------------------------------
# ğŸ”§ ë„ì›€ë§
# ------------------------------------------------------------
with st.expander("âš™ï¸ ì‚¬ìš© íŒ"):
    st.markdown(
        """
        - 245/246/700: ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ êµ¬ì„±  
        - 041/546/653/056/940: GPT-4o 1íšŒ í˜¸ì¶œ ê²°ê³¼ë¥¼ ë¡œì»¬ì—ì„œ í›„ì²˜ë¦¬  
        - 260/300/950: ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ë¡œì»¬ ìƒì„±  
        - ëª¨ë“  MARCëŠ” MRK/MRCë¡œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
        """
    )




