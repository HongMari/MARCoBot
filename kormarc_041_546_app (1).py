# ============================================
# PART 1 â€” Imports / Global Setup / NLK Key Load / Utilities
# ============================================

import re
import io
import json
import math
import html
import requests
import pandas as pd
import datetime
from dataclasses import dataclass
from typing import Optional, Dict, Any
from collections import Counter
from bs4 import BeautifulSoup

import streamlit as st
from pymarc import Record, Field, Subfield, MARCWriter
from oauth2client.service_account import ServiceAccountCredentials
import gspread

# ---------------------------
# Global Session
# ---------------------------
SESSION = requests.Session()

# ---------------------------
# NLK ì¸ì¦í‚¤ ìë™ ë¡œë”©
# ---------------------------
def _auto_load_nlk_key():
    """
    Streamlit secrets êµ¬ì¡°ê°€ ì–´ë–¤ í˜•íƒœì´ë“  ìë™ íƒì§€:
    1) st.secrets["nlk"]["cert_key"]
    2) st.secrets["cert_key"]
    3) ì—†ìœ¼ë©´ ""
    """
    try:
        if "nlk" in st.secrets and "cert_key" in st.secrets["nlk"]:
            return st.secrets["nlk"]["cert_key"]

        if "cert_key" in st.secrets:
            return st.secrets["cert_key"]

    except Exception:
        pass

    return ""

NLK_CERT_KEY = _auto_load_nlk_key()


# ---------------------------
# ì•Œë¼ë”˜ API KEY ë¡œë”©
# ---------------------------
def _auto_load_aladin_key():
    try:
        if "aladin" in st.secrets and "ttbkey" in st.secrets["aladin"]:
            return st.secrets["aladin"]["ttbkey"]
    except:
        pass
    return ""

ALADIN_TTB_KEY = _auto_load_aladin_key()


# ---------------------------
# ê³µí†µ ë””ë²„ê·¸ í•¨ìˆ˜
# ---------------------------
CURRENT_DEBUG_LINES = []

def dbg(*args):
    CURRENT_DEBUG_LINES.append(" ".join(str(a) for a in args))

def dbg_err(*args):
    CURRENT_DEBUG_LINES.append("[ERROR] " + " ".join(str(a) for a in args))


# ---------------------------
# ê³µí†µ í…ìŠ¤íŠ¸ ìœ í‹¸
# ---------------------------
def clean_text(s: str | None) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    return re.sub(r"\s+", " ", s).strip()


# ---------------------------
# NLK SearchApi â€” ì €ìë§Œ ê°€ì ¸ì˜¤ê¸°
# ---------------------------
def fetch_nlk_author_only(isbn: str):
    """
    NLK SearchApi.do ì—ì„œ AUTHORë§Œ ê°€ì ¸ì˜¤ê¸°.
    ì—ëŸ¬ ë°œìƒí•´ë„ ì ˆëŒ€ ì£½ì§€ ì•Šê³  ("", None) ë°˜í™˜.
    """
    try:
        clean_isbn = isbn.replace("-", "").strip()
        params = {
            "cert_key": NLK_CERT_KEY,
            "result_style": "json",
            "page_no": 1,
            "page_size": 1,
            "isbn": clean_isbn
        }

        url = "https://seoji.nl.go.kr/landingPage/SearchApi.do"
        res = SESSION.get(url, params=params, timeout=(5, 10))
        res.raise_for_status()
        data = res.json()

        doc = None
        if "docs" in data and isinstance(data["docs"], list) and data["docs"]:
            doc = data["docs"][0]
        elif "doc" in data and isinstance(data["doc"], list) and data["doc"]:
            doc = data["doc"][0]

        if not doc:
            return "", None

        raw_author = (
            doc.get("AUTHOR")
            or doc.get("AUTHOR1")
            or doc.get("AUTHOR2")
            or doc.get("AUTHOR3")
            or ""
        ).strip()

        return raw_author, doc

    except Exception:
        return "", None



# ============================================
# PART 2 â€” 008 ìƒì„±ê¸° / ì§€ì—­Â·êµ­ê°€ì½”ë“œ / Detect ìœ í‹¸
# ============================================

# ---------------------------
# í•œêµ­ ì§€ì—­ëª… â†’ KORMARC 3ìë¦¬ ë°œí–‰êµ­ ë¶€í˜¸
# ---------------------------
KR_REGION_TO_CODE = {
    "ì„œìš¸": "ulk", "ì„œìš¸íŠ¹ë³„ì‹œ": "ulk",
    "ê²½ê¸°": "ggk", "ê²½ê¸°ë„": "ggk",
    "ë¶€ì‚°": "bnk", "ë¶€ì‚°ê´‘ì—­ì‹œ": "bnk",
    "ëŒ€êµ¬": "tgk", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "tgk",
    "ì¸ì²œ": "ick", "ì¸ì²œê´‘ì—­ì‹œ": "ick",
    "ê´‘ì£¼": "kjk", "ê´‘ì£¼ê´‘ì—­ì‹œ": "kjk",
    "ëŒ€ì „": "tjk", "ëŒ€ì „ê´‘ì—­ì‹œ": "tjk",
    "ìš¸ì‚°": "usk", "ìš¸ì‚°ê´‘ì—­ì‹œ": "usk",
    "ì„¸ì¢…": "sjk", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "sjk",
    "ê°•ì›": "gak", "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "gak",
    "ì¶©ë¶": "hbk", "ì¶©ì²­ë¶ë„": "hbk",
    "ì¶©ë‚¨": "hck", "ì¶©ì²­ë‚¨ë„": "hck",
    "ì „ë¶": "jbk", "ì „ë¼ë¶ë„": "jbk",
    "ì „ë‚¨": "jnk", "ì „ë¼ë‚¨ë„": "jnk",
    "ê²½ë¶": "gbk", "ê²½ìƒë¶ë„": "gbk",
    "ê²½ë‚¨": "gnk", "ê²½ìƒë‚¨ë„": "gnk",
    "ì œì£¼": "jjk", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "jjk",
}

COUNTRY_FIXED = "ulk"      # ê¸°ë³¸ ë°œí–‰êµ­
LANG_FIXED    = "kor"      # ê¸°ë³¸ ì–¸ì–´ì½”ë“œ


# =====================================================
# 008 ë³¸ë¬¸ ìƒì„±ê¸°(KORMARC ë‹¨í–‰ë³¸)
# =====================================================
def build_008_kormarc_bk(
    date_entered,      # YYMMDD
    date1,             # ì¶œíŒì—°ë„ 4ìë¦¬
    country3,          # ë°œí–‰êµ­ 3ìë¦¬
    lang3,             # ì–¸ì–´ì½”ë“œ 3ìë¦¬
    date2="",          # ì¢…ë£Œ ì—°ë„(ì—°ì†ê°„í–‰ë¬¼ìš©)
    illus4="",         # ì‚½í™”ì½”ë“œ ìµœëŒ€ 4ì
    has_index="0",     # ìƒ‰ì¸ ìœ ë¬´
    lit_form=" ",      # ë¬¸í•™ í˜•íƒœì½”ë“œ
    bio=" ",           # ì „ê¸°ì  ìš”ì†Œ
    type_of_date="s",
    modified_record=" ",
    cataloging_src="a"
):
    def pad(s, n, fill=" "):
        s = "" if s is None else str(s)
        return (s[:n] + fill * n)[:n]

    if len(date_entered) != 6:
        raise ValueError("date_entered YYMMDD ì˜¤ë¥˜")

    if len(date1) != 4:
        raise ValueError("date1(ì¶œíŒì—°ë„)ì€ 4ìë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    body = "".join([
        date_entered,
        pad(type_of_date,1),
        date1,
        pad(date2,4),
        pad(country3,3),
        pad(illus4,4),
        " " * 4,
        " " * 2,
        pad(modified_record,1),
        "0",
        "0",
        has_index if has_index in ("0","1") else "0",
        pad(cataloging_src,1),
        pad(lit_form,1),
        pad(bio,1),
        pad(lang3,3),
        " " * 2
    ])

    if len(body) != 40:
        raise AssertionError(f"008 length mismatch: {len(body)}")

    return body



# =====================================================
# ì¶œíŒì—°ë„ ì¶”ì¶œ
# =====================================================
def extract_year_from_aladin_pubdate(pubdate_str: str) -> str:
    m = re.search(r"(19|20)\d{2}", pubdate_str or "")
    return m.group(0) if m else "19uu"


# =====================================================
# ë°œí–‰ì§€ ë¬¸ìì—´ â†’ country3 ì¶”ë¡ 
# =====================================================
def guess_country3_from_place(place_str: str) -> str:
    if not place_str:
        return COUNTRY_FIXED

    for key, code in KR_REGION_TO_CODE.items():
        if key in place_str:
            return code

    return COUNTRY_FIXED



# =====================================================
# ì‚½í™”/ë„í‘œ/ì‚¬ì§„ ê°ì§€
# =====================================================
def detect_illus4(text: str) -> str:
    keys = []
    if re.search(r"ì‚½í™”|ì‚½ë„|ë„í•´|ì¼ëŸ¬ìŠ¤íŠ¸|illustration|ê·¸ë¦¼", text, re.I):
        keys.append("a")
    if re.search(r"ë„í‘œ|í‘œ|ì°¨íŠ¸|ê·¸ë˜í”„|chart|graph", text, re.I):
        keys.append("d")
    if re.search(r"ì‚¬ì§„|í¬í† |photo|photograph|í™”ë³´", text, re.I):
        keys.append("o")
    out = []
    for k in keys:
        if k not in out:
            out.append(k)
    return "".join(out)[:4]


# =====================================================
# ìƒ‰ì¸ ê°ì§€
# =====================================================
def detect_index(text: str) -> str:
    return "1" if re.search(r"ìƒ‰ì¸|ì°¾ì•„ë³´ê¸°|index", text, re.I) else "0"


# =====================================================
# ë¬¸í•™ í˜•íƒœ ê°ì§€
# =====================================================
def detect_lit_form(title: str, category: str, extra_text: str = "") -> str:
    blob = f"{title} {category} {extra_text}"

    if re.search(r"ì„œê°„ì§‘|í¸ì§€|ì„œê°„ë¬¸|letters?", blob, re.I):
        return "i"
    if re.search(r"ê¸°í–‰|ì—¬í–‰ê¸°|ì¼ê¸°|ìˆ˜ê¸°|diary|travel", blob, re.I):
        return "m"
    if re.search(r"ì‹œì§‘|ì‚°ë¬¸ì‹œ|poem|poetry", blob, re.I):
        return "p"
    if re.search(r"ì†Œì„¤|novel|fiction|ì¥í¸|ì¤‘ë‹¨í¸", blob, re.I):
        return "f"
    if re.search(r"ì—ì„¸ì´|ìˆ˜í•„|essay", blob, re.I):
        return "e"

    return " "


# =====================================================
# ì „ê¸° ìš”ì†Œ ê°ì§€
# =====================================================
def detect_bio(text: str) -> str:
    if re.search(r"ìì„œì „|íšŒê³ ë¡|autobiograph", text, re.I):
        return "a"
    if re.search(r"ì „ê¸°|í‰ì „|biograph", text, re.I):
        return "b"
    if re.search(r"ì „ê¸°ì |ìì „ì |íšŒê³ ", text, re.I):
        return "d"
    return " "


# =====================================================
# ë°œí–‰ì§€ ë¯¸ìƒ íŒë‹¨
# =====================================================
def _is_unknown_place(s: str | None) -> bool:
    if not s:
        return False
    t = s.strip()
    t_no_sp = t.replace(" ", "")
    lower = t.lower()
    return (
        "ë¯¸ìƒ" in t
        or "ë¯¸ìƒ" in t_no_sp
        or "unknown" in lower
        or "place unknown" in lower
    )


# =====================================================
# ìµœì¢…: ISBN ê¸°ë°˜ 008 í•„ë“œ êµ¬ì„±
# =====================================================
def build_008_from_isbn(
    isbn: str,
    *,
    aladin_pubdate: str = "",
    aladin_title: str = "",
    aladin_category: str = "",
    aladin_desc: str = "",
    aladin_toc: str = "",
    source_300_place: str = "",
    override_country3: str = None,
    override_lang3: str = None,
    cataloging_src: str = "a",
):
    today  = datetime.datetime.now().strftime("%y%m%d")
    date1  = extract_year_from_aladin_pubdate(aladin_pubdate)

    # ---------- country3 ----------
    if override_country3:
        country3 = override_country3
    elif source_300_place:
        if _is_unknown_place(source_300_place):
            dbg("[008] ë°œí–‰ì§€ ë¯¸ìƒ ê°ì§€ â†’ country3='   '")
            country3 = "   "
        else:
            guessed = guess_country3_from_place(source_300_place)
            country3 = guessed or COUNTRY_FIXED
    else:
        country3 = COUNTRY_FIXED

    # ---------- ì–¸ì–´ ì½”ë“œ ----------
    lang3 = override_lang3 or LANG_FIXED

    # ---------- ì‚½í™”/ìƒ‰ì¸/ë¬¸í•™í˜•ì‹/ì „ê¸° ----------
    bigtext = " ".join([aladin_title, aladin_desc, aladin_toc])
    illus4    = detect_illus4(bigtext)
    has_index = detect_index(bigtext)
    lit_form  = detect_lit_form(aladin_title, aladin_category, bigtext)
    bio       = detect_bio(bigtext)

    return build_008_kormarc_bk(
        date_entered=today,
        date1=date1,
        country3=country3,
        lang3=lang3,
        illus4=illus4,
        has_index=has_index,
        lit_form=lit_form,
        bio=bio,
        cataloging_src=cataloging_src,
    )



# ============================================
# PART 3 â€” ì•Œë¼ë”˜ API / ìŠ¤í¬ë ˆì´í•‘ / KPIPA / ë¬¸ì²´ë¶€ / ë°œí–‰ì‚¬í•­ ë¬¶ìŒ
# ============================================

ALADIN_SEARCH_URL = "https://www.aladin.co.kr/search/wsearchresult.aspx"
HEADERS = {"User-Agent": "Mozilla/5.0"}


# ------------------------------------------------------
# ì•Œë¼ë”˜ ItemLookUp (API) â€” Safe Patch + ìºì‹±
# ------------------------------------------------------
@st.cache_data(ttl=3600)
def aladin_lookup_by_api(isbn13: str, ttbkey: str) -> "BookInfo | None":
    if not ttbkey:
        return None

    params = {
        "ttbkey": ttbkey,
        "itemIdType": "ISBN13",
        "ItemId": isbn13,
        "output": "js",
        "Version": "20131101",
        "OptResult": "authors,categoryName,fulldescription,toc,packaging,ratings"
    }
    try:
        r = requests.get(
            "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx",
            params=params,
            headers=HEADERS,
            timeout=15,
        )
        r.raise_for_status()
        data = r.json()
        items = data.get("item", [])
        if not items:
            dbg("[ALADIN API] ê²°ê³¼ ì—†ìŒ")
            return None

        it = items[0]

        # BookInfo dataclassì— ë§ê²Œ ë°˜í™˜
        return BookInfo(
            title=clean_text(it.get("title")),
            author=clean_text(it.get("author")),
            pub_date=clean_text(it.get("pubDate")),
            publisher=clean_text(it.get("publisher")),
            isbn13=clean_text(it.get("isbn13")) or isbn13,
            category=clean_text(it.get("categoryName")),
            description=clean_text(it.get("fulldescription")) or clean_text(it.get("description")),
            toc=clean_text(it.get("toc")),
            extra=it,
        )

    except Exception as e:
        dbg_err(f"[ALADIN API] ì˜ˆì™¸ ë°œìƒ: {e}")
        return None



# ------------------------------------------------------
# ì•Œë¼ë”˜ ì›¹ ìŠ¤í¬ë ˆì´í•‘ (ë°±ì—…) â€” Safe Patch + ìºì‹±
# ------------------------------------------------------
@st.cache_data(ttl=3600)
def aladin_lookup_by_web(isbn13: str) -> "BookInfo | None":
    try:
        params = {"SearchTarget": "Book", "SearchWord": f"isbn:{isbn13}"}
        sr = requests.get(ALADIN_SEARCH_URL, params=params, headers=HEADERS, timeout=15)
        sr.raise_for_status()
        soup = BeautifulSoup(sr.text, "html.parser")

        link_tag = soup.select_one("a.bo3")
        item_url = None

        if link_tag and link_tag.get("href"):
            item_url = "https://www.aladin.co.kr" + link_tag["href"]

        if not item_url:
            m = re.search(
                r'href=[\'"](/shop/wproduct\.aspx\?ItemId=\d+[^\'"]*)[\'"]',
                sr.text,
                re.I,
            )
            if m:
                item_url = "https://www.aladin.co.kr" + m.group(1)

        if not item_url:
            dbg_err("[ALADIN WEB] ìƒí’ˆ ë§í¬ ì°¾ê¸° ì‹¤íŒ¨")
            return None

        pr = requests.get(item_url, headers=HEADERS, timeout=15)
        pr.raise_for_status()
        psoup = BeautifulSoup(pr.text, "html.parser")

        og_title = psoup.select_one('meta[property="og:title"]')
        og_desc  = psoup.select_one('meta[property="og:description"]')

        title = clean_text(og_title["content"]) if og_title else ""
        desc  = clean_text(og_desc["content"]) if og_desc else ""

        text_body = clean_text(psoup.get_text(" "))[:4000]
        description = desc or text_body

        author, publisher, pub_date, category = "", "", "", ""
        info_box = psoup.select_one("#Ere_prod_allwrap")

        if info_box:
            text = clean_text(info_box.get_text(" "))
            ma = re.search(r"(ì €ì|ì§€ì€ì´)\s*:\s*([^\|Â·/]+)", text)
            mp = re.search(r"(ì¶œíŒì‚¬)\s*:\s*([^\|Â·/]+)", text)
            md = re.search(r"(ì¶œê°„ì¼)\s*:\s*[0-9]{4}\.[0-9]{1,2}\.[0-9]{1,2}", text)

            if ma: author = clean_text(ma.group(2))
            if mp: publisher = clean_text(mp.group(2))
            if md: pub_date = clean_text(md.group(0))

        crumbs = psoup.select(".location, .path, .breadcrumb")
        if crumbs:
            category = clean_text(" > ".join(c.get_text(" ") for c in crumbs))

        return BookInfo(
            title=title,
            description=description,
            isbn13=isbn13,
            author=author,
            publisher=publisher,
            pub_date=pub_date,
            category=category,
        )

    except Exception as e:
        dbg_err(f"[ALADIN WEB] ì˜ˆì™¸ ë°œìƒ: {e}")
        return None
# ============================================
# PART 4 â€” ì–¸ì–´(041) / 546 / ì›ì‘ì–¸ì–´Â·ë³¸ë¬¸ì–¸ì–´ ê°ì§€
# ============================================

# ------------------------------------------------------
# ì–¸ì–´ì½”ë“œ â†’ ìì—°ì–´ëª… (546 ìƒì„±ì— ì‚¬ìš©)
# ------------------------------------------------------
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´',
    'chi': 'ì¤‘êµ­ì–´', 'zho': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'fra': 'í”„ë‘ìŠ¤ì–´',
    'ger': 'ë…ì¼ì–´', 'deu': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´',
    'spa': 'ìŠ¤í˜ì¸ì–´', 'por': 'í¬ë¥´íˆ¬ê°ˆì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}


# ------------------------------------------------------
# ê¸°ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ 1ì°¨ ì–¸ì–´ ê°ì§€ â€” Safe Patch
# (ë¹ˆ ë¬¸ìì—´ / íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ì•ˆì •í™”)
# ------------------------------------------------------
def detect_language(text):
    if not text:
        return 'und'

    text = re.sub(r'[\s\W_]+', '', text or "")
    if not text:
        return 'und'
    
    ch = text[0]

    # í•œê¸€
    if '\uac00' <= ch <= '\ud7a3':
        return 'kor'
    # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜/ê°€íƒ€ì¹´ë‚˜
    if '\u3040' <= ch <= '\u30ff':
        return 'jpn'
    # ì¤‘êµ­ í•œì
    if '\u4e00' <= ch <= '\u9fff':
        return 'chi'
    # í‚¤ë¦´ ë¬¸ì
    if '\u0400' <= ch <= '\u04FF':
        return 'rus'
    # ì˜ì–´
    if 'a' <= ch.lower() <= 'z':
        return 'eng'

    return 'und'


# ------------------------------------------------------
# ìµœì¢… 546 ìƒì„±ê¸° â€” Safe Patch
# ------------------------------------------------------
def generate_546_from_041_kormarc(marc_041: str) -> str:
    """
    041ì˜ $a, $h ë¶„ì„í•´ì„œ ìì—°ì–´ ë¬¸ì¥ ìƒì„±
    """
    if not marc_041:
        return ""

    a_list, h_code = [], None
    parts = marc_041.split()

    for p in parts:
        if p.startswith("$a"):
            a_list.append(p[2:])
        elif p.startswith("$h"):
            h_code = p[2:]

    # ë³¸ë¬¸ ì–¸ì–´ 1ê°œ
    if len(a_list) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_list[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{h_lang} ì›ì‘ì„ {a_lang}ë¡œ ë²ˆì—­"
        return f"{a_lang}ë¡œ ì”€"

    # ë³¸ë¬¸ ì–¸ì–´ 2ê°œ ì´ìƒ
    if len(a_list) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(x, "ì•Œ ìˆ˜ ì—†ìŒ") for x in a_list]
        return "Â·".join(langs) + " ë³‘ê¸°"

    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"


# ------------------------------------------------------
# 041 ë¬¸ìì—´ì—ì„œ $a â†’ ì–¸ì–´ì½”ë“œ ì¶”ì¶œ
# ------------------------------------------------------
def _lang3_from_tag041(tag_041: str | None) -> str | None:
    if not tag_041:
        return None
    m = re.search(r"\$a([a-z]{3})", tag_041, flags=re.I)
    return m.group(1).lower() if m else None


# ------------------------------------------------------
# 041 ì›ì‘ì–¸ì–´($h) íŒŒì‹± (ë¬¸í•™ 8xx í›„ì²˜ë¦¬ì— ì‚¬ìš©)
# ------------------------------------------------------
def _parse_marc_041_original(marc041: str):
    if not marc041:
        return None
    s = str(marc041).lower()
    m = re.search(r"\$h([a-z]{3})", s)
    return m.group(1) if m else None


# ------------------------------------------------------
# ì›ì‘ì–¸ì–´ ê¸°ë°˜ ë¬¸í•™ ê³„ì—´ í—¤ë” ì¬ì •ë ¬
# ------------------------------------------------------
def _lang3_to_kdc_lit_base(lang3: str):
    if not lang3:
        return None
    l = lang3.lower()

    if l == "kor": return "810"
    if l in ("chi","zho"): return "820"
    if l == "jpn": return "830"
    if l == "eng": return "840"
    if l in ("ger","deu"): return "850"
    if l in ("fre","fra"): return "860"
    if l in ("spa","por"): return "870"
    if l == "ita": return "880"
    return "890"


def _rebase_8xx_with_language(code: str, marc041: str) -> str:
    if not code or len(code) < 3 or code[0] != "8":
        return code

    orig = _parse_marc_041_original(marc041 or "")
    base = _lang3_to_kdc_lit_base(orig) if orig else None
    if not base:
        return code

    m = re.match(r"^(\d{3})(\..+)?$", code)
    if not m:
        return code

    head3 = m.group(1)
    tail  = m.group(2) or ""
    genre = head3[2]

    new_head = base[:2] + genre
    return new_head + tail


# ------------------------------------------------------
# =041 / =546 MRK ë³€í™˜ê¸° (Safe Patch)
# ------------------------------------------------------
def _as_mrk_041(s: str | None) -> str:
    if not s:
        return None
    return s if s.startswith("=041") else f"=041  \\\\{s}"

def _as_mrk_546(s: str | None) -> str:
    if not s:
        return None
    return s if s.startswith("=546") else f"=546  \\\\{s}"



# ============================================
# PART 5 â€” GPT ê¸°ë°˜ 653 ìƒì„±ê¸° + ê¸ˆì¹™ì–´ í•„í„°ë§ (Safe Patch)
# ============================================

import openai

def _call_llm(system_prompt: str, user_prompt: str, max_tokens: int = 256):
    """
    GPT í˜¸ì¶œ â€” Safe Patch
    - ì˜ˆì™¸ ë°œìƒ ì‹œ None
    - prompt ì•ˆì „ì„± ê°œì„ 
    """
    try:
        client = openai.OpenAI(api_key=st.secrets["openai"]["api_key"])
        res = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            max_tokens=max_tokens,
            temperature=0.1,
        )
        return res.choices[0].message.content.strip()
    except Exception as e:
        dbg_err(f"[GPT] í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return None


# ---------------------------------------
# ê¸ˆì¹™ì–´ ì •ì˜
# ---------------------------------------
FORBIDDEN_WORDS = {
    "ì±…", "ë„ì„œ", "ì†Œì„¤", "ì‘í’ˆ", "ì €ì", "ì¶œíŒ", "ì¶œíŒì‚¬",
    "ì´ì•¼ê¸°", "ë‚´ìš©", "ë¬¸í•™", "ë¬¸í•™ì‘í’ˆ", "ëŒ€ìƒ", "ë…ì",
    "ì¥í¸ì†Œì„¤", "ë‹¨í¸ì†Œì„¤", "ê¸€", "ì‚°ë¬¸", "ë¸Œëœë“œ",
    "ì‹œë¦¬ì¦ˆ", "ì´ì„œ", "ê¶Œ", "í¸", "chapter", "index",
}


# ---------------------------------------
# í‚¤ì›Œë“œ ì •ê·œí™” (Safe Patch)
# ---------------------------------------
def _normalize_keyword(k: str):
    if not k:
        return ""
    k = k.strip().lower()
    k = re.sub(r"[^0-9a-zê°€-í£Â·\- ]+", "", k)
    return k


# ---------------------------------------
# GPT ë°˜í™˜ í‚¤ì›Œë“œ â†’ ë¦¬ìŠ¤íŠ¸
# ---------------------------------------
def _extract_keywords_from_gpt(raw: str) -> list[str]:
    if not raw:
        return []

    tokens = re.split(r"[,;\n]|Â·|\t|\|", raw)
    out = []

    for t in tokens:
        t = _normalize_keyword(t)
        if len(t) < 2:
            continue
        if t in FORBIDDEN_WORDS:
            continue
        out.append(t)

    uniq = []
    for x in out:
        if x not in uniq:
            uniq.append(x)

    return uniq[:7]


# ---------------------------------------
# GPT â†’ 653 MRK ë³€í™˜
# ---------------------------------------
def _keywords_to_653_mrk(keywords: list[str]):
    if not keywords:
        return None

    parts = [f"$a{w}" for w in keywords]
    return "=653  \\\\" + "".join(parts)


# ---------------------------------------
# 653 ìë™ ìƒì„±ê¸° (GPT)
# ---------------------------------------
def _build_653_via_gpt(item):
    if not item:
        return None

    # BookInfo dataclassë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
    title = item.title or ""
    desc  = item.description or ""
    cate  = item.category or ""
    toc   = item.toc or ""

    text_blob = (
        f"ì œëª©: {title}\n"
        f"ì¹´í…Œê³ ë¦¬: {cate}\n"
        f"ë‚´ìš©ìš”ì•½: {desc[:800]}\n"
        f"ëª©ì°¨: {toc[:500]}"
    )

    sys_prompt = (
        "ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ê³µê³µë„ì„œê´€ì˜ ì£¼ì œì „ë¬¸ ì‚¬ì„œë‹¤.\n"
        "ì…ë ¥ëœ ë„ì„œì •ë³´ë¥¼ ë¶„ì„í•´ KORMARC 653$aì— ë„£ì„ **êµ¬ì²´ì Â·ì‹¤ì§ˆì ** ì£¼ì œ í‚¤ì›Œë“œ 3~6ê°œë§Œ ì‚°ì¶œí•˜ë¼.\n"
        "ì¶œë ¥ì€ ì‰¼í‘œ êµ¬ë¶„ (ì˜ˆ: ì¸ê³µì§€ëŠ¥, ê¸°ê³„í•™ìŠµ, ë°ì´í„°ê³¼í•™)."
    )

    raw = _call_llm(sys_prompt, text_blob, max_tokens=80)
    if not raw:
        return None

    kws = _extract_keywords_from_gpt(raw)
    if not kws:
        return None

    return _keywords_to_653_mrk(kws)


# ---------------------------------------
# 653 â†’ KDC 056 íŒíŠ¸ íŒŒì‹±
# ---------------------------------------
def _parse_653_keywords(tag_653: str | None):
    if not tag_653:
        return []
    parts = re.findall(r"\$a([^$]+)", tag_653)
    out = []
    for p in parts:
        p = _normalize_keyword(p)
        if p:
            out.append(p)
    return out



# ============================================
# PART 6 â€” 056(KDC) ìë™ë¶„ë¥˜ ìƒì„±ê¸° (GPT 1íšŒ í˜¸ì¶œ)
# ============================================

# ------------------------------------------------------
# KDC ë¶„ë¥˜ ì…ë ¥ Payload êµ¬ì„± â€” Safe Patch
# ------------------------------------------------------
def _build_kdc_payload(info, keywords_hint):
    return {
        "title": info.title or "",
        "author": info.author or "",
        "publisher": info.publisher or "",
        "category": info.category or "",
        "description": info.description or "",
        "toc": clean_text(info.toc or ""),
        "keywords_hint": keywords_hint or [],
    }


# ------------------------------------------------------
# GPT ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (KDC ì „ë¬¸ê°€ ëª¨ë“œ)
# ------------------------------------------------------
KDC_SYSTEM_PROMPT = """
ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ê³µê³µë„ì„œê´€ ë¶„ë¥˜ì „ë¬¸ ì‚¬ì„œì´ë©°, KDC ì œ6íŒ ê·œì¹™ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•œë‹¤.

ì„ë¬´:
1) ì œê³µëœ ë„ì„œì •ë³´(title, author, category, description, toc, keywords_hint)ë¥¼ ë¶„ì„í•´
2) ê°€ì¥ ì í•©í•œ **KDC 3ìë¦¬ ì •ìˆ˜ 1ê°œ**ë§Œ ì‚°ì¶œí•œë‹¤ (ì˜ˆ: 370, 004, 823)
3) íŒë‹¨ì´ ì–´ë ¤ìš°ë©´ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'ë§Œ ì¶œë ¥í•œë‹¤.

ì¶œë ¥ í˜•ì‹:
- ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ **ì •ìˆ˜ 3ìë¦¬ ë˜ëŠ” ì§ì ‘ë¶„ë¥˜ì¶”ì²œ**ë§Œ ì¶œë ¥.
"""


# ------------------------------------------------------
# GPT ê¸°ë°˜ KDC ì½”ë“œ ìƒì„±ê¸° â€” Safe Patch
# ------------------------------------------------------
def ask_llm_for_kdc(info: 'BookInfo', api_key: str, model: str, keywords_hint=None):
    payload = _build_kdc_payload(info, keywords_hint)
    user_prompt = "ë„ì„œ ì •ë³´:\n" + json.dumps(payload, ensure_ascii=False, indent=2)

    try:
        client = openai.OpenAI(api_key=api_key)
        res = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": KDC_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            max_tokens=10,
            temperature=0.0,
        )
        code = res.choices[0].message.content.strip()
        dbg("[KDC GPT RAW]", code)
        return code

    except Exception as e:
        dbg_err(f"[KDC] GPT í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return None


# ------------------------------------------------------
# ISBN ê¸°ë°˜ ì „ì²´ KDC ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ â€” Safe Patch
# ------------------------------------------------------
def get_kdc_from_isbn(isbn13: str, ttbkey: str, openai_key: str, model: str,
                      keywords_hint: list[str] | None = None) -> str | None:

    # â‘  ì•Œë¼ë”˜ ê¸°ë³¸ ì •ë³´ í™•ë³´
    info = aladin_lookup_by_api(isbn13, ttbkey) if ttbkey else None
    if not info:
        info = aladin_lookup_by_web(isbn13)

    if not info:
        st.warning("âŒ ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨")
        return None

    # â‘¡ GPT í˜¸ì¶œ
    code = ask_llm_for_kdc(info, api_key=openai_key, model=model, keywords_hint=keywords_hint)

    if not code:
        return None

    if code == "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ":
        return "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"

    # ìˆ«ì ì—¬ë¶€ ê²€ì¦
    if re.fullmatch(r"\d{1,3}", code):
        return code.zfill(3)

    return None
# ============================================
# PART 7 â€” MARC ë¹Œë” (300 / 020 / 245 / 246 / 700 / 490 / 830 / 940 / 950 / 049 â€¦)
# ============================================

# =============================================================
# BookInfo Dataclass (ìµœì¢… ì •ìƒ ì •ì˜ë³¸)
# =============================================================
@dataclass
class BookInfo:
    title: str = ""
    author: str = ""
    publisher: str = ""
    pub_date: str = ""
    isbn13: str = ""
    category: str = ""
    description: str = ""
    toc: str = ""
    extra: dict = None


# =============================================================
# 300 í•„ë“œ â€” ì•Œë¼ë”˜ ìƒì„¸í˜ì´ì§€ ê¸°ë°˜ í˜•ì‚¬í•­ íŒŒì‹±
# =============================================================
def parse_aladin_physical_book_info(html):
    soup = BeautifulSoup(html, "html.parser")

    # ì œëª©Â·ë¶€ì œÂ·ì„¤ëª… (ì‚½í™” ê°ì§€ìš©)
    title = soup.select_one("span.Ere_bo_title")
    subtitle = soup.select_one("span.Ere_sub1_title")

    title_text    = clean_text(title.get_text()) if title else ""
    subtitle_text = clean_text(subtitle.get_text()) if subtitle else ""

    desc_tag = soup.select_one("div.Ere_prod_mconts_R")
    description = clean_text(desc_tag.get_text(" ")) if desc_tag else ""

    form_wrap = soup.select_one("div.conts_info_list1")

    a_part, b_part, c_part = "", "", ""
    page_value = None
    size_value = None

    if form_wrap:
        items = [x.strip() for x in form_wrap.stripped_strings if x.strip()]
        for it in items:
            # ìª½ìˆ˜
            if re.search(r"(ìª½|p)\s*$", it):
                m = re.search(r"\d+", it)
                if m:
                    page_value = int(m.group())
                    a_part = f"{page_value} p."

            # í¬ê¸° mm
            elif "mm" in it:
                m = re.search(r"(\d+)\s*[\*xÃ—X]\s*(\d+)", it)
                if m:
                    w = int(m.group(1))
                    h = int(m.group(2))
                    size_value = f"{w}x{h}mm"

                    # cm ê³„ì‚°
                    wcm = math.ceil(w / 10)
                    hcm = math.ceil(h / 10)
                    c_part = f"{wcm}x{hcm} cm"

    combined = " ".join([title_text, subtitle_text, description])
    has_illus, illus_label = detect_illustrations(combined)
    if has_illus:
        b_part = illus_label

    parts = []
    if a_part:
        part = f"$a{a_part}"
        if b_part:
            part += f" :$b{b_part}"
        parts.append(part)
    elif b_part:
        parts.append(f"$b{b_part}")

    if c_part:
        if parts:
            parts.append(f"; $c {c_part}")
        else:
            parts.append(f"$c {c_part}")

    if not parts:
        parts = ["$a1ì±…."]

    mrk = "=300  \\\\" + " ".join(parts)

    subfields = []
    if a_part: subfields.append(Subfield("a", a_part))
    if b_part: subfields.append(Subfield("b", b_part))
    if c_part: subfields.append(Subfield("c", c_part))

    return {
        "300": mrk,
        "300_subfields": subfields,
        "page_value": page_value,
        "size_value": size_value,
        "illustration_possibility": illus_label or "ì—†ìŒ"
    }


def search_aladin_detail_page(link):
    try:
        res = requests.get(link, timeout=15)
        res.raise_for_status()
        return parse_aladin_physical_book_info(res.text), None
    except Exception as e:
        return {
            "300": "=300  \\$a1ì±…. [ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜]",
            "300_subfields": [Subfield("a","1ì±… [íŒŒì‹± ì‹¤íŒ¨]")],
            "page_value": None,
            "size_value": None,
            "illustration_possibility": "ì •ë³´ ì—†ìŒ",
        }, str(e)


def build_300_from_aladin_detail(item: dict | BookInfo):
    """ item.extra.get("link") ê¸°ë°˜ """
    try:
        extra = item.extra if isinstance(item, BookInfo) else (item.get("extra") or {})
        link = extra.get("link", "")
        if not link:
            return "=300  \\$a1ì±….", Field(tag="300", indicators=[" "," "], subfields=[Subfield("a","1ì±….")])

        info, err = search_aladin_detail_page(link)
        mrk = info["300"]
        subs = info["300_subfields"]

        f300 = Field(tag="300", indicators=[" "," "], subfields=subs)
        if err:
            dbg_err("[300]", err)
        return mrk, f300

    except Exception as e:
        dbg_err(f"[300 Exception] {e}")
        return "=300  \\$a1ì±….[ì˜ˆì™¸]", Field(tag="300", indicators=[" "," "], subfields=[Subfield("a","1ì±….[ì˜ˆì™¸]")])



# =============================================================
# ì´ì„œ 490 / 830
# =============================================================
def build_490_830_mrk_from_item(item):
    si = None
    if isinstance(item, BookInfo):
        si = item.extra.get("seriesInfo") if item.extra else None
        if si is None:
            si = item.extra.get("subInfo", {}).get("seriesInfo") if item.extra else None
    else:
        si = item.get("seriesInfo") or (item.get("subInfo") or {}).get("seriesInfo")

    cand = []
    if isinstance(si, list):
        cand = si
    elif isinstance(si, dict):
        cand = [si]

    sname, svol = "", ""
    for ent in cand:
        if not isinstance(ent, dict):
            continue
        name = (ent.get("seriesName") or ent.get("name") or "").strip()
        vol  = (ent.get("volume") or "").strip()
        if name:
            sname, svol = name, vol
            break

    if not sname:
        return "", ""

    display = f"{sname} {svol}".strip()
    tag_490 = f"=490  10$a{display}"
    tag_830 = f"=830  \\0$a{display}"
    return tag_490, tag_830



# =============================================================
# ê°€ê²© / ISBN â†’ 020
# =============================================================
def _extract_price_kr(item, isbn):
    price = 0
    try:
        extra = item.extra if isinstance(item,BookInfo) else (item.get("extra") or {})
        if "priceStandard" in extra:
            price = int(extra["priceStandard"])
        elif "priceSales" in extra:
            price = int(extra["priceSales"])
    except:
        pass
    return price


def _build_020_from_item_and_nlk(isbn, item):
    price = _extract_price_kr(item, isbn)
    if price:
        return f"=020  \\\\$a{isbn} :$c{price}"
    return f"=020  \\\\$a{isbn}"


# =============================================================
# 950 (ê°€ê²©)
# =============================================================
def build_950_from_item_and_price(item, isbn):
    price = _extract_price_kr(item, isbn)
    if price:
        return f"=950  \\\\$a{price}"
    return "=950  \\\\$aë¯¸ìƒ"



# =============================================================
# 245 ì„œëª… (ì±…ì œëª©Â·ë¶€ì œÂ·ì±…ì„í‘œì‹œ)
# =============================================================
def build_245_with_people_from_sources(item, nlk_author_raw, prefer="aladin"):
    title = clean_text(item.title)
    subtitle = ""
    if isinstance(item, BookInfo):
        if item.extra and "subInfo" in item.extra:
            subtitle = clean_text(item.extra.get("subInfo", {}).get("subTitle") or "")
    else:
        subtitle = clean_text((item.get("subInfo") or {}).get("subTitle") or "")

    author  = clean_text(item.author)
    year    = clean_text(item.pub_date)[:4]

    c_part = author
    if year:
        c_part += f" ({year})"

    out = "=245  00"

    if subtitle:
        out += f"$a{title} :$b{subtitle}"
    else:
        out += f"$a{title}"

    if c_part:
        out += f" /$c{c_part}"

    return out



# =============================================================
# 246 â€” ì›ì œ(ëŒ€ë“±ì„œëª…)
# =============================================================
def build_246_from_aladin_item(item):
    orig = None
    if isinstance(item, BookInfo):
        orig = (item.extra or {}).get("originalTitle")
    else:
        orig = (item.get("extra") or {}).get("originalTitle")

    if not orig:
        return ""
    orig = clean_text(orig)

    return f"=246  31$a{orig}"



# =============================================================
# 700 â€” ì¸ëª… ì ‘ê·¼ì 
# =============================================================
def build_700_people_pref_aladin(nlk_author_raw, item, origin_lang_code=None):
    authors = clean_text(nlk_author_raw or item.author or "")
    if not authors:
        return []

    tokens = re.split(r",|;|/|Â·|\s", authors)
    tokens = [t.strip() for t in tokens if t.strip()]

    out = []
    for t in tokens:
        if not t:
            continue

        if origin_lang_code in ("eng", "fre", "ger", "spa", "rus", "ita"):
            parts = t.split()
            if len(parts) >= 2:
                lname = parts[-1]
                fname = " ".join(parts[:-1])
                name_form = f"{lname}, {fname}"
            else:
                name_form = t
        else:
            name_form = t

        out.append(f"=700  1\\$a{name_form}")

    return out



# =============================================================
# 940 â€” ì œëª© ê¸°ë°˜ ë¶„ë¥˜ê¸°
# =============================================================
def parse_245_a_n(marc245: str):
    if not marc245:
        return "", None
    m = re.search(r"\$a([^$]+)", marc245)
    a = clean_text(m.group(1)) if m else ""
    n = re.search(r"\b(\d+)\b", a)
    return a, (n.group(1) if n else None)


def build_940_from_title_a(title_a: str, use_ai=True, disable_number_reading=False):
    if disable_number_reading:
        title_clean = re.sub(r"\d+", "", title_a)
    else:
        title_clean = title_a

    if not title_clean:
        return []

    field = f"=940  \\\\$a{title_clean}"
    return [field]



# =============================================================
# 049 â€” ë“±ë¡ê¸°í˜¸
# =============================================================
def build_049(reg_mark, reg_no, copy_symbol):
    if not reg_mark and not reg_no:
        return ""
    body = f"$a{reg_mark}{reg_no}"
    if copy_symbol:
        body += f"$c{copy_symbol}"
    return f"=049  \\\\{body}"



# =============================================================
# MRK ë¬¸ìì—´ â†’ pymarc Field ë³€í™˜ê¸°
# =============================================================
def mrk_str_to_field(line):
    if not line:
        return None

    s = line.strip()
    if not s.startswith("=") or len(s) < 6:
        return None

    # ì»¨íŠ¸ë¡¤í•„ë“œ
    if re.match(r"^=\d{3}\s\s[^$]+$", s) and int(s[1:4]) < 10:
        tag = s[1:4]
        data = s[6:]
        return Field(tag=tag, data=data)

    m = re.match(r"^=(\d{3})\s{2}(.)(.)(.*)$", s)
    if not m:
        return None

    tag, ind1_raw, ind2_raw, tail = m.groups()
    ind1 = " " if ind1_raw == "\\" else ind1_raw
    ind2 = " " if ind2_raw == "\\" else ind2_raw

    subfields = []
    parts = re.split(r"(\$[a-zA-Z])", tail)
    cur_code = None
    buf = []

    for p in parts:
        if not p:
            continue
        if p.startswith("$") and len(p) == 2:
            if cur_code and buf:
                subfields.append(Subfield(cur_code, "".join(buf).strip()))
            cur_code = p[1]
            buf = []
        else:
            buf.append(p)

    if cur_code and buf:
        subfields.append(Subfield(cur_code, "".join(buf).strip()))

    return Field(tag=tag, indicators=[ind1, ind2], subfields=subfields)



# ============================================
# PART 8 â€” generate_all_oneclick / run_and_export / Streamlit UI
# ============================================

# ------------------------------------------------------
# ë©”ì¸ ì—”ì§„ â€” ë‹¨ì¼ ISBN ê¸°ë°˜ ì „ì²´ MARC ìƒì„±
# ------------------------------------------------------
def generate_all_oneclick(
    isbn: str,
    reg_mark: str = "",
    reg_no: str = "",
    copy_symbol: str = "",
    use_ai_940: bool = True,
):
    global CURRENT_DEBUG_LINES
    CURRENT_DEBUG_LINES = []

    record = Record(to_unicode=True, force_utf8=True)
    pieces = []

    # --------------------------
    # â‘  ì €ì (NLK)
    # --------------------------
    author_raw, _ = fetch_nlk_author_only(isbn)

    # --------------------------
    # â‘¡ ì•Œë¼ë”˜ item
    # --------------------------
    item = aladin_lookup_by_api(isbn, ALADIN_TTB_KEY)
    if not item:
        item = aladin_lookup_by_web(isbn)

    if not item:
        st.error("âŒ ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return record, b"", "", {}

    # --------------------------
    # â‘¢ 041 / 546
    # --------------------------
    original_title = item.extra.get("originalTitle") if item.extra else ""
    lang_main = detect_language(item.title)
    lang_orig = detect_language(original_title) if original_title else None

    if original_title:
        tag_041_text = f"$a{lang_main}$h{lang_orig}"
    else:
        tag_041_text = f"$a{lang_main}"

    tag_546_text = generate_546_from_041_kormarc(tag_041_text)

    f_041 = mrk_str_to_field(_as_mrk_041(tag_041_text))
    f_546 = mrk_str_to_field(_as_mrk_546(tag_546_text))

    if f_041:
        pieces.append((f_041, _as_mrk_041(tag_041_text)))
    if f_546:
        pieces.append((f_546, _as_mrk_546(tag_546_text)))

    origin_lang = _parse_marc_041_original(tag_041_text)

    # --------------------------
    # â‘£ 245 / 246 / 700
    # --------------------------
    marc245 = build_245_with_people_from_sources(item, author_raw)
    f_245 = mrk_str_to_field(marc245)

    marc246 = build_246_from_aladin_item(item)
    f_246 = mrk_str_to_field(marc246)

    mrk_700 = build_700_people_pref_aladin(author_raw, item, origin_lang)

    # --------------------------
    # â‘¤ ì´ì„œ (490 / 830)
    # --------------------------
    tag_490, tag_830 = build_490_830_mrk_from_item(item)
    f_490 = mrk_str_to_field(tag_490) if tag_490 else None
    f_830 = mrk_str_to_field(tag_830) if tag_830 else None

    # --------------------------
    # â‘¥ 300
    # --------------------------
    tag_300, f_300 = build_300_from_aladin_detail(item)

    # --------------------------
    # â‘¦ ë°œí–‰ì§€ + 260
    # --------------------------
    publisher_raw = item.publisher or ""
    pubdate       = item.pub_date or ""
    pubyear       = pubdate[:4] if len(pubdate) >= 4 else ""

    bundle = build_pub_location_bundle(isbn, publisher_raw)
    tag_260 = build_260(
        place_display=bundle.get("place_display"),
        publisher_name=publisher_raw,
        pubyear=pubyear,
    )
    f_260 = mrk_str_to_field(tag_260)

    # --------------------------
    # â‘§ 008
    # --------------------------
    data_008 = build_008_from_isbn(
        isbn,
        aladin_pubdate=pubdate,
        aladin_title=item.title,
        aladin_category=item.category,
        aladin_desc=item.description,
        aladin_toc=item.toc,
        override_country3=bundle.get("country_code"),
        override_lang3=_lang3_from_tag041(tag_041_text),
        cataloging_src="a",
    )
    f_008 = Field(tag="008", data=data_008)

    # --------------------------
    # â‘¨ 007
    # --------------------------
    f_007 = Field(tag="007", data="ta")

    # --------------------------
    # â‘© 020 / 950
    # --------------------------
    tag_020 = _build_020_from_item_and_nlk(isbn, item)
    f_020 = mrk_str_to_field(tag_020)

    tag_950 = build_950_from_item_and_price(item, isbn)
    f_950 = mrk_str_to_field(tag_950)

    # --------------------------
    # â‘ª 653 (GPT)
    # --------------------------
    tag_653 = _build_653_via_gpt(item)
    f_653 = mrk_str_to_field(tag_653) if tag_653 else None

    # --------------------------
    # â‘« 056 (GPT-KDC)
    # --------------------------
    kw_hint = _parse_653_keywords(tag_653) if tag_653 else []
    kdc_code = get_kdc_from_isbn(
        isbn,
        ttbkey=ALADIN_TTB_KEY,
        openai_key=st.secrets["openai"]["api_key"],
        model="gpt-4o",
        keywords_hint=kw_hint,
    )
    tag_056 = f"=056  \\\\$a{kdc_code}$26" if kdc_code else None
    f_056 = mrk_str_to_field(tag_056)

    # --------------------------
    # â‘¬ 940
    # --------------------------
    a_out, n = parse_245_a_n(marc245)
    mrk_940 = build_940_from_title_a(a_out, use_ai=use_ai_940, disable_number_reading=bool(n))

    # --------------------------
    # â‘­ 049
    # --------------------------
    tag_049 = build_049(reg_mark, reg_no, copy_symbol)
    f_049 = mrk_str_to_field(tag_049) if tag_049 else None

    # ------------------------------------------------------
    # ì¡°ë¦½
    # ------------------------------------------------------
    def add(field_obj, mrk_str):
        if field_obj and mrk_str:
            pieces.append((field_obj, mrk_str))

    add(f_008, f"=008  {data_008}")
    add(f_007, "=007  ta")
    add(f_020, tag_020)
    add(f_056, tag_056)
    add(f_245, marc245)
    add(f_246, marc246)
    add(f_260, tag_260)
    add(f_300, tag_300)
    add(f_490, tag_490)
    add(f_546, _as_mrk_546(tag_546_text))
    add(f_653, tag_653)

    # 700/940/830/950/049
    for m in mrk_700:
        add(mrk_str_to_field(m), m)
    for m in mrk_940:
        add(mrk_str_to_field(m), m)
    add(f_830, tag_830)
    add(f_950, tag_950)
    add(f_049, tag_049)

    # ------------------------------------------------------
    # MRK í…ìŠ¤íŠ¸
    # ------------------------------------------------------
    mrk_strings = [m for _, m in pieces]
    mrk_text = "\n".join(mrk_strings)

    # pymarc record ì¡°ë¦½
    for f, _ in pieces:
        record.add_field(f)

    marc_bytes = record.as_marc()

    meta = {
        "041": tag_041_text,
        "546": tag_546_text,
        "056": tag_056,
        "653": tag_653,
        "kdc_code": kdc_code,
        "Publisher_raw": publisher_raw,
        "Place_display": bundle.get("place_display"),
        "CountryCode_008": bundle.get("country_code"),
        "debug_lines": CURRENT_DEBUG_LINES.copy(),
    }

    return record, marc_bytes, mrk_text, meta



# ------------------------------------------------------
# run_and_export (íŒŒì¼ ì €ì¥ + Streamlit ë¯¸ë¦¬ë³´ê¸°)
# ------------------------------------------------------
def save_marc_files(record: Record, save_dir: str, base_filename: str):
    import os
    os.makedirs(save_dir, exist_ok=True)

    mrc_path = os.path.join(save_dir, f"{base_filename}.mrc")
    mrk_path = os.path.join(save_dir, f"{base_filename}.mrk")

    with open(mrc_path, "wb") as f:
        f.write(record.as_marc())

    mrk_text = record_to_mrk_from_record(record)
    with open(mrk_path, "w", encoding="utf-8") as f:
        f.write(mrk_text)

    return mrc_path, mrk_path



def run_and_export(
    isbn: str,
    *,
    reg_mark: str = "",
    reg_no: str = "",
    copy_symbol: str = "",
    use_ai_940: bool = True,
    save_dir: str = "./output",
    preview_in_streamlit: bool = True,
):
    record, marc_bytes, mrk_text, meta = generate_all_oneclick(
        isbn,
        reg_mark=reg_mark,
        reg_no=reg_no,
        copy_symbol=copy_symbol,
        use_ai_940=use_ai_940,
    )

    save_marc_files(record, save_dir, isbn)

    if preview_in_streamlit:
        st.success("ğŸ“¦ MRC/MRK íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        with st.expander("MRK ë¯¸ë¦¬ë³´ê¸°", expanded=True):
            st.text_area("MRK", mrk_text, height=320)

        st.download_button(
            "ğŸ“˜ MARC (mrc) ë‹¤ìš´ë¡œë“œ",
            data=marc_bytes,
            file_name=f"{isbn}.mrc",
            mime="application/marc",
        )
        st.download_button(
            "ğŸ§¾ MARC (mrk) ë‹¤ìš´ë¡œë“œ",
            data=mrk_text,
            file_name=f"{isbn}.mrk",
            mime="text/plain",
        )

    return record, marc_bytes, mrk_text, meta



# ============================================
# Streamlit UI
# ============================================

st.header("ğŸ“š ISBN â†’ MARC ìë™ ìƒì„±ê¸° (Safe Patch Version)")

with st.form("isbn_form"):
    isbn_single = st.text_input("ğŸ”¹ ë‹¨ì¼ ISBN ì…ë ¥", placeholder="ì˜ˆ: 9788937462849")
    csv_file = st.file_uploader(
        "ğŸ“ CSV ì—…ë¡œë“œ (ì—´: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)",
        type=["csv"]
    )
    submitted = st.form_submit_button("ğŸš€ ë³€í™˜ ì‹¤í–‰")

if submitted:
    jobs = []

    if isbn_single.strip():
        jobs.append([isbn_single.strip(), "", "", ""])

    if csv_file:
        df = pd.read_csv(csv_file)
        need_cols = {"ISBN", "ë“±ë¡ê¸°í˜¸", "ë“±ë¡ë²ˆí˜¸", "ë³„ì¹˜ê¸°í˜¸"}
        if not need_cols.issubset(df.columns):
            st.error("âŒ CSVì— í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        rows = df[["ISBN","ë“±ë¡ê¸°í˜¸","ë“±ë¡ë²ˆí˜¸","ë³„ì¹˜ê¸°í˜¸"]].fillna("")
        for row in rows.itertuples(index=False):
            jobs.append(list(row))

    if not jobs:
        st.warning("ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    st.write(f"ì´ {len(jobs)}ê±´ ì²˜ë¦¬ ì¤‘â€¦")
    prog = st.progress(0)

    results = []
    marc_all_texts = []

    for i, (isbn, mark, no, copy) in enumerate(jobs, start=1):
        record, mrc_bytes, mrk_text, meta = run_and_export(
            isbn,
            reg_mark=mark,
            reg_no=no,
            copy_symbol=copy,
            use_ai_940=True,
            save_dir="./output",
            preview_in_streamlit=True,
        )

        st.caption(f"ISBN {isbn} â€” 056={meta.get('kdc_code')}, 653={meta.get('653')}")
        marc_all_texts.append(mrk_text)
        results.append((record, isbn, mrk_text))

        prog.progress(i / len(jobs))

    st.download_button(
        "ğŸ“¦ ì „ì²´ MRK í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
        data="\n\n".join(marc_all_texts).encode("utf-8-sig"),
        file_name="marc_output_all.txt",
        mime="text/plain",
    )

    # ì „ì²´ MRC ë¬¶ìŒ
    buf = io.BytesIO()
    writer = MARCWriter(buf)
    for record, isbn, _ in results:
        writer.write(record)
    buf.seek(0)

    st.download_button(
        "ğŸ“¥ ì „ì²´ MRC ë‹¤ìš´ë¡œë“œ",
        data=buf,
        file_name="marc_output_all.mrc",
        mime="application/octet-stream",
    )
