import re
import os
import streamlit as st
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI

# ===== í™˜ê²½ë³€ìˆ˜ ë¡œë“œ =====
load_dotenv()
ALADIN_KEY = os.getenv("ALADIN_TTB_KEY")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_KEY)

# ===== ISDS ì–¸ì–´ì½”ë“œ ë§¤í•‘ =====
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´',
    'rus': 'ëŸ¬ì‹œì•„ì–´', 'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´',
    'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´', 'por': 'í¬ë¥´íˆ¬ê°ˆì–´', 'tur': 'í„°í‚¤ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}
ALLOWED_CODES = set(ISDS_LANGUAGE_CODES.keys()) - {"und"}

# ===== ê³µí†µ ìœ í‹¸: GPT ì‘ë‹µ íŒŒì‹±(ì½”ë“œ + ì´ìœ ) =====
def _extract_code_and_reason(content, code_key="$h"):
    code, reason, signals = "und", "", ""
    lines = [l.strip() for l in (content or "").splitlines() if l.strip()]
    for ln in lines:
        if ln.startswith(f"{code_key}="):
            code = ln.split("=", 1)[1].strip()
        elif ln.lower().startswith("#reason="):
            reason = ln.split("=", 1)[1].strip()
        elif ln.lower().startswith("#signals="):
            signals = ln.split("=", 1)[1].strip()
    return code, reason, signals

# ===== GPT íŒë‹¨ í•¨ìˆ˜ (ì›ì„œ; ì¼ë°˜) =====
def gpt_guess_original_lang(title, category, publisher, author="", original_title=""):
    prompt = f"""
    ì•„ë˜ ë„ì„œì˜ ì›ì„œ ì–¸ì–´(041 $h)ë¥¼ ISDS ì½”ë“œë¡œ ì¶”ì •í•´ì¤˜.
    ê°€ëŠ¥í•œ ì½”ë“œ: kor, eng, jpn, chi, rus, fre, ger, ita, spa, por, tur

    ë„ì„œì •ë³´:
    - ì œëª©: {title}
    - ì›ì œ: {original_title or "(ì—†ìŒ)"}
    - ë¶„ë¥˜: {category}
    - ì¶œíŒì‚¬: {publisher}
    - ì €ì: {author}

    ì§€ì¹¨:
    - êµ­ê°€/ì§€ì—­ì„ ì–¸ì–´ë¡œ ê³§ë°”ë¡œ ì¹˜í™˜í•˜ì§€ ë§ ê²ƒ.
    - ì €ì êµ­ì Â·ì£¼ ì§‘í•„ ì–¸ì–´Â·ìµœì´ˆ ì¶œê°„ ì–¸ì–´ë¥¼ ìš°ì„  ê³ ë ¤.
    - ë¶ˆí™•ì‹¤í•˜ë©´ ì„ì˜ ì¶”ì • ëŒ€ì‹  'und' ì‚¬ìš©.

    ì¶œë ¥í˜•ì‹(ì •í™•íˆ ì´ 2~3ì¤„):
    $h=[ISDS ì½”ë“œ]
    #reason=[ì§§ê²Œ ê·¼ê±° ìš”ì•½]
    #signals=[ì¡ì€ ë‹¨ì„œë“¤, ì½¤ë§ˆë¡œ](ì„ íƒ)
    """.strip()
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system","content":"ì‚¬ì„œìš© ì–¸ì–´ ì¶”ì •ê¸°"},
                      {"role":"user","content":prompt}],
            temperature=0
        )
        content = (resp.choices[0].message.content or "").strip()
        code, reason, signals = _extract_code_and_reason(content, "$h")
        if code not in ALLOWED_CODES:
            code = "und"
        st.write(f"ğŸ§­ [GPT ê·¼ê±°] $h={code}")
        if reason: st.write(f"ğŸ§­ [ì´ìœ ] {reason}")
        if signals: st.write(f"ğŸ§­ [ë‹¨ì„œ] {signals}")
        return code
    except Exception as e:
        st.error(f"GPT ì˜¤ë¥˜: {e}")
        return "und"

# ===== GPT íŒë‹¨ í•¨ìˆ˜ (ë³¸ë¬¸) =====
def gpt_guess_main_lang(title, category, publisher):
    prompt = f"""
    ì•„ë˜ ë„ì„œì˜ ë³¸ë¬¸ ì–¸ì–´(041 $a)ë¥¼ ISDS ì½”ë“œë¡œ ì¶”ì •.
    ê°€ëŠ¥í•œ ì½”ë“œ: kor, eng, jpn, chi, rus, fre, ger, ita, spa, por, tur

    ì…ë ¥:
    - ì œëª©: {title}
    - ë¶„ë¥˜: {category}
    - ì¶œíŒì‚¬: {publisher}

    ì§€ì¹¨:
    - 'ë³¸ë¬¸ ì–¸ì–´'ëŠ” ì´ ìë£Œì˜ **í˜„ì‹œë³¸(Manifestation)** ì–¸ì–´ë‹¤.
    - ì €ì êµ­ì , ì›ì‘ ì–¸ì–´, ì‹œë¦¬ì¦ˆ ì›ì‚°ì§€ ë“± **ì›ì‘ ê´€ë ¨ ë‹¨ì„œ ì‚¬ìš© ê¸ˆì§€**.
    - ì¹´í…Œê³ ë¦¬ì— 'êµ­ë‚´ë„ì„œ'ê°€ ìˆê±°ë‚˜, ì œëª©ì— **í•œê¸€ì´ 1ìë¼ë„** í¬í•¨ë˜ë©´ ë°˜ë“œì‹œ kor.
    - í—ˆìš© ì½”ë“œ ë°–ì´ê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ë©´ 'und'.

    ì¶œë ¥í˜•ì‹:
    $a=[ISDS ì½”ë“œ]
    #reason=[ì§§ê²Œ ê·¼ê±° ìš”ì•½]
    #signals=[ì¡ì€ ë‹¨ì„œë“¤, ì½¤ë§ˆë¡œ](ì„ íƒ)
    """.strip()
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system","content":"ì‚¬ì„œìš© ë³¸ë¬¸ ì–¸ì–´ ì¶”ì •ê¸°"},
                      {"role":"user","content":prompt}],
            temperature=0
        )
        content = (resp.choices[0].message.content or "").strip()
        code, reason, signals = _extract_code_and_reason(content, "$a")
        if code not in ALLOWED_CODES:
            code = "und"
        st.write(f"ğŸ§­ [GPT ê·¼ê±°] $a={code}")
        if reason: st.write(f"ğŸ§­ [ì´ìœ ] {reason}")
        if signals: st.write(f"ğŸ§­ [ë‹¨ì„œ] {signals}")
        return code
    except Exception as e:
        st.error(f"GPT ì˜¤ë¥˜: {e}")
        return "und"

# ===== GPT íŒë‹¨ í•¨ìˆ˜ (ì‹ ê·œ) â€” ì €ì ê¸°ë°˜ ì›ì„œ ì–¸ì–´ ì¶”ì • =====
def gpt_guess_original_lang_by_author(author, title="", category="", publisher=""):
    prompt = f"""
    ì €ì ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì›ì„œ ì–¸ì–´(041 $h)ë¥¼ ISDS ì½”ë“œë¡œ ì¶”ì •.
    ê°€ëŠ¥í•œ ì½”ë“œ: kor, eng, jpn, chi, rus, fre, ger, ita, spa, por, tur

    ì…ë ¥:
    - ì €ì: {author}
    - (ì°¸ê³ ) ì œëª©: {title}
    - (ì°¸ê³ ) ë¶„ë¥˜: {category}
    - (ì°¸ê³ ) ì¶œíŒì‚¬: {publisher}

    ì§€ì¹¨:
    - ì €ì êµ­ì Â·ì£¼ ì§‘í•„ ì–¸ì–´Â·ëŒ€í‘œ ì‘í’ˆ ì›ì–´ë¥¼ ìš°ì„ .
    - êµ­ê°€=ì–¸ì–´ ë‹¨ìˆœ ì¹˜í™˜ ê¸ˆì§€.
    - ë¶ˆí™•ì‹¤í•˜ë©´ 'und'.

    ì¶œë ¥í˜•ì‹:
    $h=[ISDS ì½”ë“œ]
    #reason=[ì§§ê²Œ ê·¼ê±° ìš”ì•½]
    #signals=[ì¡ì€ ë‹¨ì„œë“¤, ì½¤ë§ˆë¡œ](ì„ íƒ)
    """.strip()
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role":"system","content":"ì €ì ê¸°ë°˜ ì›ì„œ ì–¸ì–´ ì¶”ì •ê¸°"},
                      {"role":"user","content":prompt}],
            temperature=0
        )
        content = (resp.choices[0].message.content or "").strip()
        code, reason, signals = _extract_code_and_reason(content, "$h")
        if code not in ALLOWED_CODES:
            code = "und"
        st.write(f"ğŸ§­ [ì €ìê¸°ë°˜ ê·¼ê±°] $h={code}")
        if reason: st.write(f"ğŸ§­ [ì´ìœ ] {reason}")
        if signals: st.write(f"ğŸ§­ [ë‹¨ì„œ] {signals}")
        return code
    except Exception as e:
        st.error(f"GPT(ì €ìê¸°ë°˜) ì˜¤ë¥˜: {e}")
        return "und"

# ===== ì–¸ì–´ ê°ì§€ í•¨ìˆ˜ë“¤ =====
def detect_language_by_unicode(text):
    text = re.sub(r'[\s\W_]+', '', text or "")
    if not text:
        return 'und'
    c = text[0]
    if '\uac00' <= c <= '\ud7a3': return 'kor'
    if '\u3040' <= c <= '\u30ff': return 'jpn'
    if '\u4e00' <= c <= '\u9fff': return 'chi'
    if '\u0600' <= c <= '\u06FF': return 'ara'
    if '\u0e00' <= c <= '\u0e7f': return 'tha'
    return 'und'

def override_language_by_keywords(text, initial_lang):
    text = (text or "").lower()
    if initial_lang == 'chi' and re.search(r'[\u3040-\u30ff]', text): return 'jpn'
    if initial_lang in ['und', 'eng']:
        if "spanish" in text or "espaÃ±ol" in text: return "spa"
        if "italian" in text or "italiano" in text: return "ita"
        if "french" in text or "franÃ§ais" in text: return "fre"
        if "portuguese" in text or "portuguÃªs" in text: return "por"
        if "german" in text or "deutsch" in text: return "ger"
        if any(ch in text for ch in ['Ã©','Ã¨','Ãª','Ã ','Ã§','Ã¹','Ã´','Ã¢','Ã®','Ã»']): return "fre"
        if any(ch in text for ch in ['Ã±','Ã¡','Ã­','Ã³','Ãº']): return "spa"
        if any(ch in text for ch in ['Ã£','Ãµ']): return "por"
    return initial_lang

def detect_language(text):
    lang = detect_language_by_unicode(text)
    return override_language_by_keywords(text, lang)

def detect_language_from_category(text):
    words = re.split(r'[>/\s]+', text or "")
    for w in words:
        if "ì¼ë³¸" in w: return "jpn"
        if "ì¤‘êµ­" in w: return "chi"
        if "ì˜ë¯¸" in w or "ì˜ì–´" in w or "ì•„ì¼ëœë“œ" in w: return "eng"
        if "í”„ë‘ìŠ¤" in w: return "fre"
        if "ë…ì¼" in w or "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„" in w: return "ger"
        if "ëŸ¬ì‹œì•„" in w: return "rus"
        if "ì´íƒˆë¦¬ì•„" in w: return "ita"
        if "ìŠ¤í˜ì¸" in w: return "spa"
        if "í¬ë¥´íˆ¬ê°ˆ" in w: return "por"
        if "íŠ€ë¥´í‚¤ì˜ˆ" in w or "í„°í‚¤" in w: return "tur"
    return None

# ===== ì¹´í…Œê³ ë¦¬ í† í¬ë‚˜ì´ì¦ˆ & íŒì • ìœ í‹¸ =====
def tokenize_category(text: str):
    if not text:
        return []
    t = re.sub(r'[()]+', ' ', text)
    raw = re.split(r'[>/\s]+', t)
    tokens = []
    for w in raw:
        w = w.strip()
        if not w:
            continue
        if '/' in w and w.count('/') <= 3 and len(w) <= 20:
            tokens.extend([p for p in w.split('/') if p])
        else:
            tokens.append(w)
    lower_tokens = tokens + [w.lower() for w in tokens if any('A'<=ch<='Z' or 'a'<=ch<='z' for ch in w)]
    return lower_tokens

def has_kw_token(tokens, kws):
    s = set(tokens)
    return any(k in s for k in kws)

def trigger_kw_token(tokens, kws):
    s = set(tokens)
    for k in kws:
        if k in s:
            return k
    return None

def is_literature_top(category_text: str) -> bool:
    return "ì†Œì„¤/ì‹œ/í¬ê³¡" in (category_text or "")

def is_literature_category(category_text: str) -> bool:
    tokens = tokenize_category(category_text or "")
    ko_hits = ["ë¬¸í•™", "ì†Œì„¤", "ì‹œ", "í¬ê³¡"]
    en_hits = ["literature", "fiction", "novel", "poetry", "poem", "drama", "play"]
    return has_kw_token(tokens, ko_hits) or has_kw_token(tokens, en_hits)

def is_nonfiction_override(category_text: str) -> bool:
    """
    ë¬¸í•™ì²˜ëŸ¼ ë³´ì—¬ë„ 'ì—­ì‚¬/ì§€ì—­/ì „ê¸°/ì‚¬íšŒê³¼í•™/ì—ì„¸ì´' ë“± ë¹„ë¬¸í•™ ì§€í‘œê°€ ìˆìœ¼ë©´ ë¹„ë¬¸í•™ìœ¼ë¡œ ê°•ì œ.
    ë‹¨, ë¬¸í•™ ìµœìƒìœ„(ì†Œì„¤/ì‹œ/í¬ê³¡)ë©´ 'ê³¼í•™/ê¸°ìˆ 'ì€ ì œì™¸(SF ë³´í˜¸).
    """
    tokens = tokenize_category(category_text or "")
    lit_top = is_literature_top(category_text or "")

    ko_nf_strict = ["ì—­ì‚¬","ê·¼í˜„ëŒ€ì‚¬","ì„œì–‘ì‚¬","ìœ ëŸ½ì‚¬","ì „ê¸°","í‰ì „",
                    "ì‚¬íšŒ","ì •ì¹˜","ì² í•™","ê²½ì œ","ê²½ì˜","ì¸ë¬¸","ì—ì„¸ì´","ìˆ˜í•„"]
    en_nf_strict = ["history","biography","memoir","politics","philosophy",
                    "economics","science","technology","nonfiction","essay","essays"]

    sci_keys = ["ê³¼í•™","ê¸°ìˆ "]; sci_keys_en = ["science","technology"]

    k = trigger_kw_token(tokens, ko_nf_strict) or trigger_kw_token(tokens, en_nf_strict)
    if k:
        st.write(f"ğŸ” [íŒì •ê·¼ê±°] ë¹„ë¬¸í•™ í‚¤ì›Œë“œ ë°œê²¬: '{k}'")
        return True

    if not lit_top:
        k2 = trigger_kw_token(tokens, sci_keys) or trigger_kw_token(tokens, sci_keys_en)
        if k2:
            st.write(f"ğŸ” [íŒì •ê·¼ê±°] ë¹„ë¬¸í•™ ìµœìƒìœ„ ì¶”ì • & '{k2}' ë°œê²¬ â†’ ë¹„ë¬¸í•™ ì˜¤ë²„ë¼ì´ë“œ")
            return True

    if lit_top:
        st.write("ğŸ” [íŒì •ê·¼ê±°] ë¬¸í•™ ìµœìƒìœ„ ê°ì§€: 'ê³¼í•™/ê¸°ìˆ 'ì€ ì˜¤ë²„ë¼ì´ë“œì—ì„œ ì œì™¸(SF ë³´í˜¸).")
    return False

# ===== ê¸°íƒ€ ìœ í‹¸ =====
def strip_ns(tag): return tag.split('}')[-1] if '}' in tag else tag

def generate_546_from_041_kormarc(marc_041):
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"): a_codes.append(part[2:])
        elif part.startswith("$h"): h_code = part[2:]
    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{h_lang}ì›ì‘ì„ {a_lang}ë¡œ ë²ˆì—­"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"
    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"

# ===== ì›¹ í¬ë¡¤ë§ =====
def crawl_aladin_fallback(isbn13):
    url = f"https://www.aladin.co.kr/shop/wproduct.aspx?ISBN={isbn13}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        original = soup.select_one("div.info_original")
        lang_info = soup.select_one("div.conts_info_list1")
        category_text = ""
        categories = soup.select("div.conts_info_list2 li")
        for cat in categories:
            category_text += cat.get_text(separator=" ", strip=True) + " "
        detected_lang = ""
        if lang_info and "ì–¸ì–´" in lang_info.text:
            if "Japanese" in lang_info.text: detected_lang = "jpn"
            elif "Chinese" in lang_info.text: detected_lang = "chi"
            elif "English" in lang_info.text: detected_lang = "eng"
        return {
            "original_title": original.text.strip() if original else "",
            "subject_lang": detect_language_from_category(category_text) or detected_lang,
            "category_text": category_text
        }
    except Exception as e:
        st.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}

# ===== ê²°ê³¼ ì¡°ì •(ì¶©ëŒ í•´ì†Œ) =====
def reconcile_language(candidate, fallback_hint=None, author_hint=None):
    """
    candidate: 1ì°¨ GPT ê²°ê³¼
    fallback_hint: ì¹´í…Œê³ ë¦¬/ì›ì œ ê·œì¹™ì—ì„œ ì–»ì€ íŒíŠ¸(ì˜ˆ: 'ger')
    author_hint: ì €ì ê¸°ë°˜ GPT ê²°ê³¼
    """
    if author_hint and author_hint != "und" and author_hint != candidate:
        st.write(f"ğŸ” [ì¡°ì •] ì €ìê¸°ë°˜({author_hint}) â‰  1ì°¨({candidate}) â†’ ì €ìê¸°ë°˜ ìš°ì„ ")
        return author_hint
    if fallback_hint and fallback_hint != "und" and fallback_hint != candidate:
        if candidate in {"ita","fre","spa","por"}:
            st.write(f"ğŸ” [ì¡°ì •] ê·œì¹™íŒíŠ¸({fallback_hint}) vs 1ì°¨({candidate}) â†’ ê·œì¹™íŒíŠ¸ ìš°ì„ ")
            return fallback_hint
    return candidate

# ===== $h ìš°ì„ ìˆœìœ„ ê²°ì • (ì €ì ê¸°ë°˜ ë³´ì • + ê·¼ê±° ë¡œê¹… í¬í•¨) =====
def determine_h_language(
    title: str,
    original_title: str,
    category_text: str,
    publisher: str,
    author: str,
    subject_lang: str
) -> str:
    """
    ë¬¸í•™: ì¹´í…Œê³ ë¦¬/ì›¹ â†’ (ë¶€ì¡±ì‹œ) GPT â†’ (ì—¬ì „íˆ ë¶ˆí™•ì‹¤) ì €ì ê¸°ë°˜ ë³´ì •
    ë¹„ë¬¸í•™: GPT â†’ (ë¶€ì¡±ì‹œ) ì¹´í…Œê³ ë¦¬/ì›¹ â†’ (ì—¬ì „íˆ ë¶ˆí™•ì‹¤) ì €ì ê¸°ë°˜ ë³´ì •
    """
    lit_raw = is_literature_category(category_text)
    nf_override = is_nonfiction_override(category_text)
    is_lit_final = lit_raw and not nf_override

    # ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ì„¤ëª…
    if lit_raw and not nf_override:
        st.write("ğŸ“˜ [íŒì •] ì´ ìë£ŒëŠ” ë¬¸í•™(ì†Œì„¤/ì‹œ/í¬ê³¡ ë“±) ì„±ê²©ì´ ëšœë ·í•©ë‹ˆë‹¤.")
    elif lit_raw and nf_override:
        st.write("ğŸ“˜ [íŒì •] ê²‰ë³´ê¸°ì—ëŠ” ë¬¸í•™ì´ì§€ë§Œ, 'ì—­ì‚¬Â·ì—ì„¸ì´Â·ì‚¬íšŒê³¼í•™' ë“± ë¹„ë¬¸í•™ ìš”ì†Œê°€ í•¨ê»˜ ë³´ì—¬ ìµœì¢…ì ìœ¼ë¡œëŠ” ë¹„ë¬¸í•™ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif not lit_raw and nf_override:
        st.write("ğŸ“˜ [íŒì •] ë¬¸í•™ì  ë‹¨ì„œëŠ” ì—†ê³ , ë¹„ë¬¸í•™(ì—­ì‚¬Â·ì‚¬íšŒÂ·ì² í•™ ë“±) ì„±ê²©ì´ ê°•í•©ë‹ˆë‹¤.")
    else:
        st.write("ğŸ“˜ [íŒì •] ë¬¸í•™/ë¹„ë¬¸í•™ íŒë‹¨ ë‹¨ì„œê°€ ì•½í•´ ì¶”ê°€ íŒë‹¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    rule_from_original = detect_language(original_title) if original_title else "und"
    lang_h = None
    author_hint = None

    if is_lit_final:
        # ë¬¸í•™: 1) ì¹´í…Œê³ ë¦¬/ì›¹ â†’ 2) ì›ì œ ìœ ë‹ˆì½”ë“œ â†’ 3) GPT â†’ 4) ì €ì ê¸°ë°˜
        lang_h = subject_lang or rule_from_original
        st.write(f"ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) 1ì°¨ í›„ë³´: {lang_h or 'und'}")
        if not lang_h or lang_h == "und":
            st.write("ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) GPT ë³´ì™„ ì‹œë„â€¦")
            lang_h = gpt_guess_original_lang(title, category_text, publisher, author, original_title)
            st.write(f"ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) GPT ê²°ê³¼: {lang_h}")
        if (not lang_h or lang_h == "und") and author:
            st.write("ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) ì›ì œ ì—†ìŒ/ì• ë§¤ â†’ ì €ì ê¸°ë°˜ ë³´ì • ì‹œë„â€¦")
            author_hint = gpt_guess_original_lang_by_author(author, title, category_text, publisher)
            st.write(f"ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) ì €ì ê¸°ë°˜ ê²°ê³¼: {author_hint}")
    else:
        # ë¹„ë¬¸í•™: 1) GPT â†’ 2) ì¹´í…Œê³ ë¦¬/ì›¹ â†’ 3) ì›ì œ ìœ ë‹ˆì½”ë“œ â†’ 4) ì €ì ê¸°ë°˜
        st.write("ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) GPT ì„ í–‰ íŒë‹¨â€¦")
        lang_h = gpt_guess_original_lang(title, category_text, publisher, author, original_title)
        st.write(f"ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) GPT ê²°ê³¼: {lang_h or 'und'}")
        if not lang_h or lang_h == "und":
            lang_h = subject_lang or rule_from_original
            st.write(f"ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) ë³´ì¡° ê·œì¹™ ì ìš© â†’ í›„ë³´: {lang_h or 'und'}")
        if author and (not lang_h or lang_h == "und"):
            st.write("ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) ì›ì œ ì—†ìŒ/ì• ë§¤ â†’ ì €ì ê¸°ë°˜ ë³´ì • ì‹œë„â€¦")
            author_hint = gpt_guess_original_lang_by_author(author, title, category_text, publisher)
            st.write(f"ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) ì €ì ê¸°ë°˜ ê²°ê³¼: {author_hint}")

    # ì¶©ëŒ ì¡°ì •
    fallback_hint = subject_lang or rule_from_original
    lang_h = reconcile_language(candidate=lang_h, fallback_hint=fallback_hint, author_hint=author_hint)
    st.write("ğŸ“˜ [ê²°ê³¼] ì¡°ì • í›„ ì›ì„œ ì–¸ì–´(h) =", lang_h)

    return (lang_h if lang_h in ALLOWED_CODES else "und") or "und"

# ===== êµ­ë‚´ë„ì„œ ì—¬ë¶€ ê°€ë“œ =====
def is_domestic_category(category_text: str) -> bool:
    return "êµ­ë‚´ë„ì„œ" in (category_text or "")

# ===== KORMARC íƒœê·¸ ìƒì„±ê¸° =====
def get_kormarc_tags(isbn):
    isbn = isbn.strip().replace("-", "")
    url = "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
    params = {
        "ttbkey": ALADIN_KEY,
        "itemIdType": "ISBN13",
        "ItemId": isbn,
        "output": "xml",
        "Version": "20131101"
    }
    try:
        response = requests.get(url, params=params)
        if response.status_code != 200:
            raise ValueError("API í˜¸ì¶œ ì‹¤íŒ¨")
        root = ET.fromstring(response.content)
        for elem in root.iter():
            elem.tag = strip_ns(elem.tag)
        item = root.find("item")
        if item is None:
            raise ValueError("<item> íƒœê·¸ ì—†ìŒ")

        title = item.findtext("title", default="")
        publisher = item.findtext("publisher", default="")
        author = item.findtext("author", default="")
        subinfo = item.find("subInfo")
        original_title = subinfo.findtext("originalTitle") if subinfo is not None else ""

        crawl = crawl_aladin_fallback(isbn)
        if not original_title:
            original_title = crawl.get("original_title", "")
        subject_lang = crawl.get("subject_lang")
        category_text = crawl.get("category_text", "")

        # ---- $a: ë³¸ë¬¸ ì–¸ì–´ (ìš”ì²­í•œ ìˆœì„œë¡œ ì¬ì •ë ¬) ----
        # 1) ê·œì¹™ ê¸°ë°˜ 1ì°¨ ê°ì§€
        lang_a = detect_language(title)
        st.write("ğŸ“˜ [DEBUG] ê·œì¹™ ê¸°ë°˜ 1ì°¨ lang_a =", lang_a)

        # 2) ê°•í•œ ê°€ë“œ: 'êµ­ë‚´ë„ì„œ'ë©´ korë¡œ ê³ ì •
        if is_domestic_category(category_text):
            st.write("ğŸ“˜ [íŒì •] ì¹´í…Œê³ ë¦¬ì— 'êµ­ë‚´ë„ì„œ' ê°ì§€ â†’ $a=kor(ê°•í•œ ê°€ë“œ)")
            lang_a = "kor"

        # 3) GPT ë³´ì¡°: und/engì¼ ë•Œë§Œ í˜¸ì¶œ
        if lang_a in ('und', 'eng'):
            st.write("ğŸ“˜ [ì„¤ëª…] und/eng â†’ GPT ë³´ì¡°ë¡œ ë³¸ë¬¸ ì–¸ì–´ ì¬íŒì •â€¦")
            gpt_a = gpt_guess_main_lang(title, category_text, publisher)
            st.write(f"ğŸ“˜ [ì„¤ëª…] GPT íŒë‹¨ lang_a = {gpt_a}")
            if gpt_a in ALLOWED_CODES:
                lang_a = gpt_a
            else:
                lang_a = "und"

        # ---- $h: ì›ì € ì–¸ì–´ (ì €ì ê¸°ë°˜ ë³´ì • & ê·¼ê±° ë¡œê¹… í¬í•¨) ----
        st.write("ğŸ“˜ [DEBUG] ì›ì œ ê°ì§€ë¨:", bool(original_title), "| ì›ì œ:", original_title or "(ì—†ìŒ)")
        st.write("ğŸ“˜ [DEBUG] ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ lang_h í›„ë³´ =", subject_lang or "(ì—†ìŒ)")
        lang_h = determine_h_language(
            title=title,
            original_title=original_title,
            category_text=category_text,
            publisher=publisher,
            author=author,
            subject_lang=subject_lang
        )
        st.write("ğŸ“˜ [ê²°ê³¼] ìµœì¢… ì›ì„œ ì–¸ì–´(h) =", lang_h)

        # ---- íƒœê·¸ ì¡°í•© ----
        if lang_h and lang_h != lang_a and lang_h != "und":
            tag_041 = f"041 $a{lang_a} $h{lang_h}"
        else:
            tag_041 = f"041 $a{lang_a}"
        tag_546 = generate_546_from_041_kormarc(tag_041)

        return tag_041, tag_546, original_title
    except Exception as e:
        return f"ğŸ“• ì˜ˆì™¸ ë°œìƒ: {e}", "", ""

# ===== Streamlit UI =====
st.title("ğŸ“˜ KORMARC 041/546 íƒœê·¸ ìƒì„±ê¸° (ì €ì ë³´ì • + ê·¼ê±° ë¡œê¹… í†µí•©ë³¸)")

isbn_input = st.text_input("ISBNì„ ì…ë ¥í•˜ì„¸ìš” (13ìë¦¬):")
if st.button("íƒœê·¸ ìƒì„±"):
    if isbn_input:
        try:
            tag_041, tag_546, original = get_kormarc_tags(isbn_input)
            st.text(f"ğŸ“„ 041 íƒœê·¸: {tag_041}")
            if tag_546:
                st.text(f"ğŸ“„ 546 íƒœê·¸: {tag_546}")
            if original:
                st.text(f"ğŸ“• ì›ì œ: {original}")
        except Exception as e:
            st.error(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.warning("ISBNì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
