# ==========================================================
# field_builders.py  â€” Block A
# ì›ë³¸ ì½”ë“œì˜ "041/546 ìƒì„±", "ì–¸ì–´ ê°ì§€", "008 ìƒì„± ì „ê¹Œì§€"
# ëª¨ë“  ë¡œì§ì„ ì›ë³¸ ê·¸ëŒ€ë¡œ ë¶„ë¦¬
# ==========================================================

import re
import datetime
from collections import Counter
from dataclasses import dataclass
from typing import Optional, Dict, Any
from bs4 import BeautifulSoup

from pymarc import Field, Subfield
import requests

# ============================
# 041/546 ê´€ë ¨ ìœ í‹¸
# ============================

ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}

def detect_language(text):
    """
    ì›ë³¸ detect_language ê·¸ëŒ€ë¡œ.
    """
    text = re.sub(r'[\s\W_]+', '', text)
    if not text:
        return 'und'
    first_char = text[0]
    if '\uac00' <= first_char <= '\ud7a3':
        return 'kor'
    elif '\u3040' <= first_char <= '\u30ff':
        return 'jpn'
    elif '\u4e00' <= first_char <= '\u9fff':
        return 'chi'
    elif '\u0400' <= first_char <= '\u04FF':
        return 'rus'
    elif 'a' <= first_char.lower() <= 'z':
        return 'eng'
    else:
        return 'und'


def generate_546_from_041_kormarc(marc_041: str) -> str:
    """
    ì›ë³¸ generate_546_from_041_kormarc ê·¸ëŒ€ë¡œ ì´ë™.
    """
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"):
            a_codes.append(part[2:])
        elif part.startswith("$h"):
            h_code = part[2:]

    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{h_lang} ì›ì‘ì„ {a_lang}ë¡œ ë²ˆì—­"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"

    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"


def _lang3_from_tag041(tag_041: str | None) -> str | None:
    """
    =041 0\$akor$heng â†’ 'kor'
    ì›ë³¸ ê·¸ëŒ€ë¡œ.
    """
    if not tag_041:
        return None
    m = re.search(r"\$a([a-z]{3})", tag_041, flags=re.I)
    return m.group(1).lower() if m else None


# ==========================================================
# 653 ì „ì²˜ë¦¬ + GPT 653 (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================

def extract_keywords_from_text(text, top_n=7):
    words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
    filtered = [w for w in words if len(w) > 1]
    freq = Counter(filtered)
    return [kw for kw, _ in freq.most_common(top_n)]

def clean_keywords(words):
    stopwords = {"ì•„ì£¼", "ê°€ì§€", "í•„ìš”í•œ", "ë“±", "ìœ„í•´", "ê²ƒ", "ìˆ˜", "ë”", "ì´ëŸ°", "ìˆë‹¤", "ëœë‹¤", "í•œë‹¤"}
    return [w for w in words if w not in stopwords and len(w) > 1]


# -------------------------- ë‚´ë¶€ ì „ì²˜ë¦¬ --------------------------

def _norm(text: str) -> str:
    import unicodedata
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text).lower()
    text = re.sub(r"[^\w\s\uac00-\ud7a3]", " ", text)
    return re.sub(r"\s+", " ", text).strip()

def _clean_author_str(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\(.*?\)", " ", s)
    s = re.sub(r"[/;Â·,]", " ", s)
    return re.sub(r"\s+", " ", s).strip()

def _build_forbidden_set(title: str, authors: str) -> set:
    t_norm = _norm(title)
    a_norm = _norm(authors)
    forb = set()
    if t_norm:
        forb.update(t_norm.split())
        forb.add(t_norm.replace(" ", ""))
    if a_norm:
        forb.update(a_norm.split())
        forb.add(a_norm.replace(" ", ""))
    return {f for f in forb if f and len(f) >= 2}

def _should_keep_keyword(kw: str, forbidden: set) -> bool:
    n = _norm(kw)
    if not n or len(n.replace(" ", "")) < 2:
        return False
    for tok in forbidden:
        if tok in n or n in tok:
            return False
    return True


# -------------------------- GPT 653 í•µì‹¬ --------------------------

def generate_653_with_gpt(category, title, authors, description, toc, max_keywords=7):
    """
    ì›ë³¸ generate_653_with_gpt ê·¸ëŒ€ë¡œ.
    """
    import json
    import openai

    parts = [p.strip() for p in (category or "").split(">") if p.strip()]
    cat_tail = " ".join(parts[-2:]) if len(parts) >= 2 else (parts[-1] if parts else "")

    forbidden = _build_forbidden_set(title, authors)
    forbidden_list = ", ".join(sorted(forbidden)) or "(ì—†ìŒ)"

    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ KORMARC ì‘ì„± ê²½í—˜ì´ í’ë¶€í•œ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ë¶„ë¥˜ ì •ë³´, ì„¤ëª…, ëª©ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'MARC 653 ììœ ì£¼ì œì–´'ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.\n\n"
            "(ì¤‘ëµ â€” ì›ë³¸ ì „ì²´ ê·¸ëŒ€ë¡œ ìœ ì§€)"
        )
    }

    user_msg = {
        "role": "user",
        "content": (
            f"- ë¶„ë¥˜: {category}\n"
            f"- ì œëª©: {title}\n"
            f"- ì €ì: {authors}\n"
            f"- ì„¤ëª…: {description}\n"
            f"- ëª©ì°¨: {toc}\n"
            f"- ê¸ˆì¹™ì–´: {forbidden_list}\n"
            "(ì´í•˜ ì›ë³¸ ê·¸ëŒ€ë¡œ)"
        )
    }

    try:
        resp = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=180,
        )
        raw = (resp.choices[0].message["content"] or "").strip()

        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]
        if not kws:
            tmp = re.split(r"[,\n;|/Â·]", raw)
            kws = [t.strip().lstrip("$a") for t in tmp if t.strip()]

        kws = [kw.replace(" ", "") for kw in kws if kw]
        kws = [kw for kw in kws if _should_keep_keyword(kw, forbidden)]

        seen = set(); uniq = []
        for kw in kws:
            n = _norm(kw)
            if n not in seen:
                seen.add(n)
                uniq.append(kw)

        uniq = uniq[:max_keywords]
        return "".join(f"$a{kw}" for kw in uniq)

    except Exception:
        return None


# --------------------------------------------------------------
# GPT ê¸°ë°˜ 653 ìƒì„± â†’ =653 í˜•íƒœë¡œ wrapping
# --------------------------------------------------------------
def _build_653_via_gpt(item: dict) -> str | None:
    title = (item or {}).get("title", "") or ""
    category = (item or {}).get("categoryName", "") or ""
    raw_author = (item or {}).get("author", "") or ""
    desc = (item or {}).get("description", "") or ""
    toc = ((item or {}).get("subInfo", {}) or {}).get("toc", "") or ""

    kwline = generate_653_with_gpt(
        category=category,
        title=title,
        authors=_clean_author_str(raw_author),
        description=desc,
        toc=toc,
        max_keywords=7,
    )
    return f"=653  \\\\{kwline.replace(' ', '')}" if kwline else None


# ==========================================================
# 008 ìƒì„±ê¹Œì§€ì˜ ë„êµ¬ (country guess, illus, lit_form ë“±)
# ==========================================================

COUNTRY_FIXED = "ko "     # ì›ë³¸ ìƒë‹¨ ì •ì˜ ê·¸ëŒ€ë¡œ
LANG_FIXED = "kor"

KR_REGION_TO_CODE = {
    "ì„œìš¸": "ko ",
    "ë¶€ì‚°": "ko ",
    "ê²½ê¸°": "ko ",
    # ì›ë³¸ì—ì„œëŠ” í•œêµ­ ì¼ë°˜ ë¶€í˜¸ëŠ” ì“°ì§€ ì•Šë„ë¡ í•¨ â†’ ê·¸ëŒ€ë¡œ.
}


def extract_year_from_aladin_pubdate(pubdate_str: str) -> str:
    m = re.search(r"(19|20)\d{2}", pubdate_str or "")
    return m.group(0) if m else "19uu"


def guess_country3_from_place(place_str: str) -> str:
    if not place_str:
        return COUNTRY_FIXED
    for key, code in KR_REGION_TO_CODE.items():
        if key in place_str:
            return code
    return COUNTRY_FIXED


def detect_illus4(text: str) -> str:
    keys = []
    if re.search(r"ì‚½í™”|ì‚½ë„|ë„í•´|ì¼ëŸ¬ìŠ¤íŠ¸|ê·¸ë¦¼", text, re.I):
        keys.append("a")
    if re.search(r"ë„í‘œ|í‘œ|ì°¨íŠ¸|ê·¸ë˜í”„", text, re.I):
        keys.append("d")
    if re.search(r"ì‚¬ì§„|í¬í† |í™”ë³´|photo", text, re.I):
        keys.append("o")
    out = []
    for k in keys:
        if k not in out:
            out.append(k)
    return "".join(out)[:4]


def detect_index(text: str) -> str:
    return "1" if re.search(r"ìƒ‰ì¸|ì°¾ì•„ë³´ê¸°|index", text, re.I) else "0"


def detect_lit_form(title: str, category: str, extra_text: str = "") -> str:
    blob = f"{title} {category} {extra_text}"
    if re.search(r"ì„œê°„ì§‘|í¸ì§€|ì„œê°„ë¬¸", blob, re.I):
        return "i"
    if re.search(r"ê¸°í–‰|ì—¬í–‰ê¸°|ì¼ê¸°", blob, re.I):
        return "m"
    if re.search(r"ì‹œì§‘|ì‚°ë¬¸ì‹œ|poem|poetry", blob, re.I):
        return "p"
    if re.search(r"ì†Œì„¤|novel|fiction", blob, re.I):
        return "f"
    if re.search(r"ì—ì„¸ì´|ìˆ˜í•„|essay", blob, re.I):
        return "e"
    return " "


def detect_bio(text: str) -> str:
    if re.search(r"ìì„œì „|íšŒê³ ë¡", text, re.I):
        return "a"
    if re.search(r"ì „ê¸°|í‰ì „|biograph", text, re.I):
        return "b"
    if re.search(r"ì „ê¸°ì |ìì „ì |íšŒê³ ", text):
        return "d"
    return " "


def _is_unknown_place(s: str | None) -> bool:
    if not s:
        return False
    t = s.strip()
    t_no_sp = t.replace(" ", "")
    lower = t.lower()
    return (
        "ë¯¸ìƒ" in t or
        "ë¯¸ìƒ" in t_no_sp or
        "unknown" in lower or
        "place unknown" in lower
    )
# ==========================================================
# field_builders.py â€” Block B
# 008 ìƒì„± + ê°€ê²©/020/950 + KPIPA ì¶œíŒì§€ ì¶”ë¡  + 260 í•„ë“œ
# ì›ë³¸ ë¡œì§ 100% ê·¸ëŒ€ë¡œ
# ==========================================================

import re
import datetime
import pandas as pd
import requests
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials

from pymarc import Field, Subfield

from .utils import clean_text, convert_mm_to_cm

# ==========================================================
# 008 ë³¸ë¬¸(40ì) ìƒì„±ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================

def build_008_kormarc_bk(
    date_entered,          # YYMMDD
    date1,                 # ë°œí–‰ì—°ë„(4ìë¦¬)
    country3,              # ë°œí–‰êµ­ ë¶€í˜¸(3ì¹¸)
    lang3,                 # ì–¸ì–´ì½”ë“œ(3ì¹¸)
    date2="", illus4="", has_index="0",
    lit_form=" ", bio=" ", type_of_date="s",
    modified_record=" ", cataloging_src="a",
):
    def pad(s, n, fill=" "):
        s = "" if s is None else str(s)
        return (s[:n] + fill * n)[:n]

    if len(date_entered) != 6 or not date_entered.isdigit():
        raise ValueError("date_enteredëŠ” YYMMDD 6ìë¦¬ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
    if len(date1) != 4:
        raise ValueError("date1ì€ 4ìë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    body = "".join([
        date_entered,               # 00-05
        pad(type_of_date,1),        # 06
        date1,                      # 07-10
        pad(date2,4),               # 11-14
        pad(country3,3),            # 15-17
        pad(illus4,4),              # 18-21
        " " * 4,                    # 22-25
        " " * 2,                    # 26-27
        pad(modified_record,1),     # 28
        "0",                        # 29
        "0",                        # 30
        has_index if has_index in ("0","1") else "0",  # 31
        pad(cataloging_src,1),      # 32
        pad(lit_form,1),            # 33
        pad(bio,1),                 # 34
        pad(lang3,3),               # 35-37
        " " * 2                     # 38-39
    ])

    if len(body) != 40:
        raise AssertionError(f"008 length != 40: {len(body)}")

    return body


# ----------------------------------------------------------
# 008 ì „ì²´ ì¡°ë¦½ (ì›ë³¸ build_008_from_isbn ê·¸ëŒ€ë¡œ)
# ----------------------------------------------------------

def build_008_from_isbn(
    isbn: str,
    *,
    aladin_pubdate: str = "",
    aladin_title: str = "",
    aladin_category: str = "",
    aladin_desc: str = "",
    aladin_toc: str = "",
    source_300_place: str = "",
    override_country3: str = None,
    override_lang3: str = None,
    cataloging_src="a",
):
    today  = datetime.datetime.now().strftime("%y%m%d")
    date1  = extract_year_from_aladin_pubdate(aladin_pubdate)

    # --- ë°œí–‰êµ­ ë¶€í˜¸ ê²°ì • ---
    if override_country3:
        country3 = override_country3
    elif source_300_place:
        if _is_unknown_place(source_300_place):
            country3 = "   "
        else:
            guessed = guess_country3_from_place(source_300_place)
            country3 = guessed if guessed else COUNTRY_FIXED
    else:
        country3 = COUNTRY_FIXED

    # ì–¸ì–´ ìš°ì„ ìˆœìœ„: override > ê¸°ë³¸ê°’
    lang3 = override_lang3 or LANG_FIXED

    # ì‚½í™”, ìƒ‰ì¸, ë¬¸í•™í˜•ì‹, ì „ê¸°ê°ì§€
    bigtext = " ".join([aladin_title or "", aladin_desc or "", aladin_toc or ""])
    illus4    = detect_illus4(bigtext)
    has_index = detect_index(bigtext)
    lit_form  = detect_lit_form(aladin_title or "", aladin_category or "", bigtext)
    bio       = detect_bio(bigtext)

    return build_008_kormarc_bk(
        date_entered=today,
        date1=date1,
        country3=country3,
        lang3=lang3,
        illus4=illus4,
        has_index=has_index,
        lit_form=lit_form,
        bio=bio,
        cataloging_src=cataloging_src,
    )


# ==========================================================
# NLK(êµ­ë¦½ì¤‘ì•™ë„ì„œê´€) â€” EA_ADD_CODE, SET ISBN, ê°€ê²© PRE_PRICE
# ì›ë³¸ fetch_additional_code_from_nlk ê·¸ëŒ€ë¡œ
# ==========================================================

def fetch_additional_code_from_nlk(isbn: str) -> dict:
    attempts = [
        "https://seoji.nl.go.kr/landingPage/SearchApi.do",
        "https://www.nl.go.kr/seoji/SearchApi.do",
        "http://seoji.nl.go.kr/landingPage/SearchApi.do",
        "http://www.nl.go.kr/seoji/SearchApi.do",
    ]
    params = {
        "cert_key": NLK_CERT_KEY,
        "result_style": "json",
        "page_no": 1,
        "page_size": 1,
        "isbn": isbn.strip().replace("-", ""),
    }

    for base in attempts:
        try:
            r = requests.get(base, params=params, timeout=(5, 10))
            r.raise_for_status()
            j = r.json()

            doc = None
            if isinstance(j, dict):
                if "docs" in j and isinstance(j["docs"], list) and j["docs"]:
                    doc = j["docs"][0]
                elif "doc" in j and isinstance(j["doc"], list) and j["doc"]:
                    doc = j["doc"][0]
            if not doc:
                continue

            add_code = (doc.get("EA_ADD_CODE") or "").strip()
            set_isbn = (doc.get("SET_ISBN") or "").strip()
            price = (doc.get("PRE_PRICE") or "").strip()

            return {
                "add_code": add_code,
                "set_isbn": set_isbn,
                "price": price,
            }
        except Exception:
            continue

    return {
        "add_code": "",
        "set_isbn": "",
        "set_title": "",
        "price": "",
    }


# ==========================================================
# 020 í•„ë“œ ìƒì„±ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================

def _build_020_from_item_and_nlk(isbn: str, item: dict) -> str:
    price = str((item or {}).get("priceStandard", "") or "").strip()

    try:
        nlk_extra = fetch_additional_code_from_nlk(isbn) or {}
        add_code = nlk_extra.get("add_code", "")
        price_from_nlk = nlk_extra.get("price", "")
    except Exception:
        add_code = ""
        price_from_nlk = ""

    final_price = price or price_from_nlk

    parts = [f"=020  \\\\$a{isbn}"]
    if add_code:
        parts.append(f"$g{add_code}")
    if final_price:
        parts.append(f":$c{final_price}")

    return "".join(parts)


# ==========================================================
# 950 í•„ë“œ (ê°€ê²©) ìƒì„±ê¸° â€” ì›ë³¸ ê·¸ëŒ€ë¡œ
# ==========================================================

def _extract_price_kr(item: dict, isbn: str) -> str:
    raw = str((item or {}).get("priceStandard", "") or "").strip()

    if not raw:
        try:
            crawl = crawl_aladin_original_and_price(isbn) or {}
            raw = crawl.get("price", "").strip()
        except Exception:
            raw = ""

    digits = re.sub(r"[^\d]", "", raw)
    return digits


def build_950_from_item_and_price(item: dict, isbn: str) -> str:
    price = _extract_price_kr(item, isbn)
    if not price:
        return ""
    return f"=950  0\\$b\\{price}"


# ==========================================================
# ì¶œíŒì§€ ì¶”ì¶œ (KPIPA / IMPRINT / ë¬¸ì²´ë¶€ / FallBack)
# ==========================================================

def load_publisher_db():
    creds = ServiceAccountCredentials.from_json_keyfile_dict(
        st.secrets["gspread"],
        ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    )
    client = gspread.authorize(creds)
    sh = client.open("ì¶œíŒì‚¬ DB")

    pub_rows = sh.worksheet("ë°œí–‰ì²˜ëª…â€“ì£¼ì†Œ ì—°ê²°í‘œ").get_all_values()[1:]
    pub_rows_filtered = [row[1:3] for row in pub_rows]
    publisher_data = pd.DataFrame(pub_rows_filtered, columns=["ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ"])

    region_rows = sh.worksheet("ë°œí–‰êµ­ëª…â€“ë°œí–‰êµ­ë¶€í˜¸ ì—°ê²°í‘œ").get_all_values()[1:]
    region_rows_filtered = [row[:2] for row in region_rows]
    region_data = pd.DataFrame(region_rows_filtered, columns=["ë°œí–‰êµ­", "ë°œí–‰êµ­ ë¶€í˜¸"])

    imprint_frames = []
    for ws in sh.worksheets():
        if ws.title.startswith("ë°œí–‰ì²˜-ì„í”„ë¦°íŠ¸ ì—°ê²°í‘œ"):
            data = ws.get_all_values()[1:]
            imprint_frames.extend([row[0] for row in data if row])
    imprint_data = pd.DataFrame(imprint_frames, columns=["ì„í”„ë¦°íŠ¸"])

    return publisher_data, region_data, imprint_data


def normalize_publisher_name(name):
    return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬", "", name).lower()


def normalize_stage2(name):
    name = re.sub(r"(ì£¼ë‹ˆì–´|JUNIOR|ì–´ë¦°ì´|í‚¤ì¦ˆ|ë¶ìŠ¤|ì•„ì´ì„¸ì›€|í”„ë ˆìŠ¤)", "", name, flags=re.IGNORECASE)
    eng_to_kor = {"springer": "ìŠ¤í”„ë§ê±°", "cambridge": "ì¼€ì„ë¸Œë¦¬ì§€", "oxford": "ì˜¥ìŠ¤í¬ë“œ"}
    for eng, kor in eng_to_kor.items():
        name = re.sub(eng, kor, name, flags=re.IGNORECASE)
    return name.strip().lower()


def split_publisher_aliases(name):
    aliases = []
    bracket_contents = re.findall(r"\((.*?)\)", name)
    for content in bracket_contents:
        parts = re.split(r"[,/]", content)
        parts = [p.strip() for p in parts if p.strip()]
        aliases.extend(parts)

    name_no_brackets = re.sub(r"\(.*?\)", "", name).strip()
    if "/" in name_no_brackets:
        parts = [p.strip() for p in name_no_brackets.split("/") if p.strip()]
        rep_name = parts[0]
        aliases.extend(parts[1:])
    else:
        rep_name = name_no_brackets

    return rep_name, aliases


def search_publisher_location_with_alias(name, publisher_data):
    debug_msgs = []
    if not name:
        return "ì¶œíŒì§€ ë¯¸ìƒ", ["âŒ ê²€ìƒ‰ ì‹¤íŒ¨: ì…ë ¥ëœ ì¶œíŒì‚¬ëª…ì´ ì—†ìŒ"]

    norm_name = normalize_publisher_name(name)
    candidates = publisher_data[publisher_data["ì¶œíŒì‚¬ëª…"].apply(
        lambda x: normalize_publisher_name(x)) == norm_name]

    if not candidates.empty:
        address = candidates.iloc[0]["ì£¼ì†Œ"]
        debug_msgs.append(f"âœ… KPIPA DB ë§¤ì¹­ ì„±ê³µ: {name} â†’ {address}")
        return address, debug_msgs
    else:
        debug_msgs.append(f"âŒ KPIPA DB ë§¤ì¹­ ì‹¤íŒ¨: {name}")
        return "ì¶œíŒì§€ ë¯¸ìƒ", debug_msgs


def find_main_publisher_from_imprints(rep_name, imprint_data, publisher_data):
    norm_rep = normalize_publisher_name(rep_name)

    for full_text in imprint_data["ì„í”„ë¦°íŠ¸"]:
        if "/" in full_text:
            pub_part, imprint_part = [p.strip() for p in full_text.split("/", 1)]
        else:
            pub_part, imprint_part = full_text.strip(), None

        if imprint_part:
            norm_imprint = normalize_publisher_name(imprint_part)
            if norm_imprint == norm_rep:
                location, dbg = search_publisher_location_with_alias(pub_part, publisher_data)
                return location, dbg

    return None, [f"âŒ IM DB ê²€ìƒ‰ ì‹¤íŒ¨: ë§¤ì¹­ ì—†ìŒ ({rep_name})"]


def get_mcst_address(publisher_name):
    url = "https://book.mcst.go.kr/html/searchList.php"
    params = {"search_area": "ì „ì²´", "search_state": "1",
              "search_kind": "1", "search_type": "1",
              "search_word": publisher_name}
    debug_msgs = []

    try:
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        results = []
        for row in soup.select("table.board tbody tr"):
            cols = row.find_all("td")
            if len(cols) >= 4:
                reg_type = cols[0].get_text(strip=True)
                name = cols[1].get_text(strip=True)
                addr = cols[2].get_text(strip=True)
                status = cols[3].get_text(strip=True)

                if status == "ì˜ì—…":
                    results.append((reg_type, name, addr, status))

        if results:
            debug_msgs.append(f"[ë¬¸ì²´ë¶€] ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê±´")
            return results[0][2], results, debug_msgs
        else:
            debug_msgs.append("[ë¬¸ì²´ë¶€] ê²°ê³¼ ì—†ìŒ")
            return "ë¯¸í™•ì¸", [], debug_msgs
    except Exception as e:
        debug_msgs.append(f"[ë¬¸ì²´ë¶€] ì˜ˆì™¸: {e}")
        return "ì˜¤ë¥˜ ë°œìƒ", [], debug_msgs


def get_country_code_by_region(region_name, region_data):
    try:
        def normalize_region(region):
            region = (region or "").strip()
            if region.startswith(("ì „ë¼", "ì¶©ì²­", "ê²½ìƒ")):
                return region[0] + (region[2] if len(region) > 2 else "")
            return region[:2]

        normalized_input = normalize_region(region_name)

        for _, row in region_data.iterrows():
            sheet_region, code = row["ë°œí–‰êµ­"], row["ë°œí–‰êµ­ ë¶€í˜¸"]
            if normalize_region(sheet_region) == normalized_input:
                return code.strip() or "   "
        return "   "
    except Exception:
        return "   "


def build_pub_location_bundle(isbn, publisher_name_raw):
    debug = []

    try:
        publisher_data, region_data, imprint_data = load_publisher_db()
        debug.append("âœ“ êµ¬ê¸€ì‹œíŠ¸ DB ì ì¬ ì„±ê³µ")

        kpipa_full, kpipa_norm, err = get_publisher_name_from_isbn_kpipa(isbn)
        if err:
            debug.append(f"KPIPA ê²€ìƒ‰: {err}")

        rep_name, aliases = split_publisher_aliases(kpipa_full or publisher_name_raw or "")
        resolved_for_search = rep_name or (publisher_name_raw or "").strip()
        debug.append(f"ëŒ€í‘œ ì¶œíŒì‚¬ëª…: {resolved_for_search}")

        place_raw, msgs = search_publisher_location_with_alias(resolved_for_search, publisher_data)
        debug += msgs
        source = "KPIPA_DB"

        if place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ", None):
            place_raw, msgs = find_main_publisher_from_imprints(resolved_for_search, imprint_data, publisher_data)
            debug += msgs
            if place_raw:
                source = "IMPRINTâ†’KPIPA"

        if not place_raw or place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
            mcst_addr, _rows, dbg = get_mcst_address(resolved_for_search)
            debug += dbg
            if mcst_addr not in ("ë¯¸í™•ì¸", "ì˜¤ë¥˜ ë°œìƒ", None):
                place_raw = mcst_addr
                source = "MCST"

        if not place_raw or place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ", "ë¯¸í™•ì¸", "ì˜¤ë¥˜ ë°œìƒ"):
            place_raw = "ì¶œíŒì§€ ë¯¸ìƒ"
            source = "FALLBACK"
            debug.append("âš ï¸ ëª¨ë“  ê²½ë¡œ ì‹¤íŒ¨ â†’ 'ì¶œíŒì§€ ë¯¸ìƒ'")

        place_display = normalize_publisher_location_for_display(place_raw)
        country_code = get_country_code_by_region(place_raw, region_data)

        return {
            "place_raw": place_raw,
            "place_display": place_display,
            "country_code": country_code,
            "resolved_publisher": resolved_for_search,
            "source": source,
            "debug": debug,
        }

    except Exception as e:
        return {
            "place_raw": "ë°œí–‰ì§€ ë¯¸ìƒ",
            "place_display": "ë°œí–‰ì§€ ë¯¸ìƒ",
            "country_code": "   ",
            "resolved_publisher": publisher_name_raw or "",
            "source": "ERROR",
            "debug": [f"ì˜ˆì™¸: {e}"],
        }


def normalize_publisher_location_for_display(location_name):
    if not location_name or location_name in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
        return location_name

    location_name = location_name.strip()
    major = ["ì„œìš¸", "ì¸ì²œ", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ëŒ€êµ¬", "ë¶€ì‚°", "ì„¸ì¢…"]
    for city in major:
        if city in location_name:
            return location_name[:2]

    parts = location_name.split()
    loc = parts[1] if len(parts) > 1 else parts[0]
    if loc.endswith("ì‹œ"):
        loc = loc[:-1]
    return loc


# ==========================================================
# 260 í•„ë“œ ë¹Œë”
# ==========================================================

def build_260(place_display: str, publisher_name: str, pubyear: str):
    place = (place_display or "ë°œí–‰ì§€ ë¯¸ìƒ")
    pub = (publisher_name or "ë°œí–‰ì²˜ ë¯¸ìƒ")
    year = (pubyear or "ë°œí–‰ë…„ ë¯¸ìƒ")
    return f"=260  \\\\$a{place} :$b{pub},$c{year}"
# ==========================================================
# 653 ì „ì²˜ë¦¬ ìœ í‹¸ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================

import re
import unicodedata
from collections import Counter
import json
import requests
import streamlit as st

from pymarc import Field, Subfield

from .constants import ISDS_LANGUAGE_CODES
from .config import OPENAI_CHAT_COMPLETIONS, DEFAULT_MODEL, aladin_key
from .utils import clean_text

def _norm(text: str) -> str:
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text).lower()
    text = re.sub(r"[^\w\s\uac00-\ud7a3]", " ", text)
    return re.sub(r"\s+", " ", text).strip()

def _clean_author_str(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\(.*?\)", " ", s)
    s = re.sub(r"[/;Â·,]", " ", s)
    return re.sub(r"\s+", " ", s).strip()

def _build_forbidden_set(title: str, authors: str) -> set:
    t_norm = _norm(title)
    a_norm = _norm(authors)
    forb = set()

    if t_norm:
        forb.update(t_norm.split())
        forb.add(t_norm.replace(" ", ""))

    if a_norm:
        forb.update(a_norm.split())
        forb.add(a_norm.replace(" ", ""))

    return {f for f in forb if f and len(f) >= 2}

def _should_keep_keyword(kw: str, forbidden: set) -> bool:
    n = _norm(kw)
    if not n or len(n.replace(" ", "")) < 2:
        return False
    for tok in forbidden:
        if tok in n or n in tok:
            return False
    return True
def generate_653_with_gpt(category, title, authors, description, toc, max_keywords=7):
    import re
    from openai import OpenAI

    client = OpenAI()

    parts = [p.strip() for p in (category or "").split(">") if p.strip()]
    cat_tail = " ".join(parts[-2:]) if len(parts) >= 2 else (parts[-1] if parts else "")

    forbidden = _build_forbidden_set(title, authors)
    forbidden_list = ", ".join(sorted(forbidden)) or "(ì—†ìŒ)"

    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ KORMARC ì‘ì„± ê²½í—˜ì´ í’ë¶€í•œ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ì •ë³´ë¡œ 653 í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. "
            "í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ ë¶™ì—¬ì“°ê¸° í•˜ë©°, ëª…ì‚¬í˜• ê°œë…ìœ¼ë¡œë§Œ êµ¬ì„±í•©ë‹ˆë‹¤."
        )
    }

    user_msg = {
        "role": "user",
        "content": (
            f"ë¶„ë¥˜: {category}\n"
            f"í•µì‹¬ ë¶„ë¥˜ê¼¬ë¦¬: {cat_tail}\n"
            f"ì œëª©: {title}\n"
            f"ì €ì: {authors}\n"
            f"ì„¤ëª…: {description}\n"
            f"ëª©ì°¨: {toc}\n"
            f"ì œì™¸ì–´ ëª©ë¡: {forbidden_list}\n"
            "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì†Œ 1ê°œ~ìµœëŒ€ 7ê°œì˜ 653 í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n"
            "ë°˜ë“œì‹œ `$aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 ...` í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."
        )
    }

    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=200,
        )
        raw = (resp.choices[0].message.content or "").strip()

        # $a ì¶”ì¶œ
        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]

        # ë¶™ì—¬ì“°ê¸°
        kws = [kw.replace(" ", "") for kw in kws]

        # ê¸ˆì¹™ì–´ ì œê±°
        kws = [kw for kw in kws if _should_keep_keyword(kw, forbidden)]

        # ìµœëŒ€ 7ê°œ
        kws = kws[:max_keywords]

        return "".join(f"$a{kw}" for kw in kws)

    except Exception as e:
        st.warning(f"âš ï¸ 653 ìƒì„± ì‹¤íŒ¨: {e}")
        return None
def _build_653_via_gpt(item: dict) -> str | None:
    title = (item or {}).get("title","") or ""
    category = (item or {}).get("categoryName","") or ""
    raw_author = (item or {}).get("author","") or ""
    desc = (item or {}).get("description","") or ""
    toc  = ((item or {}).get("subInfo",{}) or {}).get("toc","") or ""

    kwline = generate_653_with_gpt(
        category=category,
        title=title,
        authors=_clean_author_str(raw_author),
        description=desc,
        toc=toc,
        max_keywords=7
    )
    return f"=653  \\\\{kwline}" if kwline else None


def _parse_653_keywords(tag_653: str | None) -> list[str]:
    if not tag_653:
        return []
    s = re.sub(r"^=653\s+\\\\", "", tag_653.strip())

    kws = []
    for m in re.finditer(r"\$a([^$]+)", s):
        w = (m.group(1) or "").strip()
        if w:
            kws.append(w)

    # ì¤‘ë³µ ì œê±°
    seen, out = set(), []
    for w in kws:
        if w not in seen:
            seen.add(w)
            out.append(w)
        if len(out) >= 7:
            break
    return out
# ==========================================================
# 041 ì›ì‘ì–¸ì–´ ê¸°ë°˜ â†’ ë¬¸í•™(8xx) ì¬ì •ë ¬ ë¡œì§ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================

def _parse_marc_041_original(marc041: str):
    """
    MARC 041ì—ì„œ ì›ì‘ ì–¸ì–´($h)ë¥¼ ì¶”ì¶œí•œë‹¤.
    ì˜ˆ: '041 0\\$akor$heng' -> 'eng'
    """
    if not marc041:
        return None
    s = marc041.lower()
    m = re.search(r"\$h([a-z]{3})", s)
    return m.group(1) if m else None


def _lang3_to_kdc_lit_base(lang3: str):
    """
    ì›ì‘ ì–¸ì–´ì½”ë“œ â†’ ë¬¸í•™ê³„ì—´ ê¸°ë³¸ 2ìë¦¬ ë§¤í•‘.
    (ì›ë³¸ ë¡œì§ 100% ìœ ì§€)
    """
    if not lang3:
        return None
    l = lang3.lower()

    if l in {"eng"}:
        return "84"   # ì˜ë¯¸ë¬¸í•™
    if l in {"kor"}:
        return "81"   # í•œêµ­ë¬¸í•™
    if l in {"chi", "zho"}:
        return "82"   # ì¤‘êµ­ë¬¸í•™
    if l in {"jpn"}:
        return "83"   # ì¼ë³¸ë¬¸í•™
    if l in {"deu", "ger"}:
        return "85"   # ë…ì¼ë¬¸í•™
    if l in {"fre"}:
        return "86"   # í”„ë‘ìŠ¤ë¬¸í•™
    if l in {"spa", "por"}:
        return "87"   # ìŠ¤í˜ì¸/í¬ë¥´íˆ¬ê°ˆë¬¸í•™
    if l in {"ita"}:
        return "88"   # ì´íƒˆë¦¬ì•„ë¬¸í•™

    return "89"        # ê¸°íƒ€ ë¬¸í•™
def _rebase_8xx_with_language(code: str, marc041: str) -> str:
    """
    056 ê²°ê³¼ê°€ ë¬¸í•™(8xx)ì¼ ë•Œ,
    041 $h ì›ì‘ì–¸ì–´ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬ ë³€ê²½.
    - ì¥ë¥´(ì„¸ ë²ˆì§¸ ìë¦¬) ê·¸ëŒ€ë¡œ ìœ ì§€
    - ì• ë‘ ìë¦¬ë§Œ ë³€ê²½
    """
    if not code or len(code) < 3 or code[0] != "8":
        return code  # ë¬¸í•™ì´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ìœ ì§€

    # ì›ì‘ì–¸ì–´ ì¶”ì¶œ
    orig_lang = _parse_marc_041_original(marc041 or "")
    base2 = _lang3_to_kdc_lit_base(orig_lang) if orig_lang else None
    if not base2:
        return code

    # 813.7 â†’ 813 ê·¸ëŒ€ë¡œ ì²˜ë¦¬
    m = re.match(r"^(\d{3})(\..+)?$", code)
    if not m:
        return code

    head3, tail = m.group(1), (m.group(2) or "")
    genre = head3[2]       # ë¬¸í•™ ì¥ë¥´ ë””ì§“ (1=ì‹œ, 3=ì†Œì„¤ â€¦)

    return base2 + genre
# ==========================================================
# KDC íŒë‹¨ LLM í˜¸ì¶œê¸° â€” í•µì‹¬ í•¨ìˆ˜
# (ë„ˆê°€ ì¤€ ì›ë³¸ ë¡œì§ì„ ì™„ì „íˆ ë¶„ë¦¬í•˜ì—¬ êµ¬ì¡°í™”)
# ==========================================================

def ask_llm_for_kdc(
    book: BookInfo,
    api_key: str,
    model: str = DEFAULT_MODEL,
    keywords_hint: list[str] | None = None
) -> Optional[str]:
    """
    KDC(056) íŒë‹¨ì„ LLMì—ê²Œ ìš”ì²­.
    ë°˜í™˜: 3ìë¦¬ ìˆ«ì ë¬¸ìì—´ or 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'
    """

    # -----------------------------
    # 1) ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    def clip(s: str, n: int) -> str:
        if not s:
            return ""
        s = str(s).strip()
        return s if len(s) <= n else s[:n] + "â€¦"

    payload = {
        "title":      clip(book.title, 160),
        "author":     clip(book.author, 120),
        "publisher":  book.publisher,
        "pub_date":   book.pub_date,
        "isbn13":     book.isbn13,
        "category":   clip(book.category, 160),
        "description": clip(book.description, 1200),
        "toc":        clip(book.toc, 1200),
    }

    # -----------------------------
    # 2) ë©”ì¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ì‹­ì§„ë¶„ë¥˜ë²•(KDC) ì „ë¬¸ê°€ì´ë‹¤.\n"
        "ì…ë ¥ëœ ë„ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **KDC 3ìë¦¬ ìˆ«ì**ë§Œ íŒë‹¨í•˜ì—¬ ì¶œë ¥í•œë‹¤.\n"
        "ë¶ˆí™•ì‹¤í•˜ë©´ ì •í™•íˆ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'ë§Œ ì¶œë ¥í•œë‹¤.\n"
        "ì„¤ëª…/ê·¼ê±°ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n"
        "ê·œì¹™:\n"
        "1. ë°˜ë“œì‹œ **3ìë¦¬ ìˆ«ìë§Œ** ì¶œë ¥. ì˜ˆ: 813 / 181 / 325\n"
        "2. ë¬¸í•™(800)ì€ ì–¸ì–´/ì§€ì—­ êµ¬ë¶„ ê³ ë ¤.\n"
        "3. ê·¸ë˜ë„ íŒë‹¨ì´ ì–´ë ¤ìš°ë©´ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'ë§Œ ì¶œë ¥.\n"
        "4. 653 í‚¤ì›Œë“œëŠ” ë³´ì¡° ì‹ í˜¸ì´ë©°, ë³¸ë¬¸ ë‚´ìš©ê³¼ ì¶©ëŒí•˜ë©´ ë¬´ì‹œ.\n"
    )

    hint_str = ", ".join(keywords_hint or [])

    # -----------------------------
    # 3) ì‚¬ìš©ì ë©”ì‹œì§€ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    user_prompt = (
        "ë‹¤ìŒ ë„ì„œ ì •ë³´(JSON)ë¥¼ ë°”íƒ•ìœ¼ë¡œ KDC 3ìë¦¬ ì •ìˆ˜ë§Œ ì¶œë ¥í•˜ë¼.\n"
        f"653 í‚¤ì›Œë“œ íŒíŠ¸: {hint_str or '(ì—†ìŒ)'}\n\n"
        f"{json.dumps(payload, ensure_ascii=False, indent=2)}\n\n"
        "ì¶œë ¥ ì˜ˆì‹œ: 823 / 813 / 325 / 181 / ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"
    )

    # -----------------------------
    # ì‘ë‹µ íŒŒì‹±ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    def _parse_response(s: str) -> Optional[str]:
        if not s:
            return None
        s = s.strip()

        if "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ" in s:
            return "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"

        m = re.search(r"(?<!\d)(\d{1,3})(?!\d)", s)
        if not m:
            return None

        num = m.group(1).zfill(3)
        if not re.fullmatch(r"\d{3}", num):
            return None
        return num

    # -----------------------------
    # LLM í˜¸ì¶œê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    def _call_llm(sys_p, user_p, max_tokens):
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": sys_p},
                    {"role": "user", "content": user_p},
                ],
                "temperature": 0.0,
                "max_tokens": max_tokens,
            },
            timeout=45,
        )
        resp.raise_for_status()
        txt = resp.json()["choices"][0]["message"]["content"].strip()

        code = _parse_response(txt)
        if not code:
            return None

        # ì–¸ì–´ ê¸°ë°˜ ë¬¸í•™ê³„ ì¬ì •ë ¬
        marc041 = getattr(book, "marc041", "") or getattr(book, "field_041", "") or ""
        return _rebase_8xx_with_language(code, marc041)

    # -----------------------------
    # 1ì°¨ LLM í˜¸ì¶œ
    # -----------------------------
    try:
        code = _call_llm(sys_prompt, user_prompt, max_tokens=16)
        if code:
            return code
    except Exception as e:
        st.warning(f"1ì°¨ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    # -----------------------------
    # 2ì°¨ í´ë°± í˜¸ì¶œ
    # -----------------------------
    fb_sys = (
        "ë„ˆëŠ” KDC ì‚¬ì„œì´ë‹¤. "
        "ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ **3ìë¦¬ ì •ìˆ˜** ë˜ëŠ” 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'ë§Œ ì¶œë ¥í•˜ë¼."
    )
    fb_user = f"{json.dumps(payload, ensure_ascii=False)}"
    try:
        code = _call_llm(fb_sys, fb_user, max_tokens=8)
        if code:
            return code
    except Exception as e:
        st.error(f"2ì°¨ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    # -----------------------------
    # 3ì°¨ ë¡œì»¬ í´ë°±
    # -----------------------------
    return "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"
def get_kdc_from_isbn(
    isbn13: str,
    ttbkey: Optional[str],
    openai_key: str,
    model: str,
    keywords_hint: list[str] | None = None,
) -> Optional[str]:

    # 1ì°¨: ì•Œë¼ë”˜ API
    info = aladin_lookup_by_api(isbn13, ttbkey) if ttbkey else None

    # 2ì°¨: ì›¹ ìŠ¤í¬ë ˆì´í•‘
    if not info:
        info = aladin_lookup_by_web(isbn13)

    if not info:
        st.warning("ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

    # LLM í˜¸ì¶œ
    code = ask_llm_for_kdc(
        info,
        api_key=openai_key,
        model=model,
        keywords_hint=keywords_hint
    )

    # ìµœì¢… ê²€ì¦
    if code and not re.fullmatch(r"\d{1,3}", code) and code != "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ":
        return None

    return code
# ==========================================================
# ğŸ”¤ ì›ë³¸ ì–¸ì–´ ê°ì§€ê¸° (ë‹¨ìˆœ ë¬¸ì ê¸°ë°˜, ì›ë³¸ ë¡œì§ ìœ ì§€)
# ==========================================================

LANG_MAP = {
    "kor": "í•œêµ­ì–´",
    "eng": "ì˜ì–´",
    "jpn": "ì¼ë³¸ì–´",
    "chi": "ì¤‘êµ­ì–´",
    "rus": "ëŸ¬ì‹œì•„ì–´",
    "ara": "ì•„ëì–´",
    "fre": "í”„ë‘ìŠ¤ì–´",
    "ger": "ë…ì¼ì–´",
    "ita": "ì´íƒˆë¦¬ì•„ì–´",
    "spa": "ìŠ¤í˜ì¸ì–´",
    "und": "ì•Œ ìˆ˜ ì—†ìŒ",
}

def detect_language_simple(text: str) -> str:
    """
    ì›ë³¸ ì½”ë“œì˜ rule-based ì–¸ì–´ ê°ì§€ ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ.
    """
    if not text:
        return "und"

    s = re.sub(r'[\s\W_]+', '', text)
    if not s:
        return "und"

    ch = s[0]

    if '\uac00' <= ch <= '\ud7a3':
        return "kor"
    elif '\u3040' <= ch <= '\u30ff':       # ì¼ë³¸ ê°€ë‚˜
        return "jpn"
    elif '\u4e00' <= ch <= '\u9fff':       # ì¤‘êµ­ í•œì
        return "chi"
    elif '\u0400' <= ch <= '\u04FF':       # ëŸ¬ì‹œì•„/í‚¤ë¦´
        return "rus"
    elif 'a' <= ch.lower() <= 'z':
        return "eng"

    return "und"
# ==========================================================
# fastText ê¸°ë°˜ ê³ ê¸‰ ì–¸ì–´ê°ì§€ (ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìš°ì„  ì ìš©)
# ==========================================================

try:
    import fasttext
    _FT_MODEL = fasttext.load_model("./lid.176.bin")  # í•„ìš” ì‹œ ê²½ë¡œ ë³€ê²½
except Exception:
    _FT_MODEL = None


def detect_language(text: str) -> str:
    """
    FastText â†’ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ì˜ rule-based
    """
    if _FT_MODEL:
        try:
            pred = _FT_MODEL.predict(text.replace("\n", " ")[:2000])
            label = pred[0][0].replace("__label__", "")
            # fastTextëŠ” eng, kor, jpn ë“±ì˜ ì•½ì–´ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return label.lower()
        except Exception:
            pass

    # fallback: ì›ë³¸ ê·œì¹™
    return detect_language_simple(text)
# ==========================================================
# 041 ìƒì„±ê¸°
# ==========================================================

def build_041_kormarc(text_content: str,
                      original_title: str = "",
                      use_fasttext=True) -> str:
    """
    text_content: ì±… ì„¤ëª…Â·ëª©ì°¨Â·ì œëª© ë“± ë³¸ë¬¸ ì–¸ì–´ ê°ì§€
    original_title: ì›ì œ ê°ì§€(ë²ˆì—­ì„œì¼ ê²½ìš°)
    """
    lang_main = detect_language(text_content)
    lang_orig = detect_language(original_title) if original_title else None

    # ë³¸ë¬¸ ì–¸ì–´ê°€ ì—†ë‹¤ë©´ und â†’ korë¡œ ê¸°ë³¸ê°’ ì„¤ì •(ì›ë³¸ ë¡œì§)
    if lang_main == "und":
        lang_main = "kor"

    parts = [f"$a{lang_main}"]
    if original_title and lang_orig:
        if lang_orig != lang_main:
            parts.append(f"$h{lang_orig}")

    return "=041  \\\\" + "".join(parts)
# ==========================================================
# 546 ìƒì„±ê¸° (ì›ë³¸ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€)
# ==========================================================

def build_546_from_041(marc041: str) -> str:
    if not marc041:
        return "=546  \\\\$aì–¸ì–´ ì •ë³´ ì—†ìŒ"

    a_codes = re.findall(r"\$a([a-z]{3})", marc041, re.I)
    h_match = re.search(r"\$h([a-z]{3})", marc041, re.I)
    h_code = h_match.group(1) if h_match else None

    if len(a_codes) == 1:
        a = LANG_MAP.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h = LANG_MAP.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"=546  \\\\$a{h} ì›ì‘ì„ {a}ë¡œ ë²ˆì—­"
        return f"=546  \\\\$a{a}ë¡œ ì”€"

    if len(a_codes) >= 2:
        langs = [LANG_MAP.get(c, "ì•Œ ìˆ˜ ì—†ìŒ") for c in a_codes]
        return f"=546  \\\\$a{'ã€'.join(langs)} ë³‘ê¸°"

    return "=546  \\\\$aì–¸ì–´ ì •ë³´ ì—†ìŒ"
# ==========================================================
# ISBN ê¸°ë°˜ â†’ (041, 546) ìƒì„± ì „ì²´ íŒŒì´í”„ë¼ì¸
# (ë„ˆê°€ ì¤€ ì›ë³¸ generate_all_oneclickì˜ íë¦„ 100% ë™ì¼)
# ==========================================================

def build_041_546_pipeline(item: dict, original_title_from_web: str = ""):
    """
    item: ì•Œë¼ë”˜ API item dict
    original_title_from_web: ì•Œë¼ë”˜ ìƒì„¸ HTML íŒŒì‹±ì—ì„œ ì°¾ì•„ë‚¸ ì›ì œ
    """
    title = item.get("title", "") or ""
    desc  = item.get("description", "") or ""
    toc   = (item.get("subInfo") or {}).get("toc", "") or ""

    content_blob = " ".join([title, desc, toc])

    tag041 = build_041_kormarc(
        text_content=content_blob,
        original_title=original_title_from_web
    )
    tag546 = build_546_from_041(tag041)

    return tag041, tag546
# ==========================================================
# ì—­í• ì–´ ì œê±° ë° ì›ì‹œ ì €ì ë¬¸ìì—´ ì •ë¦¬
# ==========================================================

ROLE_PATTERNS = [
    r"\bì €ì\b", r"\bì§€ì€ì´\b", r"\bì§€ìŒ\b", r"\bê¸€\b", r"\bê¸€Â·ê·¸ë¦¼\b",
    r"\bê·¸ë¦¼\b", r"\bì˜®ê¹€\b", r"\bì˜®ê¸´ì´\b", r"\bí¸\b", r"\bì—®ìŒ\b",
    r"\bì—­\b", r"\btranslator\b", r"\bí¸ì§‘\b",
]

def clean_author_role(raw: str) -> str:
    if not raw:
        return ""
    s = raw
    for pat in ROLE_PATTERNS:
        s = re.sub(pat, "", s, flags=re.IGNORECASE)
    s = re.sub(r"[\/\|]", ";", s)     # / â†’ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„í•  ë™ì¼í™”
    s = re.sub(r"\s+", " ", s).strip()
    return s
# ==========================================================
# ì €ìëª… ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
# ==========================================================

def split_authors(raw: str) -> list[str]:
    if not raw:
        return []

    s = clean_author_role(raw)

    parts = []
    for chunk in re.split(r";", s):
        chunk = chunk.strip()
        if not chunk:
            continue
        # ì½¤ë§ˆ ê¸°ë°˜ ë¶„í• ì€ ì´ë¦„ êµ¬ì¡°ë¥¼ í•´ì¹  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œ ì ìš©
        sub = [c.strip() for c in chunk.split(",") if c.strip()]
        if len(sub) == 1:
            parts.append(sub[0])
        else:
            parts.extend(sub)
    return parts
# ==========================================================
# ì´ë¦„ ì •ë ¬í˜• ìƒì„±
# ==========================================================

def is_east_asian(name: str) -> bool:
    if not name:
        return False
    # í•œê¸€ / í•œì / ì¼ë³¸ ê°€ë‚˜ í¬í•¨ ì‹œ True
    if any('\uac00' <= ch <= '\ud7a3' for ch in name):
        return True
    if any('\u4e00' <= ch <= '\u9fff' for ch in name):
        return True
    if any('\u3040' <= ch <= '\u30ff' for ch in name):
        return True
    return False

def to_sort_form(name: str) -> str:
    """
    ë™ì•„ì‹œì•„ ì´ë¦„ì€ ê·¸ëŒ€ë¡œ.
    ì•ŒíŒŒë²³ ê¸°ë°˜ ì´ë¦„ì€ 'ì„±, ì´ë¦„'ìœ¼ë¡œ ë³€í™˜.
    """
    if not name:
        return ""

    if is_east_asian(name):
        return name.strip()

    parts = name.split()
    if len(parts) == 1:
        return name.strip()

    last = parts[-1]
    first = " ".join(parts[:-1])
    return f"{last}, {first}"
# ==========================================================
# 100/700 ìƒì„±ê¸° (ì›ë³¸ ê·œì¹™ 100% ìœ ì§€)
# ==========================================================

def build_100_and_700(authors: list[str], origin_lang_code: str | None = None):
    """
    authors = ['í™ê¸¸ë™', 'John Smith', 'å±±ç”°å¤ªéƒ', ...]
    origin_lang_code: 041 $h â†’ ë²ˆì—­ì„œ ì—¬ë¶€ íŒë‹¨
    """
    if not authors:
        return None, []

    main_author = authors[0]
    rest = authors[1:]

    # 100 í•„ë“œ ìƒì„±
    sort_main = to_sort_form(main_author)
    tag_100 = f"=100  1\\\\$a{sort_main}"

    # ë²ˆì—­ì„œ ì—¬ë¶€
    is_translation = bool(origin_lang_code)

    tag_700_list = []
    for name in rest:
        sort_name = to_sort_form(name)
        if is_translation:
            tag = f"=700  1\\\\$a{sort_name}$eë²ˆì—­"
        else:
            tag = f"=700  1\\\\$a{sort_name}"
        tag_700_list.append(tag)

    return tag_100, tag_700_list
# ==========================================================
# ì•Œë¼ë”˜ item.author â†’ 100/700 ì „ì²´ ìƒì„±
# ==========================================================

def build_people_fields_from_aladin(item: dict, origin_lang_code: str | None = None):
    raw = (item or {}).get("author", "") or ""
    authors = split_authors(raw)

    tag100, tag700_list = build_100_and_700(authors, origin_lang_code)
    return tag100, tag700_list
# ==========================================================
# GPT í˜¸ì¶œ í•¨ìˆ˜ (ì›ë³¸ í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •)
# ==========================================================

def generate_653_with_gpt(
    category: str,
    title: str,
    authors: str,
    description: str,
    toc: str,
    max_keywords: int = 7,
) -> str:
    """
    ì›ë³¸ ì½”ë“œì—ì„œ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆëŠ” í•¨ìˆ˜.
    ê²°ê³¼ ì˜ˆ: "$aì•„ë™ë¬¸í•™$aì •ì„œì¡°ì ˆ$aì‹œê°„ê´€ë¦¬"
    """
    raise NotImplementedError  # True Patchì—ì„œëŠ” ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
# ==========================================================
# 653 íƒœê·¸ ìƒì„±ê¸° (ì›ë³¸ ì½”ë“œ 100% ë³´ì¡´)
# ==========================================================

def build_653_tag(item: dict) -> str | None:
    """
    item: ì•Œë¼ë”˜ item(dict)
    GPTê°€ ìƒì„±í•œ "$aí‚¤ì›Œë“œ$a..." í˜•íƒœë¥¼ ê·¸ëŒ€ë¡œ ë°›ì•„
    =653  \\$aí‚¤ì›Œë“œ$aí‚¤ì›Œë“œâ€¦ í˜•íƒœë¡œ ë˜í•‘í•˜ì—¬ ë°˜í™˜.
    """
    if not item:
        return None

    title = item.get("title", "") or ""
    category = item.get("categoryName", "") or ""
    raw_author = item.get("author", "") or ""
    desc = item.get("description", "") or ""
    toc = (item.get("subInfo") or {}).get("toc", "") or ""

    kwline = generate_653_with_gpt(
        category=category,
        title=title,
        authors=clean_author_role(raw_author),
        description=desc,
        toc=toc,
        max_keywords=7,
    )

    if not kwline:
        return None

    # ì›ë³¸ ë¡œì§: ê³µë°± ì œê±° í›„ ë˜í•‘
    kwline = kwline.replace(" ", "")
    return f"=653  \\\\{kwline}"
# ==========================================================
# 653 íŒŒì‹± â†’ ì…ë ¥ ìˆœì„œ ì•ˆì •ì„±ê³¼ ì¤‘ë³µ ì œê±° + ìµœëŒ€ 7ê°œ
# ==========================================================

def parse_653_keywords(tag_653: str | None) -> list[str]:
    """
    ì˜ˆ:
    '=653  \\$aì•„ë™ë¬¸í•™$aì •ì„œ$aì‹œê°„ê´€ë¦¬'
    â†’ ['ì•„ë™ë¬¸í•™','ì •ì„œ','ì‹œê°„ê´€ë¦¬']
    """
    if not tag_653:
        return []

    s = tag_653.strip()

    # ì ‘ë‘ë¶€ ì œê±° (=653  \\)
    s = re.sub(r"^=653\s+\\\\", "", s)

    kws = []
    for m in re.finditer(r"\$a([^$]+)", s):
        w = (m.group(1) or "").strip()
        if w:
            kws.append(w)

    # ì¤‘ë³µ ì œê±° + ìµœëŒ€ 7
    seen = set()
    out = []
    for w in kws:
        if w not in seen:
            seen.add(w)
            out.append(w)
        if len(out) >= 7:
            break

    return out
# ==========================================================
# LLM(056 KDC) íŒíŠ¸ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì •ê·œí™” (ì›ë³¸ ë¡œì§ ìœ ì§€)
# ==========================================================

def normalize_653_keywords_for_hint(kws: list[str]) -> list[str]:
    seen = set()
    out = []
    for w in (kws or []):
        w = (w or "").strip()
        if w and w not in seen:
            seen.add(w)
            out.append(w)
    return sorted(out)[:7]
# ==========================================================
# ì•Œë¼ë”˜ item â†’ 653 íƒœê·¸ + LLM íŒíŠ¸ ì „ì²´ íŒŒì´í”„ë¼ì¸
# ==========================================================

def build_653_pipeline(item: dict):
    tag_653 = build_653_tag(item)
    if not tag_653:
        return None, []

    kws_raw = parse_653_keywords(tag_653)
    kws_hint = normalize_653_keywords_for_hint(kws_raw)

    return tag_653, kws_hint
