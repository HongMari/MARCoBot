# ==========================================================
# field_builders.py  â€” Block A
# ì›ë³¸ ì½”ë“œì˜ "041/546 ìƒì„±", "ì–¸ì–´ ê°ì§€", "008 ìƒì„± ì „ê¹Œì§€"
# ëª¨ë“  ë¡œì§ì„ ì›ë³¸ ê·¸ëŒ€ë¡œ ë¶„ë¦¬
# ==========================================================

import re
import datetime
from collections import Counter
from dataclasses import dataclass
from typing import Optional, Dict, Any
from bs4 import BeautifulSoup

from pymarc import Field, Subfield
import requests

# ============================
# 041/546 ê´€ë ¨ ìœ í‹¸
# ============================

ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}

def detect_language(text):
    """
    ì›ë³¸ detect_language ê·¸ëŒ€ë¡œ.
    """
    text = re.sub(r'[\s\W_]+', '', text)
    if not text:
        return 'und'
    first_char = text[0]
    if '\uac00' <= first_char <= '\ud7a3':
        return 'kor'
    elif '\u3040' <= first_char <= '\u30ff':
        return 'jpn'
    elif '\u4e00' <= first_char <= '\u9fff':
        return 'chi'
    elif '\u0400' <= first_char <= '\u04FF':
        return 'rus'
    elif 'a' <= first_char.lower() <= 'z':
        return 'eng'
    else:
        return 'und'


def generate_546_from_041_kormarc(marc_041: str) -> str:
    """
    ì›ë³¸ generate_546_from_041_kormarc ê·¸ëŒ€ë¡œ ì´ë™.
    """
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"):
            a_codes.append(part[2:])
        elif part.startswith("$h"):
            h_code = part[2:]

    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{h_lang} ì›ì‘ì„ {a_lang}ë¡œ ë²ˆì—­"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"

    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"


def _lang3_from_tag041(tag_041: str | None) -> str | None:
    """
    =041 0\$akor$heng â†’ 'kor'
    ì›ë³¸ ê·¸ëŒ€ë¡œ.
    """
    if not tag_041:
        return None
    m = re.search(r"\$a([a-z]{3})", tag_041, flags=re.I)
    return m.group(1).lower() if m else None


# ==========================================================
# 653 ì „ì²˜ë¦¬ + GPT 653 (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================

def extract_keywords_from_text(text, top_n=7):
    words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
    filtered = [w for w in words if len(w) > 1]
    freq = Counter(filtered)
    return [kw for kw, _ in freq.most_common(top_n)]

def clean_keywords(words):
    stopwords = {"ì•„ì£¼", "ê°€ì§€", "í•„ìš”í•œ", "ë“±", "ìœ„í•´", "ê²ƒ", "ìˆ˜", "ë”", "ì´ëŸ°", "ìˆë‹¤", "ëœë‹¤", "í•œë‹¤"}
    return [w for w in words if w not in stopwords and len(w) > 1]


# -------------------------- ë‚´ë¶€ ì „ì²˜ë¦¬ --------------------------

def _norm(text: str) -> str:
    import unicodedata
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text).lower()
    text = re.sub(r"[^\w\s\uac00-\ud7a3]", " ", text)
    return re.sub(r"\s+", " ", text).strip()

def _clean_author_str(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\(.*?\)", " ", s)
    s = re.sub(r"[/;Â·,]", " ", s)
    return re.sub(r"\s+", " ", s).strip()

def _build_forbidden_set(title: str, authors: str) -> set:
    t_norm = _norm(title)
    a_norm = _norm(authors)
    forb = set()
    if t_norm:
        forb.update(t_norm.split())
        forb.add(t_norm.replace(" ", ""))
    if a_norm:
        forb.update(a_norm.split())
        forb.add(a_norm.replace(" ", ""))
    return {f for f in forb if f and len(f) >= 2}

def _should_keep_keyword(kw: str, forbidden: set) -> bool:
    n = _norm(kw)
    if not n or len(n.replace(" ", "")) < 2:
        return False
    for tok in forbidden:
        if tok in n or n in tok:
            return False
    return True


# -------------------------- GPT 653 í•µì‹¬ --------------------------

def generate_653_with_gpt(category, title, authors, description, toc, max_keywords=7):
    """
    ì›ë³¸ generate_653_with_gpt ê·¸ëŒ€ë¡œ.
    """
    import json
    import openai

    parts = [p.strip() for p in (category or "").split(">") if p.strip()]
    cat_tail = " ".join(parts[-2:]) if len(parts) >= 2 else (parts[-1] if parts else "")

    forbidden = _build_forbidden_set(title, authors)
    forbidden_list = ", ".join(sorted(forbidden)) or "(ì—†ìŒ)"

    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ KORMARC ì‘ì„± ê²½í—˜ì´ í’ë¶€í•œ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ë¶„ë¥˜ ì •ë³´, ì„¤ëª…, ëª©ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'MARC 653 ììœ ì£¼ì œì–´'ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.\n\n"
            "(ì¤‘ëµ â€” ì›ë³¸ ì „ì²´ ê·¸ëŒ€ë¡œ ìœ ì§€)"
        )
    }

    user_msg = {
        "role": "user",
        "content": (
            f"- ë¶„ë¥˜: {category}\n"
            f"- ì œëª©: {title}\n"
            f"- ì €ì: {authors}\n"
            f"- ì„¤ëª…: {description}\n"
            f"- ëª©ì°¨: {toc}\n"
            f"- ê¸ˆì¹™ì–´: {forbidden_list}\n"
            "(ì´í•˜ ì›ë³¸ ê·¸ëŒ€ë¡œ)"
        )
    }

    try:
        resp = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=180,
        )
        raw = (resp.choices[0].message["content"] or "").strip()

        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]
        if not kws:
            tmp = re.split(r"[,\n;|/Â·]", raw)
            kws = [t.strip().lstrip("$a") for t in tmp if t.strip()]

        kws = [kw.replace(" ", "") for kw in kws if kw]
        kws = [kw for kw in kws if _should_keep_keyword(kw, forbidden)]

        seen = set(); uniq = []
        for kw in kws:
            n = _norm(kw)
            if n not in seen:
                seen.add(n)
                uniq.append(kw)

        uniq = uniq[:max_keywords]
        return "".join(f"$a{kw}" for kw in uniq)

    except Exception:
        return None


# --------------------------------------------------------------
# GPT ê¸°ë°˜ 653 ìƒì„± â†’ =653 í˜•íƒœë¡œ wrapping
# --------------------------------------------------------------
def _build_653_via_gpt(item: dict) -> str | None:
    title = (item or {}).get("title", "") or ""
    category = (item or {}).get("categoryName", "") or ""
    raw_author = (item or {}).get("author", "") or ""
    desc = (item or {}).get("description", "") or ""
    toc = ((item or {}).get("subInfo", {}) or {}).get("toc", "") or ""

    kwline = generate_653_with_gpt(
        category=category,
        title=title,
        authors=_clean_author_str(raw_author),
        description=desc,
        toc=toc,
        max_keywords=7,
    )
    return f"=653  \\\\{kwline.replace(' ', '')}" if kwline else None


# ==========================================================
# 008 ìƒì„±ê¹Œì§€ì˜ ë„êµ¬ (country guess, illus, lit_form ë“±)
# ==========================================================

COUNTRY_FIXED = "ko "     # ì›ë³¸ ìƒë‹¨ ì •ì˜ ê·¸ëŒ€ë¡œ
LANG_FIXED = "kor"

KR_REGION_TO_CODE = {
    "ì„œìš¸": "ko ",
    "ë¶€ì‚°": "ko ",
    "ê²½ê¸°": "ko ",
    # ì›ë³¸ì—ì„œëŠ” í•œêµ­ ì¼ë°˜ ë¶€í˜¸ëŠ” ì“°ì§€ ì•Šë„ë¡ í•¨ â†’ ê·¸ëŒ€ë¡œ.
}


def extract_year_from_aladin_pubdate(pubdate_str: str) -> str:
    m = re.search(r"(19|20)\d{2}", pubdate_str or "")
    return m.group(0) if m else "19uu"


def guess_country3_from_place(place_str: str) -> str:
    if not place_str:
        return COUNTRY_FIXED
    for key, code in KR_REGION_TO_CODE.items():
        if key in place_str:
            return code
    return COUNTRY_FIXED


def detect_illus4(text: str) -> str:
    keys = []
    if re.search(r"ì‚½í™”|ì‚½ë„|ë„í•´|ì¼ëŸ¬ìŠ¤íŠ¸|ê·¸ë¦¼", text, re.I):
        keys.append("a")
    if re.search(r"ë„í‘œ|í‘œ|ì°¨íŠ¸|ê·¸ë˜í”„", text, re.I):
        keys.append("d")
    if re.search(r"ì‚¬ì§„|í¬í† |í™”ë³´|photo", text, re.I):
        keys.append("o")
    out = []
    for k in keys:
        if k not in out:
            out.append(k)
    return "".join(out)[:4]


def detect_index(text: str) -> str:
    return "1" if re.search(r"ìƒ‰ì¸|ì°¾ì•„ë³´ê¸°|index", text, re.I) else "0"


def detect_lit_form(title: str, category: str, extra_text: str = "") -> str:
    blob = f"{title} {category} {extra_text}"
    if re.search(r"ì„œê°„ì§‘|í¸ì§€|ì„œê°„ë¬¸", blob, re.I):
        return "i"
    if re.search(r"ê¸°í–‰|ì—¬í–‰ê¸°|ì¼ê¸°", blob, re.I):
        return "m"
    if re.search(r"ì‹œì§‘|ì‚°ë¬¸ì‹œ|poem|poetry", blob, re.I):
        return "p"
    if re.search(r"ì†Œì„¤|novel|fiction", blob, re.I):
        return "f"
    if re.search(r"ì—ì„¸ì´|ìˆ˜í•„|essay", blob, re.I):
        return "e"
    return " "


def detect_bio(text: str) -> str:
    if re.search(r"ìì„œì „|íšŒê³ ë¡", text, re.I):
        return "a"
    if re.search(r"ì „ê¸°|í‰ì „|biograph", text, re.I):
        return "b"
    if re.search(r"ì „ê¸°ì |ìì „ì |íšŒê³ ", text):
        return "d"
    return " "


def _is_unknown_place(s: str | None) -> bool:
    if not s:
        return False
    t = s.strip()
    t_no_sp = t.replace(" ", "")
    lower = t.lower()
    return (
        "ë¯¸ìƒ" in t or
        "ë¯¸ìƒ" in t_no_sp or
        "unknown" in lower or
        "place unknown" in lower
    )
# ==========================================================
# field_builders.py â€” Block B
# 008 ìƒì„± + ê°€ê²©/020/950 + KPIPA ì¶œíŒì§€ ì¶”ë¡  + 260 í•„ë“œ
# ì›ë³¸ ë¡œì§ 100% ê·¸ëŒ€ë¡œ
# ==========================================================

import re
import datetime
import pandas as pd
import requests
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials

from pymarc import Field, Subfield

from .utils import clean_text, convert_mm_to_cm

# ==========================================================
# 008 ë³¸ë¬¸(40ì) ìƒì„±ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================

def build_008_kormarc_bk(
    date_entered,          # YYMMDD
    date1,                 # ë°œí–‰ì—°ë„(4ìë¦¬)
    country3,              # ë°œí–‰êµ­ ë¶€í˜¸(3ì¹¸)
    lang3,                 # ì–¸ì–´ì½”ë“œ(3ì¹¸)
    date2="", illus4="", has_index="0",
    lit_form=" ", bio=" ", type_of_date="s",
    modified_record=" ", cataloging_src="a",
):
    def pad(s, n, fill=" "):
        s = "" if s is None else str(s)
        return (s[:n] + fill * n)[:n]

    if len(date_entered) != 6 or not date_entered.isdigit():
        raise ValueError("date_enteredëŠ” YYMMDD 6ìë¦¬ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
    if len(date1) != 4:
        raise ValueError("date1ì€ 4ìë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    body = "".join([
        date_entered,               # 00-05
        pad(type_of_date,1),        # 06
        date1,                      # 07-10
        pad(date2,4),               # 11-14
        pad(country3,3),            # 15-17
        pad(illus4,4),              # 18-21
        " " * 4,                    # 22-25
        " " * 2,                    # 26-27
        pad(modified_record,1),     # 28
        "0",                        # 29
        "0",                        # 30
        has_index if has_index in ("0","1") else "0",  # 31
        pad(cataloging_src,1),      # 32
        pad(lit_form,1),            # 33
        pad(bio,1),                 # 34
        pad(lang3,3),               # 35-37
        " " * 2                     # 38-39
    ])

    if len(body) != 40:
        raise AssertionError(f"008 length != 40: {len(body)}")

    return body


# ----------------------------------------------------------
# 008 ì „ì²´ ì¡°ë¦½ (ì›ë³¸ build_008_from_isbn ê·¸ëŒ€ë¡œ)
# ----------------------------------------------------------

def build_008_from_isbn(
    isbn: str,
    *,
    aladin_pubdate: str = "",
    aladin_title: str = "",
    aladin_category: str = "",
    aladin_desc: str = "",
    aladin_toc: str = "",
    source_300_place: str = "",
    override_country3: str = None,
    override_lang3: str = None,
    cataloging_src="a",
):
    today  = datetime.datetime.now().strftime("%y%m%d")
    date1  = extract_year_from_aladin_pubdate(aladin_pubdate)

    # --- ë°œí–‰êµ­ ë¶€í˜¸ ê²°ì • ---
    if override_country3:
        country3 = override_country3
    elif source_300_place:
        if _is_unknown_place(source_300_place):
            country3 = "   "
        else:
            guessed = guess_country3_from_place(source_300_place)
            country3 = guessed if guessed else COUNTRY_FIXED
    else:
        country3 = COUNTRY_FIXED

    # ì–¸ì–´ ìš°ì„ ìˆœìœ„: override > ê¸°ë³¸ê°’
    lang3 = override_lang3 or LANG_FIXED

    # ì‚½í™”, ìƒ‰ì¸, ë¬¸í•™í˜•ì‹, ì „ê¸°ê°ì§€
    bigtext = " ".join([aladin_title or "", aladin_desc or "", aladin_toc or ""])
    illus4    = detect_illus4(bigtext)
    has_index = detect_index(bigtext)
    lit_form  = detect_lit_form(aladin_title or "", aladin_category or "", bigtext)
    bio       = detect_bio(bigtext)

    return build_008_kormarc_bk(
        date_entered=today,
        date1=date1,
        country3=country3,
        lang3=lang3,
        illus4=illus4,
        has_index=has_index,
        lit_form=lit_form,
        bio=bio,
        cataloging_src=cataloging_src,
    )


# ==========================================================
# NLK(êµ­ë¦½ì¤‘ì•™ë„ì„œê´€) â€” EA_ADD_CODE, SET ISBN, ê°€ê²© PRE_PRICE
# ì›ë³¸ fetch_additional_code_from_nlk ê·¸ëŒ€ë¡œ
# ==========================================================

def fetch_additional_code_from_nlk(isbn: str) -> dict:
    attempts = [
        "https://seoji.nl.go.kr/landingPage/SearchApi.do",
        "https://www.nl.go.kr/seoji/SearchApi.do",
        "http://seoji.nl.go.kr/landingPage/SearchApi.do",
        "http://www.nl.go.kr/seoji/SearchApi.do",
    ]
    params = {
        "cert_key": NLK_CERT_KEY,
        "result_style": "json",
        "page_no": 1,
        "page_size": 1,
        "isbn": isbn.strip().replace("-", ""),
    }

    for base in attempts:
        try:
            r = requests.get(base, params=params, timeout=(5, 10))
            r.raise_for_status()
            j = r.json()

            doc = None
            if isinstance(j, dict):
                if "docs" in j and isinstance(j["docs"], list) and j["docs"]:
                    doc = j["docs"][0]
                elif "doc" in j and isinstance(j["doc"], list) and j["doc"]:
                    doc = j["doc"][0]
            if not doc:
                continue

            add_code = (doc.get("EA_ADD_CODE") or "").strip()
            set_isbn = (doc.get("SET_ISBN") or "").strip()
            price = (doc.get("PRE_PRICE") or "").strip()

            return {
                "add_code": add_code,
                "set_isbn": set_isbn,
                "price": price,
            }
        except Exception:
            continue

    return {
        "add_code": "",
        "set_isbn": "",
        "set_title": "",
        "price": "",
    }


# ==========================================================
# 020 í•„ë“œ ìƒì„±ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================

def _build_020_from_item_and_nlk(isbn: str, item: dict) -> str:
    price = str((item or {}).get("priceStandard", "") or "").strip()

    try:
        nlk_extra = fetch_additional_code_from_nlk(isbn) or {}
        add_code = nlk_extra.get("add_code", "")
        price_from_nlk = nlk_extra.get("price", "")
    except Exception:
        add_code = ""
        price_from_nlk = ""

    final_price = price or price_from_nlk

    parts = [f"=020  \\\\$a{isbn}"]
    if add_code:
        parts.append(f"$g{add_code}")
    if final_price:
        parts.append(f":$c{final_price}")

    return "".join(parts)


# ==========================================================
# 950 í•„ë“œ (ê°€ê²©) ìƒì„±ê¸° â€” ì›ë³¸ ê·¸ëŒ€ë¡œ
# ==========================================================

def _extract_price_kr(item: dict, isbn: str) -> str:
    raw = str((item or {}).get("priceStandard", "") or "").strip()

    if not raw:
        try:
            crawl = crawl_aladin_original_and_price(isbn) or {}
            raw = crawl.get("price", "").strip()
        except Exception:
            raw = ""

    digits = re.sub(r"[^\d]", "", raw)
    return digits


def build_950_from_item_and_price(item: dict, isbn: str) -> str:
    price = _extract_price_kr(item, isbn)
    if not price:
        return ""
    return f"=950  0\\$b\\{price}"


# ==========================================================
# ì¶œíŒì§€ ì¶”ì¶œ (KPIPA / IMPRINT / ë¬¸ì²´ë¶€ / FallBack)
# ==========================================================

def load_publisher_db():
    creds = ServiceAccountCredentials.from_json_keyfile_dict(
        st.secrets["gspread"],
        ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    )
    client = gspread.authorize(creds)
    sh = client.open("ì¶œíŒì‚¬ DB")

    pub_rows = sh.worksheet("ë°œí–‰ì²˜ëª…â€“ì£¼ì†Œ ì—°ê²°í‘œ").get_all_values()[1:]
    pub_rows_filtered = [row[1:3] for row in pub_rows]
    publisher_data = pd.DataFrame(pub_rows_filtered, columns=["ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ"])

    region_rows = sh.worksheet("ë°œí–‰êµ­ëª…â€“ë°œí–‰êµ­ë¶€í˜¸ ì—°ê²°í‘œ").get_all_values()[1:]
    region_rows_filtered = [row[:2] for row in region_rows]
    region_data = pd.DataFrame(region_rows_filtered, columns=["ë°œí–‰êµ­", "ë°œí–‰êµ­ ë¶€í˜¸"])

    imprint_frames = []
    for ws in sh.worksheets():
        if ws.title.startswith("ë°œí–‰ì²˜-ì„í”„ë¦°íŠ¸ ì—°ê²°í‘œ"):
            data = ws.get_all_values()[1:]
            imprint_frames.extend([row[0] for row in data if row])
    imprint_data = pd.DataFrame(imprint_frames, columns=["ì„í”„ë¦°íŠ¸"])

    return publisher_data, region_data, imprint_data


def normalize_publisher_name(name):
    return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬", "", name).lower()


def normalize_stage2(name):
    name = re.sub(r"(ì£¼ë‹ˆì–´|JUNIOR|ì–´ë¦°ì´|í‚¤ì¦ˆ|ë¶ìŠ¤|ì•„ì´ì„¸ì›€|í”„ë ˆìŠ¤)", "", name, flags=re.IGNORECASE)
    eng_to_kor = {"springer": "ìŠ¤í”„ë§ê±°", "cambridge": "ì¼€ì„ë¸Œë¦¬ì§€", "oxford": "ì˜¥ìŠ¤í¬ë“œ"}
    for eng, kor in eng_to_kor.items():
        name = re.sub(eng, kor, name, flags=re.IGNORECASE)
    return name.strip().lower()


def split_publisher_aliases(name):
    aliases = []
    bracket_contents = re.findall(r"\((.*?)\)", name)
    for content in bracket_contents:
        parts = re.split(r"[,/]", content)
        parts = [p.strip() for p in parts if p.strip()]
        aliases.extend(parts)

    name_no_brackets = re.sub(r"\(.*?\)", "", name).strip()
    if "/" in name_no_brackets:
        parts = [p.strip() for p in name_no_brackets.split("/") if p.strip()]
        rep_name = parts[0]
        aliases.extend(parts[1:])
    else:
        rep_name = name_no_brackets

    return rep_name, aliases


def search_publisher_location_with_alias(name, publisher_data):
    debug_msgs = []
    if not name:
        return "ì¶œíŒì§€ ë¯¸ìƒ", ["âŒ ê²€ìƒ‰ ì‹¤íŒ¨: ì…ë ¥ëœ ì¶œíŒì‚¬ëª…ì´ ì—†ìŒ"]

    norm_name = normalize_publisher_name(name)
    candidates = publisher_data[publisher_data["ì¶œíŒì‚¬ëª…"].apply(
        lambda x: normalize_publisher_name(x)) == norm_name]

    if not candidates.empty:
        address = candidates.iloc[0]["ì£¼ì†Œ"]
        debug_msgs.append(f"âœ… KPIPA DB ë§¤ì¹­ ì„±ê³µ: {name} â†’ {address}")
        return address, debug_msgs
    else:
        debug_msgs.append(f"âŒ KPIPA DB ë§¤ì¹­ ì‹¤íŒ¨: {name}")
        return "ì¶œíŒì§€ ë¯¸ìƒ", debug_msgs


def find_main_publisher_from_imprints(rep_name, imprint_data, publisher_data):
    norm_rep = normalize_publisher_name(rep_name)

    for full_text in imprint_data["ì„í”„ë¦°íŠ¸"]:
        if "/" in full_text:
            pub_part, imprint_part = [p.strip() for p in full_text.split("/", 1)]
        else:
            pub_part, imprint_part = full_text.strip(), None

        if imprint_part:
            norm_imprint = normalize_publisher_name(imprint_part)
            if norm_imprint == norm_rep:
                location, dbg = search_publisher_location_with_alias(pub_part, publisher_data)
                return location, dbg

    return None, [f"âŒ IM DB ê²€ìƒ‰ ì‹¤íŒ¨: ë§¤ì¹­ ì—†ìŒ ({rep_name})"]


def get_mcst_address(publisher_name):
    url = "https://book.mcst.go.kr/html/searchList.php"
    params = {"search_area": "ì „ì²´", "search_state": "1",
              "search_kind": "1", "search_type": "1",
              "search_word": publisher_name}
    debug_msgs = []

    try:
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        results = []
        for row in soup.select("table.board tbody tr"):
            cols = row.find_all("td")
            if len(cols) >= 4:
                reg_type = cols[0].get_text(strip=True)
                name = cols[1].get_text(strip=True)
                addr = cols[2].get_text(strip=True)
                status = cols[3].get_text(strip=True)

                if status == "ì˜ì—…":
                    results.append((reg_type, name, addr, status))

        if results:
            debug_msgs.append(f"[ë¬¸ì²´ë¶€] ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê±´")
            return results[0][2], results, debug_msgs
        else:
            debug_msgs.append("[ë¬¸ì²´ë¶€] ê²°ê³¼ ì—†ìŒ")
            return "ë¯¸í™•ì¸", [], debug_msgs
    except Exception as e:
        debug_msgs.append(f"[ë¬¸ì²´ë¶€] ì˜ˆì™¸: {e}")
        return "ì˜¤ë¥˜ ë°œìƒ", [], debug_msgs


def get_country_code_by_region(region_name, region_data):
    try:
        def normalize_region(region):
            region = (region or "").strip()
            if region.startswith(("ì „ë¼", "ì¶©ì²­", "ê²½ìƒ")):
                return region[0] + (region[2] if len(region) > 2 else "")
            return region[:2]

        normalized_input = normalize_region(region_name)

        for _, row in region_data.iterrows():
            sheet_region, code = row["ë°œí–‰êµ­"], row["ë°œí–‰êµ­ ë¶€í˜¸"]
            if normalize_region(sheet_region) == normalized_input:
                return code.strip() or "   "
        return "   "
    except Exception:
        return "   "


def build_pub_location_bundle(isbn, publisher_name_raw):
    debug = []

    try:
        publisher_data, region_data, imprint_data = load_publisher_db()
        debug.append("âœ“ êµ¬ê¸€ì‹œíŠ¸ DB ì ì¬ ì„±ê³µ")

        kpipa_full, kpipa_norm, err = get_publisher_name_from_isbn_kpipa(isbn)
        if err:
            debug.append(f"KPIPA ê²€ìƒ‰: {err}")

        rep_name, aliases = split_publisher_aliases(kpipa_full or publisher_name_raw or "")
        resolved_for_search = rep_name or (publisher_name_raw or "").strip()
        debug.append(f"ëŒ€í‘œ ì¶œíŒì‚¬ëª…: {resolved_for_search}")

        place_raw, msgs = search_publisher_location_with_alias(resolved_for_search, publisher_data)
        debug += msgs
        source = "KPIPA_DB"

        if place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ", None):
            place_raw, msgs = find_main_publisher_from_imprints(resolved_for_search, imprint_data, publisher_data)
            debug += msgs
            if place_raw:
                source = "IMPRINTâ†’KPIPA"

        if not place_raw or place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
            mcst_addr, _rows, dbg = get_mcst_address(resolved_for_search)
            debug += dbg
            if mcst_addr not in ("ë¯¸í™•ì¸", "ì˜¤ë¥˜ ë°œìƒ", None):
                place_raw = mcst_addr
                source = "MCST"

        if not place_raw or place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ", "ë¯¸í™•ì¸", "ì˜¤ë¥˜ ë°œìƒ"):
            place_raw = "ì¶œíŒì§€ ë¯¸ìƒ"
            source = "FALLBACK"
            debug.append("âš ï¸ ëª¨ë“  ê²½ë¡œ ì‹¤íŒ¨ â†’ 'ì¶œíŒì§€ ë¯¸ìƒ'")

        place_display = normalize_publisher_location_for_display(place_raw)
        country_code = get_country_code_by_region(place_raw, region_data)

        return {
            "place_raw": place_raw,
            "place_display": place_display,
            "country_code": country_code,
            "resolved_publisher": resolved_for_search,
            "source": source,
            "debug": debug,
        }

    except Exception as e:
        return {
            "place_raw": "ë°œí–‰ì§€ ë¯¸ìƒ",
            "place_display": "ë°œí–‰ì§€ ë¯¸ìƒ",
            "country_code": "   ",
            "resolved_publisher": publisher_name_raw or "",
            "source": "ERROR",
            "debug": [f"ì˜ˆì™¸: {e}"],
        }


def normalize_publisher_location_for_display(location_name):
    if not location_name or location_name in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
        return location_name

    location_name = location_name.strip()
    major = ["ì„œìš¸", "ì¸ì²œ", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ëŒ€êµ¬", "ë¶€ì‚°", "ì„¸ì¢…"]
    for city in major:
        if city in location_name:
            return location_name[:2]

    parts = location_name.split()
    loc = parts[1] if len(parts) > 1 else parts[0]
    if loc.endswith("ì‹œ"):
        loc = loc[:-1]
    return loc


# ==========================================================
# 260 í•„ë“œ ë¹Œë”
# ==========================================================

def build_260(place_display: str, publisher_name: str, pubyear: str):
    place = (place_display or "ë°œí–‰ì§€ ë¯¸ìƒ")
    pub = (publisher_name or "ë°œí–‰ì²˜ ë¯¸ìƒ")
    year = (pubyear or "ë°œí–‰ë…„ ë¯¸ìƒ")
    return f"=260  \\\\$a{place} :$b{pub},$c{year}"

# ==========================================================
# field_builders.py â€” Block C
# C1. 653 ìƒì„±ê¸° (GPT ê¸°ë°˜) + ì „ì²˜ë¦¬ ìœ í‹¸
# C2. 056(KDC) ìƒì„± ì „ì²´
# C3. 041/546 ì–¸ì–´ ê°ì§€ ë° í•„ë“œ ë¹Œë”
# C4. 245 / 246 / 700 / 90010 / 940 (ì œëª©Â·ì €ìÂ·ì—­ìÂ·LOD)
# C5. 300 ìƒì„¸ í˜•ì‹ íŒŒì„œ
# C6. ê°€ê²©/020/950 ëª¨ë“ˆ
# C7. KPIPA/MCST/ë°œí–‰ì§€ + 260 ëª¨ë“ˆ
# C8. 008 ìƒì„±ê¸° (lang3 override / country3 override í¬í•¨)
# C9. 056(KDC) GPT ë¶„ë¥˜ê¸° ì „ì²´ ëª¨ë“ˆ
# C10. 300(í˜•íƒœì‚¬í•­) í¬ë¡¤ëŸ¬ + íŒŒì„œ ëª¨ë“ˆ
# C11. 490/830 ì´ì„œ ëª¨ë“ˆ
# C12. ìµœì¢… MARC Builder ì¡°ë¦½ê¸°
# C13. 049 í•„ë“œ ìƒì„±ê¸°
# ì›ë³¸ ë¡œì§ 100% ê·¸ëŒ€ë¡œ
# ==========================================================
# C1. 653 ìƒì„±ê¸° (GPT ê¸°ë°˜) + ì „ì²˜ë¦¬ ìœ í‹¸
# Chunk C-1 : 653 í‚¤ì›Œë“œ ê´€ë ¨ ì „ì²˜ë¦¬ + GPT ê¸°ë°˜ 653 ìƒì„±ê¸°
def generate_653_with_gpt(category, title, authors, description, toc, max_keywords=7):
    import re
    from openai import OpenAI

    client = OpenAI()

    parts = [p.strip() for p in (category or "").split(">") if p.strip()]
    cat_tail = " ".join(parts[-2:]) if len(parts) >= 2 else (parts[-1] if parts else "")

    forbidden = _build_forbidden_set(title, authors)
    forbidden_list = ", ".join(sorted(forbidden)) or "(ì—†ìŒ)"

    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ KORMARC ì‘ì„± ê²½í—˜ì´ í’ë¶€í•œ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ì •ë³´ë¡œ 653 í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. "
            "í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ ë¶™ì—¬ì“°ê¸° í•˜ë©°, ëª…ì‚¬í˜• ê°œë…ìœ¼ë¡œë§Œ êµ¬ì„±í•©ë‹ˆë‹¤."
        )
    }

    user_msg = {
        "role": "user",
        "content": (
            f"ë¶„ë¥˜: {category}\n"
            f"í•µì‹¬ ë¶„ë¥˜ê¼¬ë¦¬: {cat_tail}\n"
            f"ì œëª©: {title}\n"
            f"ì €ì: {authors}\n"
            f"ì„¤ëª…: {description}\n"
            f"ëª©ì°¨: {toc}\n"
            f"ì œì™¸ì–´ ëª©ë¡: {forbidden_list}\n"
            "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì†Œ 1ê°œ~ìµœëŒ€ 7ê°œì˜ 653 í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n"
            "ë°˜ë“œì‹œ `$aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 ...` í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."
        )
    }

    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=200,
        )
        raw = (resp.choices[0].message.content or "").strip()

        # $a ì¶”ì¶œ
        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]

        # ë¶™ì—¬ì“°ê¸°
        kws = [kw.replace(" ", "") for kw in kws]

        # ê¸ˆì¹™ì–´ ì œê±°
        kws = [kw for kw in kws if _should_keep_keyword(kw, forbidden)]

        # ìµœëŒ€ 7ê°œ
        kws = kws[:max_keywords]

        return "".join(f"$a{kw}" for kw in kws)

    except Exception as e:
        st.warning(f"âš ï¸ 653 ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    
# GPT ê¸°ë°˜ 653 ìƒì„±ê¸°
def generate_653_with_gpt(category, title, authors, description, toc, max_keywords=7):
    import re
    from openai import OpenAI

    client = OpenAI()

    parts = [p.strip() for p in (category or "").split(">") if p.strip()]
    cat_tail = " ".join(parts[-2:]) if len(parts) >= 2 else (parts[-1] if parts else "")

    forbidden = _build_forbidden_set(title, authors)
    forbidden_list = ", ".join(sorted(forbidden)) or "(ì—†ìŒ)"

    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ KORMARC ì‘ì„± ê²½í—˜ì´ í’ë¶€í•œ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ì •ë³´ë¡œ 653 í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. "
            "í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ ë¶™ì—¬ì“°ê¸° í•˜ë©°, ëª…ì‚¬í˜• ê°œë…ìœ¼ë¡œë§Œ êµ¬ì„±í•©ë‹ˆë‹¤."
        )
    }

    user_msg = {
        "role": "user",
        "content": (
            f"ë¶„ë¥˜: {category}\n"
            f"í•µì‹¬ ë¶„ë¥˜ê¼¬ë¦¬: {cat_tail}\n"
            f"ì œëª©: {title}\n"
            f"ì €ì: {authors}\n"
            f"ì„¤ëª…: {description}\n"
            f"ëª©ì°¨: {toc}\n"
            f"ì œì™¸ì–´ ëª©ë¡: {forbidden_list}\n"
            "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì†Œ 1ê°œ~ìµœëŒ€ 7ê°œì˜ 653 í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n"
            "ë°˜ë“œì‹œ `$aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 ...` í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."
        )
    }

    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=200,
        )
        raw = (resp.choices[0].message.content or "").strip()

        # $a ì¶”ì¶œ
        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]

        # ë¶™ì—¬ì“°ê¸°
        kws = [kw.replace(" ", "") for kw in kws]

        # ê¸ˆì¹™ì–´ ì œê±°
        kws = [kw for kw in kws if _should_keep_keyword(kw, forbidden)]

        # ìµœëŒ€ 7ê°œ
        kws = kws[:max_keywords]

        return "".join(f"$a{kw}" for kw in kws)

    except Exception as e:
        st.warning(f"âš ï¸ 653 ìƒì„± ì‹¤íŒ¨: {e}")
        return None
# 653 â†’ MRK
def _build_653_via_gpt(item: dict) -> str | None:
    title = (item or {}).get("title","") or ""
    category = (item or {}).get("categoryName","") or ""
    raw_author = (item or {}).get("author","") or ""
    desc = (item or {}).get("description","") or ""
    toc  = ((item or {}).get("subInfo",{}) or {}).get("toc","") or ""

    kwline = generate_653_with_gpt(
        category=category,
        title=title,
        authors=_clean_author_str(raw_author),
        description=desc,
        toc=toc,
        max_keywords=7
    )
    return f"=653  \\\\{kwline}" if kwline else None


def _parse_653_keywords(tag_653: str | None) -> list[str]:
    if not tag_653:
        return []
    s = re.sub(r"^=653\s+\\\\", "", tag_653.strip())

    kws = []
    for m in re.finditer(r"\$a([^$]+)", s):
        w = (m.group(1) or "").strip()
        if w:
            kws.append(w)

    # ì¤‘ë³µ ì œê±°
    seen, out = set(), []
    for w in kws:
        if w not in seen:
            seen.add(w)
            out.append(w)
        if len(out) >= 7:
            break
    return out

# C2. 056(KDC) ìƒì„± ì „ì²´
# Chunk C-2-A : 041 ê¸°ë°˜ ë¬¸í•™ ì¬ì •ë ¬ ìœ í‹¸ë¦¬í‹°
# ==========================================================
# 041 ì›ì‘ì–¸ì–´ ê¸°ë°˜ â†’ ë¬¸í•™(8xx) ì¬ì •ë ¬ ë¡œì§ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================
def _parse_marc_041_original(marc041: str):
    """
    MARC 041ì—ì„œ ì›ì‘ ì–¸ì–´($h)ë¥¼ ì¶”ì¶œí•œë‹¤.
    ì˜ˆ: '041 0\\$akor$heng' -> 'eng'
    """
    if not marc041:
        return None
    s = marc041.lower()
    m = re.search(r"\$h([a-z]{3})", s)
    return m.group(1) if m else None


def _lang3_to_kdc_lit_base(lang3: str):
    """
    ì›ì‘ ì–¸ì–´ì½”ë“œ â†’ ë¬¸í•™ê³„ì—´ ê¸°ë³¸ 2ìë¦¬ ë§¤í•‘.
    (ì›ë³¸ ë¡œì§ 100% ìœ ì§€)
    """
    if not lang3:
        return None
    l = lang3.lower()

    if l in {"eng"}:
        return "84"   # ì˜ë¯¸ë¬¸í•™
    if l in {"kor"}:
        return "81"   # í•œêµ­ë¬¸í•™
    if l in {"chi", "zho"}:
        return "82"   # ì¤‘êµ­ë¬¸í•™
    if l in {"jpn"}:
        return "83"   # ì¼ë³¸ë¬¸í•™
    if l in {"deu", "ger"}:
        return "85"   # ë…ì¼ë¬¸í•™
    if l in {"fre"}:
        return "86"   # í”„ë‘ìŠ¤ë¬¸í•™
    if l in {"spa", "por"}:
        return "87"   # ìŠ¤í˜ì¸/í¬ë¥´íˆ¬ê°ˆë¬¸í•™
    if l in {"ita"}:
        return "88"   # ì´íƒˆë¦¬ì•„ë¬¸í•™

    return "89"        # ê¸°íƒ€ ë¬¸í•™
    
# Chunk C-2-B : ë¬¸í•™ì½”ë“œ ì¬ì •ë ¬ê¸° (ì›ë³¸ ìœ ì§€)
def _rebase_8xx_with_language(code: str, marc041: str) -> str:
    """
    056 ê²°ê³¼ê°€ ë¬¸í•™(8xx)ì¼ ë•Œ,
    041 $h ì›ì‘ì–¸ì–´ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬ ë³€ê²½.
    - ì¥ë¥´(ì„¸ ë²ˆì§¸ ìë¦¬) ê·¸ëŒ€ë¡œ ìœ ì§€
    - ì• ë‘ ìë¦¬ë§Œ ë³€ê²½
    """
    if not code or len(code) < 3 or code[0] != "8":
        return code  # ë¬¸í•™ì´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ìœ ì§€

    # ì›ì‘ì–¸ì–´ ì¶”ì¶œ
    orig_lang = _parse_marc_041_original(marc041 or "")
    base2 = _lang3_to_kdc_lit_base(orig_lang) if orig_lang else None
    if not base2:
        return code

    # 813.7 â†’ 813 ê·¸ëŒ€ë¡œ ì²˜ë¦¬
    m = re.match(r"^(\d{3})(\..+)?$", code)
    if not m:
        return code

    head3, tail = m.group(1), (m.group(2) or "")
    genre = head3[2]       # ë¬¸í•™ ì¥ë¥´ ë””ì§“ (1=ì‹œ, 3=ì†Œì„¤ â€¦)

    return base2 + genre
    
# Chunk C-2-C : LLM KDC íŒë‹¨ê¸° (ì›ë³¸ ë¡œì§ ê·¸ëŒ€ë¡œ ëª¨ë“ˆí™”)
# ==========================================================
# KDC íŒë‹¨ LLM í˜¸ì¶œê¸° â€” í•µì‹¬ í•¨ìˆ˜
# (ë„ˆê°€ ì¤€ ì›ë³¸ ë¡œì§ì„ ì™„ì „íˆ ë¶„ë¦¬í•˜ì—¬ êµ¬ì¡°í™”)
# ==========================================================

def ask_llm_for_kdc(
    book: BookInfo,
    api_key: str,
    model: str = DEFAULT_MODEL,
    keywords_hint: list[str] | None = None
) -> Optional[str]:
    """
    KDC(056) íŒë‹¨ì„ LLMì—ê²Œ ìš”ì²­.
    ë°˜í™˜: 3ìë¦¬ ìˆ«ì ë¬¸ìì—´ or 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'
    """

    # -----------------------------
    # 1) ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    def clip(s: str, n: int) -> str:
        if not s:
            return ""
        s = str(s).strip()
        return s if len(s) <= n else s[:n] + "â€¦"

    payload = {
        "title":      clip(book.title, 160),
        "author":     clip(book.author, 120),
        "publisher":  book.publisher,
        "pub_date":   book.pub_date,
        "isbn13":     book.isbn13,
        "category":   clip(book.category, 160),
        "description": clip(book.description, 1200),
        "toc":        clip(book.toc, 1200),
    }

    # -----------------------------
    # 2) ë©”ì¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ì‹­ì§„ë¶„ë¥˜ë²•(KDC) ì „ë¬¸ê°€ì´ë‹¤.\n"
        "ì…ë ¥ëœ ë„ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **KDC 3ìë¦¬ ìˆ«ì**ë§Œ íŒë‹¨í•˜ì—¬ ì¶œë ¥í•œë‹¤.\n"
        "ë¶ˆí™•ì‹¤í•˜ë©´ ì •í™•íˆ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'ë§Œ ì¶œë ¥í•œë‹¤.\n"
        "ì„¤ëª…/ê·¼ê±°ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n"
        "ê·œì¹™:\n"
        "1. ë°˜ë“œì‹œ **3ìë¦¬ ìˆ«ìë§Œ** ì¶œë ¥. ì˜ˆ: 813 / 181 / 325\n"
        "2. ë¬¸í•™(800)ì€ ì–¸ì–´/ì§€ì—­ êµ¬ë¶„ ê³ ë ¤.\n"
        "3. ê·¸ë˜ë„ íŒë‹¨ì´ ì–´ë ¤ìš°ë©´ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'ë§Œ ì¶œë ¥.\n"
        "4. 653 í‚¤ì›Œë“œëŠ” ë³´ì¡° ì‹ í˜¸ì´ë©°, ë³¸ë¬¸ ë‚´ìš©ê³¼ ì¶©ëŒí•˜ë©´ ë¬´ì‹œ.\n"
    )

    hint_str = ", ".join(keywords_hint or [])

    # -----------------------------
    # 3) ì‚¬ìš©ì ë©”ì‹œì§€ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    user_prompt = (
        "ë‹¤ìŒ ë„ì„œ ì •ë³´(JSON)ë¥¼ ë°”íƒ•ìœ¼ë¡œ KDC 3ìë¦¬ ì •ìˆ˜ë§Œ ì¶œë ¥í•˜ë¼.\n"
        f"653 í‚¤ì›Œë“œ íŒíŠ¸: {hint_str or '(ì—†ìŒ)'}\n\n"
        f"{json.dumps(payload, ensure_ascii=False, indent=2)}\n\n"
        "ì¶œë ¥ ì˜ˆì‹œ: 823 / 813 / 325 / 181 / ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"
    )

    # -----------------------------
    # ì‘ë‹µ íŒŒì‹±ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    def _parse_response(s: str) -> Optional[str]:
        if not s:
            return None
        s = s.strip()

        if "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ" in s:
            return "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"

        m = re.search(r"(?<!\d)(\d{1,3})(?!\d)", s)
        if not m:
            return None

        num = m.group(1).zfill(3)
        if not re.fullmatch(r"\d{3}", num):
            return None
        return num

    # -----------------------------
    # LLM í˜¸ì¶œê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    def _call_llm(sys_p, user_p, max_tokens):
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": sys_p},
                    {"role": "user", "content": user_p},
                ],
                "temperature": 0.0,
                "max_tokens": max_tokens,
            },
            timeout=45,
        )
        resp.raise_for_status()
        txt = resp.json()["choices"][0]["message"]["content"].strip()

        code = _parse_response(txt)
        if not code:
            return None

        # ì–¸ì–´ ê¸°ë°˜ ë¬¸í•™ê³„ ì¬ì •ë ¬
        marc041 = getattr(book, "marc041", "") or getattr(book, "field_041", "") or ""
        return _rebase_8xx_with_language(code, marc041)

    # -----------------------------
    # 1ì°¨ LLM í˜¸ì¶œ
    # -----------------------------
    try:
        code = _call_llm(sys_prompt, user_prompt, max_tokens=16)
        if code:
            return code
    except Exception as e:
        st.warning(f"1ì°¨ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    # -----------------------------
    # 2ì°¨ í´ë°± í˜¸ì¶œ
    # -----------------------------
    fb_sys = (
        "ë„ˆëŠ” KDC ì‚¬ì„œì´ë‹¤. "
        "ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ **3ìë¦¬ ì •ìˆ˜** ë˜ëŠ” 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'ë§Œ ì¶œë ¥í•˜ë¼."
    )
    fb_user = f"{json.dumps(payload, ensure_ascii=False)}"
    try:
        code = _call_llm(fb_sys, fb_user, max_tokens=8)
        if code:
            return code
    except Exception as e:
        st.error(f"2ì°¨ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    # -----------------------------
    # 3ì°¨ ë¡œì»¬ í´ë°±
    # -----------------------------
    return "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"

# Chunk C-2-D : ISBN ì…ë ¥ â†’ 056 ìƒì„± ì „ì²´ íŒŒì´í”„ë¼ì¸
def get_kdc_from_isbn(
    isbn13: str,
    ttbkey: Optional[str],
    openai_key: str,
    model: str,
    keywords_hint: list[str] | None = None,
) -> Optional[str]:

    # 1ì°¨: ì•Œë¼ë”˜ API
    info = aladin_lookup_by_api(isbn13, ttbkey) if ttbkey else None

    # 2ì°¨: ì›¹ ìŠ¤í¬ë ˆì´í•‘
    if not info:
        info = aladin_lookup_by_web(isbn13)

    if not info:
        st.warning("ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

    # LLM í˜¸ì¶œ
    code = ask_llm_for_kdc(
        info,
        api_key=openai_key,
        model=model,
        keywords_hint=keywords_hint
    )

    # ìµœì¢… ê²€ì¦
    if code and not re.fullmatch(r"\d{1,3}", code) and code != "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ":
        return None

    return code

# C3. 041/546 ì–¸ì–´ ê°ì§€ ë° í•„ë“œ ë¹Œë”
# Chunk C-3-A : ì›ë³¸ ì–¸ì–´ ê°ì§€ê¸° (ì´ˆê°„ë‹¨ rule-based)
# ==========================================================
# ğŸ”¤ ì›ë³¸ ì–¸ì–´ ê°ì§€ê¸° (ë‹¨ìˆœ ë¬¸ì ê¸°ë°˜, ì›ë³¸ ë¡œì§ ìœ ì§€)
# ==========================================================

LANG_MAP = {
    "kor": "í•œêµ­ì–´",
    "eng": "ì˜ì–´",
    "jpn": "ì¼ë³¸ì–´",
    "chi": "ì¤‘êµ­ì–´",
    "rus": "ëŸ¬ì‹œì•„ì–´",
    "ara": "ì•„ëì–´",
    "fre": "í”„ë‘ìŠ¤ì–´",
    "ger": "ë…ì¼ì–´",
    "ita": "ì´íƒˆë¦¬ì•„ì–´",
    "spa": "ìŠ¤í˜ì¸ì–´",
    "und": "ì•Œ ìˆ˜ ì—†ìŒ",
}

def detect_language_simple(text: str) -> str:
    """
    ì›ë³¸ ì½”ë“œì˜ rule-based ì–¸ì–´ ê°ì§€ ê¸°ëŠ¥ ê·¸ëŒ€ë¡œ.
    """
    if not text:
        return "und"

    s = re.sub(r'[\s\W_]+', '', text)
    if not s:
        return "und"

    ch = s[0]

    if '\uac00' <= ch <= '\ud7a3':
        return "kor"
    elif '\u3040' <= ch <= '\u30ff':       # ì¼ë³¸ ê°€ë‚˜
        return "jpn"
    elif '\u4e00' <= ch <= '\u9fff':       # ì¤‘êµ­ í•œì
        return "chi"
    elif '\u0400' <= ch <= '\u04FF':       # ëŸ¬ì‹œì•„/í‚¤ë¦´
        return "rus"
    elif 'a' <= ch.lower() <= 'z':
        return "eng"

    return "und"

# Chunk C-3-B : FastText ê¸°ë°˜ ê³ ë„í™”(Lang ID) ì‚½ì… ì§€ì 
# ==========================================================
# fastText ê¸°ë°˜ ê³ ê¸‰ ì–¸ì–´ê°ì§€ (ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìš°ì„  ì ìš©)
# ==========================================================

try:
    import fasttext
    _FT_MODEL = fasttext.load_model("./lid.176.bin")  # í•„ìš” ì‹œ ê²½ë¡œ ë³€ê²½
except Exception:
    _FT_MODEL = None


def detect_language(text: str) -> str:
    """
    FastText â†’ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ì˜ rule-based
    """
    if _FT_MODEL:
        try:
            pred = _FT_MODEL.predict(text.replace("\n", " ")[:2000])
            label = pred[0][0].replace("__label__", "")
            # fastTextëŠ” eng, kor, jpn ë“±ì˜ ì•½ì–´ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return label.lower()
        except Exception:
            pass

    # fallback: ì›ë³¸ ê·œì¹™
    return detect_language_simple(text)

# Chunk C-3-C : 041 ìƒì„±ê¸° (ì›ë³¸ ê·œì¹™ + ì‚¬ìš©ìì˜ ìš”êµ¬ í¬í•¨)
# ==========================================================
# 041 ìƒì„±ê¸°
# ==========================================================

def build_041_kormarc(text_content: str,
                      original_title: str = "",
                      use_fasttext=True) -> str:
    """
    text_content: ì±… ì„¤ëª…Â·ëª©ì°¨Â·ì œëª© ë“± ë³¸ë¬¸ ì–¸ì–´ ê°ì§€
    original_title: ì›ì œ ê°ì§€(ë²ˆì—­ì„œì¼ ê²½ìš°)
    """
    lang_main = detect_language(text_content)
    lang_orig = detect_language(original_title) if original_title else None

    # ë³¸ë¬¸ ì–¸ì–´ê°€ ì—†ë‹¤ë©´ und â†’ korë¡œ ê¸°ë³¸ê°’ ì„¤ì •(ì›ë³¸ ë¡œì§)
    if lang_main == "und":
        lang_main = "kor"

    parts = [f"$a{lang_main}"]
    if original_title and lang_orig:
        if lang_orig != lang_main:
            parts.append(f"$h{lang_orig}")

    return "=041  \\\\" + "".join(parts)

# Chunk C-3-D : 546 í…ìŠ¤íŠ¸ ìƒì„±ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================
# 546 ìƒì„±ê¸° (ì›ë³¸ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€)
# ==========================================================

def build_546_from_041(marc041: str) -> str:
    if not marc041:
        return "=546  \\\\$aì–¸ì–´ ì •ë³´ ì—†ìŒ"

    a_codes = re.findall(r"\$a([a-z]{3})", marc041, re.I)
    h_match = re.search(r"\$h([a-z]{3})", marc041, re.I)
    h_code = h_match.group(1) if h_match else None

    if len(a_codes) == 1:
        a = LANG_MAP.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h = LANG_MAP.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"=546  \\\\$a{h} ì›ì‘ì„ {a}ë¡œ ë²ˆì—­"
        return f"=546  \\\\$a{a}ë¡œ ì”€"

    if len(a_codes) >= 2:
        langs = [LANG_MAP.get(c, "ì•Œ ìˆ˜ ì—†ìŒ") for c in a_codes]
        return f"=546  \\\\$a{'ã€'.join(langs)} ë³‘ê¸°"

    return "=546  \\\\$aì–¸ì–´ ì •ë³´ ì—†ìŒ"

# Chunk C-3-E : 041/546 ì „ì²´ íŒŒì´í”„ë¼ì¸ (ì›ë³¸ generate_all_oneclick íë¦„ ìœ ì§€)
# ==========================================================
# ISBN ê¸°ë°˜ â†’ (041, 546) ìƒì„± ì „ì²´ íŒŒì´í”„ë¼ì¸
# (ë„ˆê°€ ì¤€ ì›ë³¸ generate_all_oneclickì˜ íë¦„ 100% ë™ì¼)
# ==========================================================

def build_041_546_pipeline(item: dict, original_title_from_web: str = ""):
    """
    item: ì•Œë¼ë”˜ API item dict
    original_title_from_web: ì•Œë¼ë”˜ ìƒì„¸ HTML íŒŒì‹±ì—ì„œ ì°¾ì•„ë‚¸ ì›ì œ
    """
    title = item.get("title", "") or ""
    desc  = item.get("description", "") or ""
    toc   = (item.get("subInfo") or {}).get("toc", "") or ""

    content_blob = " ".join([title, desc, toc])

    tag041 = build_041_kormarc(
        text_content=content_blob,
        original_title=original_title_from_web
    )
    tag546 = build_546_from_041(tag041)

    return tag041, tag546

# C4. 245 / 246 / 700 / 90010 / 940 (ì œëª©Â·ì €ìÂ·ì—­ìÂ·LOD)
# Chunk C-4-A : ì—­í• ì–´ ì •ë¦¬
# ==========================================================
# ì—­í• ì–´ ì œê±° ë° ì›ì‹œ ì €ì ë¬¸ìì—´ ì •ë¦¬
# ==========================================================

ROLE_PATTERNS = [
    r"\bì €ì\b", r"\bì§€ì€ì´\b", r"\bì§€ìŒ\b", r"\bê¸€\b", r"\bê¸€Â·ê·¸ë¦¼\b",
    r"\bê·¸ë¦¼\b", r"\bì˜®ê¹€\b", r"\bì˜®ê¸´ì´\b", r"\bí¸\b", r"\bì—®ìŒ\b",
    r"\bì—­\b", r"\btranslator\b", r"\bí¸ì§‘\b",
]

def clean_author_role(raw: str) -> str:
    if not raw:
        return ""
    s = raw
    for pat in ROLE_PATTERNS:
        s = re.sub(pat, "", s, flags=re.IGNORECASE)
    s = re.sub(r"[\/\|]", ";", s)     # / â†’ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„í•  ë™ì¼í™”
    s = re.sub(r"\s+", " ", s).strip()
    return s

# Chunk C-4-B : ì €ìëª… ë¶„ë¦¬
# ==========================================================
# ì €ìëª… ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
# ==========================================================

def split_authors(raw: str) -> list[str]:
    if not raw:
        return []

    s = clean_author_role(raw)

    parts = []
    for chunk in re.split(r";", s):
        chunk = chunk.strip()
        if not chunk:
            continue
        # ì½¤ë§ˆ ê¸°ë°˜ ë¶„í• ì€ ì´ë¦„ êµ¬ì¡°ë¥¼ í•´ì¹  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œ ì ìš©
        sub = [c.strip() for c in chunk.split(",") if c.strip()]
        if len(sub) == 1:
            parts.append(sub[0])
        else:
            parts.extend(sub)
    return parts

# Chunk C-4-C : ë™ì•„ì‹œì•„/ì„œì–‘ ì´ë¦„ êµ¬ë³„ â†’ ì •ë ¬í˜• ìƒì„±
# ==========================================================
# ì´ë¦„ ì •ë ¬í˜• ìƒì„±
# ==========================================================

def is_east_asian(name: str) -> bool:
    if not name:
        return False
    # í•œê¸€ / í•œì / ì¼ë³¸ ê°€ë‚˜ í¬í•¨ ì‹œ True
    if any('\uac00' <= ch <= '\ud7a3' for ch in name):
        return True
    if any('\u4e00' <= ch <= '\u9fff' for ch in name):
        return True
    if any('\u3040' <= ch <= '\u30ff' for ch in name):
        return True
    return False

def to_sort_form(name: str) -> str:
    """
    ë™ì•„ì‹œì•„ ì´ë¦„ì€ ê·¸ëŒ€ë¡œ.
    ì•ŒíŒŒë²³ ê¸°ë°˜ ì´ë¦„ì€ 'ì„±, ì´ë¦„'ìœ¼ë¡œ ë³€í™˜.
    """
    if not name:
        return ""

    if is_east_asian(name):
        return name.strip()

    parts = name.split()
    if len(parts) == 1:
        return name.strip()

    last = parts[-1]
    first = " ".join(parts[:-1])
    return f"{last}, {first}"

# Chunk C-4-D : ì €ì â†’ 100/700 í•„ë“œ ìƒì„± (ì—­í•  í¬í•¨)
# ==========================================================
# 100/700 ìƒì„±ê¸° (ì›ë³¸ ê·œì¹™ 100% ìœ ì§€)
# ==========================================================

def build_100_and_700(authors: list[str], origin_lang_code: str | None = None):
    """
    authors = ['í™ê¸¸ë™', 'John Smith', 'å±±ç”°å¤ªéƒ', ...]
    origin_lang_code: 041 $h â†’ ë²ˆì—­ì„œ ì—¬ë¶€ íŒë‹¨
    """
    if not authors:
        return None, []

    main_author = authors[0]
    rest = authors[1:]

    # 100 í•„ë“œ ìƒì„±
    sort_main = to_sort_form(main_author)
    tag_100 = f"=100  1\\\\$a{sort_main}"

    # ë²ˆì—­ì„œ ì—¬ë¶€
    is_translation = bool(origin_lang_code)

    tag_700_list = []
    for name in rest:
        sort_name = to_sort_form(name)
        if is_translation:
            tag = f"=700  1\\\\$a{sort_name}$eë²ˆì—­"
        else:
            tag = f"=700  1\\\\$a{sort_name}"
        tag_700_list.append(tag)

    return tag_100, tag_700_list

# Chunk C-4-E : ì•Œë¼ë”˜ item.author íŒŒì‹± â†’ 100/700 ì „ì²´ íŒŒì´í”„ë¼ì¸
# ==========================================================
# ì•Œë¼ë”˜ item.author â†’ 100/700 ì „ì²´ ìƒì„±
# ==========================================================

def build_people_fields_from_aladin(item: dict, origin_lang_code: str | None = None):
    raw = (item or {}).get("author", "") or ""
    authors = split_authors(raw)

    tag100, tag700_list = build_100_and_700(authors, origin_lang_code)
    return tag100, tag700_list

# C5. 300 ìƒì„¸ í˜•ì‹ íŒŒì„œ
# Chunk C-5-A : GPT í˜¸ì¶œ í•¨ìˆ˜ (ì›ë³¸ ìœ ì§€)
# ==========================================================
# GPT í˜¸ì¶œ í•¨ìˆ˜ (ì›ë³¸ í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •)
# ==========================================================

def generate_653_with_gpt(
    category: str,
    title: str,
    authors: str,
    description: str,
    toc: str,
    max_keywords: int = 7,
) -> str:
    """
    ì›ë³¸ ì½”ë“œì—ì„œ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆëŠ” í•¨ìˆ˜.
    ê²°ê³¼ ì˜ˆ: "$aì•„ë™ë¬¸í•™$aì •ì„œì¡°ì ˆ$aì‹œê°„ê´€ë¦¬"
    """
    raise NotImplementedError  # True Patchì—ì„œëŠ” ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©

# Chunk C-5-B : 653 íƒœê·¸ ìƒì„±ê¸° (ì›ë³¸ ë¡œì§ ê·¸ëŒ€ë¡œ ì¬êµ¬ì¶•)
# ==========================================================
# 653 íƒœê·¸ ìƒì„±ê¸° (ì›ë³¸ ì½”ë“œ 100% ë³´ì¡´)
# ==========================================================

def build_653_tag(item: dict) -> str | None:
    """
    item: ì•Œë¼ë”˜ item(dict)
    GPTê°€ ìƒì„±í•œ "$aí‚¤ì›Œë“œ$a..." í˜•íƒœë¥¼ ê·¸ëŒ€ë¡œ ë°›ì•„
    =653  \\$aí‚¤ì›Œë“œ$aí‚¤ì›Œë“œâ€¦ í˜•íƒœë¡œ ë˜í•‘í•˜ì—¬ ë°˜í™˜.
    """
    if not item:
        return None

    title = item.get("title", "") or ""
    category = item.get("categoryName", "") or ""
    raw_author = item.get("author", "") or ""
    desc = item.get("description", "") or ""
    toc = (item.get("subInfo") or {}).get("toc", "") or ""

    kwline = generate_653_with_gpt(
        category=category,
        title=title,
        authors=clean_author_role(raw_author),
        description=desc,
        toc=toc,
        max_keywords=7,
    )

    if not kwline:
        return None

    # ì›ë³¸ ë¡œì§: ê³µë°± ì œê±° í›„ ë˜í•‘
    kwline = kwline.replace(" ", "")
    return f"=653  \\\\{kwline}"

# Chunk C-5-C : 653 â†’ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ íŒŒì‹± (ì›ë³¸ ì½”ë“œ 100% ë³´ì¡´)
# ==========================================================
# 653 íŒŒì‹± â†’ ì…ë ¥ ìˆœì„œ ì•ˆì •ì„±ê³¼ ì¤‘ë³µ ì œê±° + ìµœëŒ€ 7ê°œ
# ==========================================================

def parse_653_keywords(tag_653: str | None) -> list[str]:
    """
    ì˜ˆ:
    '=653  \\$aì•„ë™ë¬¸í•™$aì •ì„œ$aì‹œê°„ê´€ë¦¬'
    â†’ ['ì•„ë™ë¬¸í•™','ì •ì„œ','ì‹œê°„ê´€ë¦¬']
    """
    if not tag_653:
        return []

    s = tag_653.strip()

    # ì ‘ë‘ë¶€ ì œê±° (=653  \\)
    s = re.sub(r"^=653\s+\\\\", "", s)

    kws = []
    for m in re.finditer(r"\$a([^$]+)", s):
        w = (m.group(1) or "").strip()
        if w:
            kws.append(w)

    # ì¤‘ë³µ ì œê±° + ìµœëŒ€ 7
    seen = set()
    out = []
    for w in kws:
        if w not in seen:
            seen.add(w)
            out.append(w)
        if len(out) >= 7:
            break

    return out

# Chunk C-5-D : LLM íŒíŠ¸ìš© 653 ì •ê·œí™”
# ==========================================================
# LLM(056 KDC) íŒíŠ¸ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì •ê·œí™” (ì›ë³¸ ë¡œì§ ìœ ì§€)
# ==========================================================

def normalize_653_keywords_for_hint(kws: list[str]) -> list[str]:
    seen = set()
    out = []
    for w in (kws or []):
        w = (w or "").strip()
        if w and w not in seen:
            seen.add(w)
            out.append(w)
    return sorted(out)[:7]

# Chunk C-5-E : 653 ì „ì²´ íŒŒì´í”„ë¼ì¸
# ==========================================================
# ì•Œë¼ë”˜ item â†’ 653 íƒœê·¸ + LLM íŒíŠ¸ ì „ì²´ íŒŒì´í”„ë¼ì¸
# ==========================================================

def build_653_pipeline(item: dict):
    tag_653 = build_653_tag(item)
    if not tag_653:
        return None, []

    kws_raw = parse_653_keywords(tag_653)
    kws_hint = normalize_653_keywords_for_hint(kws_raw)

    return tag_653, kws_hint

# C6. ê°€ê²©/020/950 ëª¨ë“ˆ
# Chunk C-6-A : ê°€ê²© ì¶”ì¶œ í—¬í¼ (ì›ë³¸ ë¡œì§ ê·¸ëŒ€ë¡œ ì¬í˜„)
# ==========================================================
# ê°€ê²© ì¶”ì¶œ í—¬í¼ - ì›ë³¸ ë¡œì§ 100% ë™ì¼
# ==========================================================

def extract_price_kr(item: dict, isbn: str) -> str:
    """
    ì›ë³¸: _extract_price_kr()
    1) ì•Œë¼ë”˜ priceStandard
    2) ì—†ìœ¼ë©´ ì•Œë¼ë”˜ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ê°€ê²©
    3) ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
    """
    raw = str((item or {}).get("priceStandard", "") or "").strip()

    # 2) priceStandard ì—†ìœ¼ë©´ í¬ë¡¤ë§ ë°±ì—…
    if not raw:
        try:
            crawl = crawl_aladin_original_and_price(isbn) or {}
            raw = crawl.get("price", "").strip()
        except Exception:
            raw = ""

    # 3) ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
    digits = re.sub(r"[^\d]", "", raw)
    return digits

# Chunk C-6-B : 020 í•„ë“œ ìƒì„±ê¸° (ì›ë³¸ ë¡œì§ 100% ë™ì¼)
# ==========================================================
# 020 ìƒì„±ê¸° - ì›ë³¸ _build_020_from_item_and_nlk ì™„ì „ ì¬í˜„
# ==========================================================

def build_020_field(isbn: str, item: dict) -> str:
    """
    ISBN + ë¶€ê°€ê¸°í˜¸ + ê°€ê²©ì„ í¬í•¨í•œ 020 ìƒì„±.
    ì›ë³¸ _build_020_from_item_and_nlk()ì˜ ë…¼ë¦¬ë¥¼ ê°ê° ë¶„ë¦¬í•´ì„œ ì¬í˜„.
    """
    # 1) ì•Œë¼ë”˜ ê°€ê²©
    price = str((item or {}).get("priceStandard", "") or "").strip()

    # 2) NLKì—ì„œ add_code, set_isbn, price ê°€ì ¸ì˜¤ê¸°
    try:
        nlk_extra = fetch_additional_code_from_nlk(isbn) or {}
        add_code = nlk_extra.get("add_code", "")
        price_from_nlk = nlk_extra.get("price", "")
    except Exception:
        add_code = ""
        price_from_nlk = ""

    # 3) ê°€ê²© ìš°ì„ ìˆœìœ„
    final_price = price or price_from_nlk

    # 4) ë¬¸ìì—´ ì¡°ë¦½
    parts = [f"=020  \\\\$a{isbn}"]
    
    if add_code:
        parts.append(f"$g{add_code}")

    if final_price:
        # ì›ë³¸ì²˜ëŸ¼ ':' ë’¤ì— $c ìˆ«ìë§Œ
        parts.append(f":$c{final_price}")

    return "".join(parts)

# Chunk C-6-C : SET ISBN (020 1) ìƒì„±ê¸°
# ==========================================================
# SET ISBN 020 ìƒì„±ê¸°
# ==========================================================

def build_020_set_field(set_isbn: str | None) -> str | None:
    if not set_isbn:
        return None
    return f"=020  1\\$a{set_isbn} (set)"

# Chunk C-6-D : 950 ìƒì„±ê¸° (ì›ë³¸ê³¼ ì™„ì „íˆ ë™ì¼)
# ==========================================================
# 950 ìƒì„±ê¸° - ì›ë³¸ build_950_from_item_and_price
# ==========================================================

def build_950_field(item: dict, isbn: str) -> str | None:
    """
    ê°€ê²©ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ì§€ ì•ŠìŒ.
    """
    price = extract_price_kr(item, isbn)
    if not price:
        return None

    # ì›ë³¸ì²˜ëŸ¼ ì—­ìŠ¬ë˜ì‹œ ìœ ì§€
    return f"=950  0\\$b\\{price}"

# Chunk C-6-E : 020 + 950 íŒŒì´í”„ë¼ì¸
# ==========================================================
# 020 + 950 ì „ì²´ íŒŒì´í”„ë¼ì¸
# ==========================================================

def build_price_related_fields(isbn: str, item: dict):
    """
    ê²°ê³¼:
      - tag_020
      - tag_020_set (optional)
      - tag_950 (optional)
      - set_isbn (for metadata)
    """
    tag_020 = build_020_field(isbn, item)

    # SET ISBN (ë¶€ê°€ê¸°í˜¸ API)
    nlk_info = fetch_additional_code_from_nlk(isbn)
    set_isbn = (nlk_info or {}).get("set_isbn", "").strip()
    tag_020_set = build_020_set_field(set_isbn)

    # 950 (ê°€ê²©ë§Œ)
    tag_950 = build_950_field(item, isbn)

    return tag_020, tag_020_set, tag_950, set_isbn

# C7. KPIPA/MCST/ë°œí–‰ì§€ + 260 ëª¨ë“ˆ
# Chunk C-7-A : ì§€ì—­ëª… ì •ê·œí™” â†’ ë°œí–‰êµ­ ì½”ë“œ ì°¾ê¸°
# ==========================================================
# ì§€ì—­ëª… ì •ê·œí™” + ë°œí–‰êµ­(ë‚˜ë¼ì½”ë“œ) ì°¾ê¸°
# ì›ë³¸ get_country_code_by_region() 100% ë³µì› + êµ¬ì¡°í™”
# ==========================================================

def normalize_region_for_country_code(region: str) -> str:
    """
    ì›ë³¸: get_country_code_by_region ë‚´ë¶€ normalize ì „ëµ ë¶„ë¦¬
    ì „ë¼ë‚¨ë„ â†’ ì „ë‚¨ / ê²½ìƒë¶ë„ â†’ ê²½ë¶ / ì„œìš¸íŠ¹ë³„ì‹œ â†’ ì„œìš¸
    """
    if not region:
        return ""

    region = region.strip()

    # ì „ë¼/ì¶©ì²­/ê²½ìƒ ê³„ì—´ ì²˜ë¦¬
    if region.startswith(("ì „ë¼", "ì¶©ì²­", "ê²½ìƒ")):
        if len(region) >= 3:
            return region[0] + region[2]   # ì „ë¼ë‚¨ë„ â†’ ì „ë‚¨
        return region[:2]

    # ì„œìš¸íŠ¹ë³„ì‹œ â†’ ì„œìš¸
    return region[:2]


def get_country_code_by_region(region_name: str, region_df) -> str:
    """
    ì§€ì—­ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ 008 ë°œí–‰êµ­ ì½”ë“œ(3ìë¦¬)ë¥¼ ë°˜í™˜.
    region_df: Google Sheet "ë°œí–‰êµ­ëª…â€“ë°œí–‰êµ­ë¶€í˜¸" ì‹œíŠ¸.
    """
    try:
        target = normalize_region_for_country_code(region_name)

        for _, row in region_df.iterrows():
            sheet_region = normalize_region_for_country_code(row["ë°œí–‰êµ­"])
            if sheet_region == target:
                return (row["ë°œí–‰êµ­ ë¶€í˜¸"] or "   ").strip()

        return "   "  # fallback: ê³µë°± 3ì¹¸
    except Exception:
        return "   "

# Chunk C-7-B : KPIPA ISBN ê²€ìƒ‰ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================
# KPIPA ISBN ê²€ìƒ‰: ì¶œíŒì‚¬ëª… / ì„í”„ë¦°íŠ¸ ì¶”ì¶œ
# ì›ë³¸ get_publisher_name_from_isbn_kpipa ì™„ì „ ë³µì›
# ==========================================================

def fetch_kpipa_publisher_info(isbn: str):
    url = "https://bnk.kpipa.or.kr/home/v3/addition/search"
    params = {
        "ST": isbn,
        "PG": 1,
        "PG2": 1,
        "DSF": "Y",
        "SO": "weight",
        "DT": "A",
    }
    headers = {"User-Agent": "Mozilla/5.0"}

    def norm(name):
        return re.sub(
            r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬|í”„ë ˆìŠ¤",
            "",
            (name or ""),
            flags=re.IGNORECASE
        ).lower()

    try:
        res = requests.get(url, params=params, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # ê²€ìƒ‰ ê²°ê³¼ 1ê±´
        first = soup.select_one("a.book-grid-item")
        if not first:
            return None, None, "âŒ KPIPA ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        detail = "https://bnk.kpipa.or.kr" + first["href"]
        dres = requests.get(detail, headers=headers, timeout=15)
        dres.raise_for_status()
        dsoup = BeautifulSoup(dres.text, "html.parser")

        tag = dsoup.find("dt", string="ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸")
        if not tag:
            return None, None, "âŒ KPIPA 'ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸' í•­ëª© ì—†ìŒ"

        dd = tag.find_next_sibling("dd")
        if not dd:
            return None, None, "âŒ KPIPA dd íƒœê·¸ ì—†ìŒ"

        full = dd.get_text(strip=True)
        main = full.split("/")[0].strip()

        return full, norm(main), None  # (ì „ì²´í…ìŠ¤íŠ¸, ëŒ€í‘œì¶œíŒì‚¬ëª…(ì •ê·œí™”), ì˜¤ë¥˜)
    except Exception as e:
        return None, None, f"âŒ KPIPA ì˜ˆì™¸: {e}"

# Chunk C-7-C : Google Sheet ê¸°ë°˜ ì¶œíŒì§€ ê²€ìƒ‰
# ==========================================================
# KPIPA ì¶œíŒì‚¬ DB ë§¤ì¹­ (ì›ë³¸ search_publisher_location_with_alias)
# ==========================================================

def locate_publisher_in_kpipa_db(name: str, publisher_df):
    """
    publisher_df: ['ì¶œíŒì‚¬ëª…', 'ì£¼ì†Œ']
    """
    debug = []
    if not name:
        return "ì¶œíŒì§€ ë¯¸ìƒ", ["âŒ ì…ë ¥ëœ ì¶œíŒì‚¬ëª…ì´ ì—†ìŒ"]

    norm_name = normalize_publisher_name(name)
    candidates = publisher_df[publisher_df["ì¶œíŒì‚¬ëª…"].apply(
        lambda x: normalize_publisher_name(x) == norm_name
    )]

    if not candidates.empty:
        addr = candidates.iloc[0]["ì£¼ì†Œ"]
        debug.append(f"âœ“ KPIPA DB ë§¤ì¹­ ì„±ê³µ: {name} â†’ {addr}")
        return addr, debug
    else:
        debug.append(f"âŒ KPIPA DB ë§¤ì¹­ ì‹¤íŒ¨: {name}")
        return "ì¶œíŒì§€ ë¯¸ìƒ", debug

# Chunk C-7-D : ì„í”„ë¦°íŠ¸ fallback ê²€ìƒ‰
# ==========================================================
# IMPRINT fallback (ì›ë³¸ find_main_publisher_from_imprints)
# ==========================================================

def find_imprint_parent_publisher(rep_name, imprint_df, publisher_df):
    norm_rep = normalize_publisher_name(rep_name)
    debug = []

    for full in imprint_df["ì„í”„ë¦°íŠ¸"]:
        if "/" in full:
            main, imp = [x.strip() for x in full.split("/", 1)]
        else:
            main, imp = full.strip(), None

        if imp and normalize_publisher_name(imp) == norm_rep:
            addr, dbg2 = locate_publisher_in_kpipa_db(main, publisher_df)
            debug.extend(dbg2)
            if addr and addr not in ("ì¶œíŒì§€ ë¯¸ìƒ", None):
                return addr, debug

    debug.append(f"âŒ IMPRINT ë§¤ì¹­ ì‹¤íŒ¨: {rep_name}")
    return None, debug

# Chunk C-7-E : ë¬¸ì²´ë¶€(MCST) fallback ê²€ìƒ‰
# ==========================================================
# ë¬¸ì²´ë¶€ ë„ì„œë“±ë¡ë¶€ ê²€ìƒ‰ (ì›ë³¸ get_mcst_address ì™„ì „ ë³µì›)
# ==========================================================

def search_mcst_publisher_address(name: str):
    url = "https://book.mcst.go.kr/html/searchList.php"
    params = {
        "search_area": "ì „ì²´",
        "search_state": "1",
        "search_kind": "1",
        "search_type": "1",
        "search_word": name,
    }
    debug = []

    try:
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        rows = []
        for tr in soup.select("table.board tbody tr"):
            tds = tr.find_all("td")
            if len(tds) >= 4:
                reg_type = tds[0].get_text(strip=True)
                nm = tds[1].get_text(strip=True)
                addr = tds[2].get_text(strip=True)
                status = tds[3].get_text(strip=True)
                if status == "ì˜ì—…":
                    rows.append((nm, addr))

        if rows:
            debug.append("âœ“ ë¬¸ì²´ë¶€ ê²€ìƒ‰ ì„±ê³µ")
            return rows[0][1], rows, debug

        debug.append("âŒ ë¬¸ì²´ë¶€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return "ë¯¸í™•ì¸", [], debug

    except Exception as e:
        debug.append(f"âŒ ë¬¸ì²´ë¶€ ì˜ˆì™¸: {e}")
        return "ì˜¤ë¥˜ ë°œìƒ", [], debug

# Chunk C-7-F : ìµœì¢… ë°œí–‰ì§€ ê²°ì • íŒŒì´í”„ë¼ì¸ (ì›ë³¸ build_pub_location_bundle ì™„ì „ ë³µì›)
# ==========================================================
# ìµœì¢… ë°œí–‰ì§€ ê²°ì • íŒŒì´í”„ë¼ì¸ - ì›ë³¸ 100% ì¬í˜„
# ==========================================================

def resolve_publisher_location(isbn, publisher_raw, publisher_df, region_df, imprint_df):
    debug = []
    debug.append("âœ“ Google Sheet DB ë¡œë“œ ì™„ë£Œ")

    # 1) KPIPA ISBN ê²€ìƒ‰
    kpipa_full, kpipa_norm, err = fetch_kpipa_publisher_info(isbn)
    if err:
        debug.append(f"KPIPA ê²€ìƒ‰: {err}")

    # ëŒ€í‘œ ì¶œíŒì‚¬ëª… ì¶”ì •
    rep_name, aliases = split_publisher_aliases(
        kpipa_full or publisher_raw or ""
    )
    resolved_name = rep_name or publisher_raw or ""
    debug.append(f"ëŒ€í‘œ ì¶œíŒì‚¬ëª… ì¶”ì •: {resolved_name} / ë³„ì¹­: {aliases}")

    # 2) KPIPA DB ë§¤ì¹­
    place_raw, dbg2 = locate_publisher_in_kpipa_db(resolved_name, publisher_df)
    debug.extend(dbg2)
    source = "KPIPA_DB"

    # 3) imprint fallback
    if place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", None, "ì˜ˆì™¸ ë°œìƒ"):
        imp_addr, dbg3 = find_imprint_parent_publisher(resolved_name, imprint_df, publisher_df)
        debug.extend(dbg3)
        if imp_addr:
            place_raw = imp_addr
            source = "IMPRINTâ†’KPIPA"

    # 4) ë¬¸ì²´ë¶€ fallback
    if not place_raw or place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ë¯¸í™•ì¸", "ì˜ˆì™¸ ë°œìƒ"):
        mcst_addr, _, dbg4 = search_mcst_publisher_address(resolved_name)
        debug.extend(dbg4)
        if mcst_addr not in ("ë¯¸í™•ì¸", "ì˜¤ë¥˜ ë°œìƒ"):
            place_raw = mcst_addr
            source = "MCST"

    # 5) ìµœì¢… ì‹¤íŒ¨ â†’ ì¶œíŒì§€ ë¯¸ìƒ
    if not place_raw or place_raw in ("ë¯¸í™•ì¸", "ì˜¤ë¥˜ ë°œìƒ", None):
        place_raw = "ì¶œíŒì§€ ë¯¸ìƒ"
        source = "FALLBACK"
        debug.append("âš  ëª¨ë“  ê²½ë¡œ ì‹¤íŒ¨ â†’ 'ì¶œíŒì§€ ë¯¸ìƒ'")

    # í™”ë©´ í‘œì‹œìš©
    place_display = normalize_publisher_location_for_display(place_raw)

    # 008ìš© country code
    country_code = get_country_code_by_region(place_raw, region_df)

    return {
        "place_raw": place_raw,
        "place_display": place_display,
        "country_code": country_code,
        "resolved_publisher": resolved_name,
        "source": source,
        "debug": debug,
    }

# Chunk C-7-G : 260 í•„ë“œ ìƒì„±ê¸°
# ==========================================================
# 260 ìƒì„±ê¸° - ì›ë³¸ build_260 ì™„ì „ ë³µì›
# ==========================================================

def build_260_field(place_display: str, publisher: str, pubyear: str) -> str:
    place = place_display or "ë°œí–‰ì§€ ë¯¸ìƒ"
    pub = publisher or "ë°œí–‰ì²˜ ë¯¸ìƒ"
    year = pubyear or "ë°œí–‰ë…„ ë¯¸ìƒ"

    return f"=260  \\\\$a{place} :$b{pub},$c{year}"

# C8. 008 ìƒì„±ê¸° (lang3 override / country3 override í¬í•¨)
# C-8-A : 008 ë¬¸ìì—´ ìƒì„±ê¸° (ì›ë³¸ build_008_kormarc_bk ì™„ì „ ì¬í˜„)
# ==========================================================
# 008 ë³¸ë¬¸ 40ë°”ì´íŠ¸ ìƒì„±ê¸° â€” ì›ë³¸ build_008_kormarc_bk() ì™„ì „ ë™ì¼
# ==========================================================

def build_008_body_bk(
    date_entered,      # YYMMDD
    date1,             # 4ìë¦¬ ì—°ë„ ë˜ëŠ” 19uu
    country3,          # ë°œí–‰êµ­ ë¶€í˜¸ 3ìë¦¬
    lang3,             # ì–¸ì–´ì½”ë“œ 3ìë¦¬
    *,
    date2="",          # 11-14
    illus4="",         # ì‚½í™”/ë„í‘œ/ì‚¬ì§„ í‚¤(ìµœëŒ€ 4ë¬¸ì)
    has_index="0",     # ìƒ‰ì¸ìœ ë¬´
    lit_form=" ",      # ë¬¸í•™í˜•ì‹ (p ì‹œ / f ì†Œì„¤ / e ìˆ˜í•„ / m ê¸°í–‰ / i ì„œê°„ë¬¸)
    bio=" ",           # ìì„œì „(a), ì „ê¸°(b), ì „ê¸°ì  ìš”ì†Œ(d)
    type_of_date="s",  # 06
    modified_record=" ",
    cataloging_src="a",
):
    """
    ì •í™•íˆ 40 bytesë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤.
    """

    def pad(s, n, fill=" "):
        s = "" if s is None else str(s)
        return (s[:n] + fill * n)[:n]

    # --- ì…ë ¥ ê²€ì¦ (ì›ë³¸ ë™ì¼) ---
    if len(date_entered) != 6 or not date_entered.isdigit():
        raise ValueError("date_enteredëŠ” YYMMDD 6ìë¦¬ ìˆ«ì")
    if len(date1) != 4:
        raise ValueError("date1ì€ 4ìë¦¬ (ì˜ˆ: 2025, 19uu)")

    body = "".join([
        date_entered,             # 00-05
        pad(type_of_date, 1),     # 06
        date1,                    # 07-10
        pad(date2, 4),            # 11-14
        pad(country3, 3),         # 15-17
        pad(illus4, 4),           # 18-21
        " " * 4,                  # 22-25: ì´ìš©ëŒ€ìƒ/ìë£Œí˜•íƒœ/ë‚´ìš©í˜•ì‹
        " " * 2,                  # 26-27
        pad(modified_record, 1),  # 28
        "0",                      # 29 íšŒì˜ê°„í–‰ë¬¼
        "0",                      # 30 ê¸°ë…ë…¼ë¬¸ì§‘
        has_index if has_index in ("0","1") else "0",  # 31 ìƒ‰ì¸ìœ ë¬´
        pad(cataloging_src, 1),   # 32 ëª©ë¡ ì „ê±°
        pad(lit_form, 1),         # 33 ë¬¸í•™í˜•ì‹
        pad(bio, 1),              # 34 ì „ê¸°/ìì„œì „
        pad(lang3, 3),            # 35-37 ì–¸ì–´
        " " * 2                   # 38-39 ê³µë°±
    ])

    if len(body) != 40:
        raise AssertionError(f"008 length != 40: {len(body)}")

    return body

# C-8-B : ì—°ë„ ì¶”ì¶œê¸° (ì›ë³¸ extract_year_from_aladin_pubdate)
# ==========================================================
# ë°œí–‰ì—°ë„ ì¶”ì¶œê¸° â€” ì›ë³¸ extract_year_from_aladin_pubdate ì™„ì „ ë³µì›
# ==========================================================

def extract_year_from_pubdate(pubdate: str) -> str:
    m = re.search(r"(19|20)\d{2}", pubdate or "")
    return m.group(0) if m else "19uu"

# C-8-C : ì‚½í™”/ë„í‘œ/ì‚¬ì§„ ê°ì§€ê¸° (008ìš© illus4)
# ==========================================================
# ì‚½í™” ê°ì§€ â€” ì›ë³¸ detect_illus4 ì™„ì „ ì¬í˜„
# ==========================================================

def detect_illus4(text: str) -> str:
    if not text:
        return ""
    keys = []

    if re.search(r"ì‚½í™”|ì‚½ë„|ë„í•´|ì¼ëŸ¬ìŠ¤íŠ¸|illustration|ê·¸ë¦¼", text, re.I):
        keys.append("a")
    if re.search(r"ë„í‘œ|í‘œ|ì°¨íŠ¸|ê·¸ë˜í”„|chart|graph", text, re.I):
        keys.append("d")
    if re.search(r"ì‚¬ì§„|í¬í† |í™”ë³´|photo|photograph|ì»¬ëŸ¬ì‚¬ì§„|ì¹¼ë¼ì‚¬ì§„", text, re.I):
        keys.append("o")

    # ì¤‘ë³µ ìˆœì„œ ìœ ì§€ + ìµœëŒ€ 4ë¬¸ì
    out = []
    for k in keys:
        if k not in out:
            out.append(k)

    return "".join(out)[:4]

# C-8-D : ìƒ‰ì¸ ê°ì§€ê¸°
# ==========================================================
# ìƒ‰ì¸ ê°ì§€ â€” ì›ë³¸ detect_index
# ==========================================================

def detect_index_flag(text: str) -> str:
    return "1" if re.search(r"ìƒ‰ì¸|ì°¾ì•„ë³´ê¸°|ì¸ëª…ìƒ‰ì¸|ì‚¬í•­ìƒ‰ì¸|index", text, re.I) else "0"

# C-8-E : ë¬¸í•™í˜•ì‹ ê°ì§€ê¸° (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ==========================================================
# ë¬¸í•™í˜•ì‹ ê°ì§€ê¸° â€” ì›ë³¸ detect_lit_form ì™„ì „ ë³µì›
# ==========================================================

def detect_lit_form(title: str, category: str, extra: str = "") -> str:
    blob = f"{title} {category} {extra}"

    if re.search(r"ì„œê°„ì§‘|í¸ì§€|ì„œê°„ë¬¸|letters?", blob, re.I):
        return "i"  # ì„œê°„ë¬¸í•™
    if re.search(r"ê¸°í–‰|ì—¬í–‰ê¸°|ì—¬í–‰ ì—ì„¸ì´|ì¼ê¸°|ìˆ˜ê¸°|diary|travel", blob, re.I):
        return "m"  # ê¸°í–‰/ì¼ê¸°/ìˆ˜ê¸°
    if re.search(r"ì‹œì§‘|ì‚°ë¬¸ì‹œ|poem|poetry", blob, re.I):
        return "p"  # ì‹œ
    if re.search(r"ì†Œì„¤|ì¥í¸|ì¤‘ë‹¨í¸|novel|fiction", blob, re.I):
        return "f"  # ì†Œì„¤
    if re.search(r"ì—ì„¸ì´|ìˆ˜í•„|essay", blob, re.I):
        return "e"  # ìˆ˜í•„

    return " "

# C-8-F : ì „ê¸° / ìì„œì „ ê°ì§€ê¸°
# ==========================================================
# ì „ê¸°/ìì„œì „ ê°ì§€ê¸° â€” ì›ë³¸ detect_bio
# ==========================================================

def detect_bio_marker(text: str) -> str:
    if re.search(r"ìì„œì „|íšŒê³ ë¡|autobiograph", text, re.I):
        return "a"
    if re.search(r"ì „ê¸°|í‰ì „|ì¸ë¬¼ í‰ì „|biograph", text, re.I):
        return "b"
    if re.search(r"ì „ê¸°ì |ìì „ì |íšŒê³ |íšŒìƒ", text):
        return "d"
    return " "

# C-8-G : ë°œí–‰êµ­(country3) ìš°ì„ ìˆœìœ„ ê²°ì • ë¡œì§ (ì›ë³¸ build_008_from_isbn ì¼ë¶€)
# ==========================================================
# ë°œí–‰êµ­ ì½”ë“œ ìš°ì„ ìˆœìœ„ ê²°ì •
# ì›ë³¸ build_008_from_isbn ì˜ country3 ê²°ì •ê³¼ ë™ì¼
# ==========================================================

def select_country3(source_place: str, override_country3: str | None, region_default: str) -> str:
    """
    override > (ë°œí–‰ì§€ ê¸°ë°˜ ì¶”ì •) > default(KR ì½”ë“œ)
    """
    # 1) override ìµœìš°ì„ 
    if override_country3:
        return override_country3

    # 2) ë°œí–‰ì§€ ë¬¸ìì—´ ê¸°ë°˜ ë§¤í•‘
    if source_place:
        # "ë°œí–‰ì§€ ë¯¸ìƒ" ì²˜ë¦¬
        s = source_place.strip()
        if "ë¯¸ìƒ" in s or "unknown" in s.lower():
            return "   "  # ê³µë°± 3ì¹¸

        # ì›ë³¸ guess_country3_from_place ëŠ” ì´ë¯¸ ë‹¤ë¥¸ ë¸”ë¡ì—ì„œ ì¬í˜„ë¨
        guessed = guess_country3_from_place(s)
        if guessed:
            return guessed
        return region_default

    # 3) ì•„ë¬´ ê²ƒë„ ì—†ì„ ë•Œ default
    return region_default

# C-8-H : ì–¸ì–´ override (041 $a â†’ lang3)
def override_lang_from_041(tag_041: str | None, fallback_lang: str) -> str:
    """
    041 $a ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ 008 lang3ì— ì ìš©.
    ì›ë³¸ _lang3_from_tag041 ê³¼ ë™ì¼.
    """
    if not tag_041:
        return fallback_lang

    m = re.search(r"\$a([a-z]{3})", tag_041, flags=re.I)
    return m.group(1).lower() if m else fallback_lang

# C-8-I : ìµœì¢… 008 ì¡°í•©ê¸° (ì›ë³¸ build_008_from_isbn ì™„ì „ ì¬í˜„)
# ==========================================================
# ìµœì¢… 008 ìƒì„±ê¸° â€” ì›ë³¸ build_008_from_isbn ì™„ì „ ë³µì›
# ==========================================================

def build_008_full(
    isbn: str,
    *,
    aladin_title="",
    aladin_desc="",
    aladin_category="",
    aladin_toc="",
    aladin_pubdate="",
    override_country3=None,     # KPIPA/IMPRINT/MCSTë¡œë¶€í„° ì˜´
    override_lang3=None,        # 041 $aë¡œë¶€í„° ì˜´
    default_country3="ko ",     # ì „ì—­ COUNTRY_FIXED ë™ì¼
    default_lang3="kor",        # ì „ì—­ LANG_FIXED ë™ì¼
):
    today = datetime.datetime.now().strftime("%y%m%d")
    date1 = extract_year_from_pubdate(aladin_pubdate)

    # country3 ê²°ì • (ì›ë³¸ ìˆœì„œ)
    country3 = override_country3 or default_country3

    # lang3 ê²°ì • (041 $a â†’ override)
    lang3 = override_lang3 or default_lang3

    # bigtext = ì œëª© + ì†Œê°œ + ëª©ì°¨
    bigtext = " ".join([aladin_title or "", aladin_desc or "", aladin_toc or ""])

    # ê°ì§€ê¸°ë“¤
    illus4    = detect_illus4(bigtext)
    has_index = detect_index_flag(bigtext)
    lit_form  = detect_lit_form(aladin_title, aladin_category, bigtext)
    bio       = detect_bio_marker(bigtext)

    # ë³¸ë¬¸ 40 bytes ìƒì„±
    body = build_008_body_bk(
        date_entered=today,
        date1=date1,
        country3=country3,
        lang3=lang3,
        illus4=illus4,
        has_index=has_index,
        lit_form=lit_form,
        bio=bio,
        cataloging_src="a",
    )
    return body

# C9. 056(KDC) GPT ë¶„ë¥˜ê¸° ì „ì²´ ëª¨ë“ˆ
# C-9-A : KDC ì‚¬ì „ ì •ê·œí™” í•¨ìˆ˜(ì›ë³¸ normalize_kdc_3digit)
# =================================================================
# KDC ë¬¸ìì—´ì—ì„œ "ì„ í–‰ 1~3ìë¦¬ ì •ìˆ˜"ë§Œ ì¶”ì¶œí•˜ëŠ” ì›ë³¸ normalize í•¨ìˆ˜
# =================================================================

def normalize_kdc_3digit(code: Optional[str]) -> Optional[str]:
    """
    ì…ë ¥ ì˜ˆ: '813.7', '813', '81', '5', 'KDC 325.1'
    ì¶œë ¥ ì˜ˆ: '813', '813', '81', '5', '325'
    ì¦‰, ê°€ì¥ ì•ì˜ ì—°ì†ëœ 1~3ìë¦¬ ìˆ«ìë¥¼ ë°˜í™˜.
    """
    if not code:
        return None
    m = re.search(r"(\d{1,3})", code)
    return m.group(1) if m else None

# C-9-B : KDC ì‘ë‹µ íŒŒì„œ â€” GPT ì‘ë‹µì—ì„œ â€˜3ìë¦¬ ì •ìˆ˜ ë˜ëŠ” ì§ì ‘ë¶„ë¥˜ì¶”ì²œâ€™ë§Œ í—ˆìš©
# =================================================================
# GPT ì‘ë‹µ â†’ 056 ìˆ«ìë§Œ ë‚¨ê¸°ëŠ” íŒŒì„œ
# =================================================================

def parse_llm_kdc_response(text: str) -> Optional[str]:
    """
    ê·œì¹™:
    - 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ' í¬í•¨ â†’ ê·¸ëŒ€ë¡œ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'
    - ìˆ«ì â†’ ê°€ì¥ ì•ì˜ ì—°ì†ëœ 1~3ìë¦¬ë§Œ ì¶”ì¶œ â†’ í•­ìƒ 3ìë¦¬ zero-fill
    - ì˜ˆ: 5 â†’ '005', 81 â†’ '081', 813 â†’ '813'
    """

    if not text:
        return None

    s = text.strip()

    # 1) directly ask for ì§ì ‘ë¶„ë¥˜ì¶”ì²œ
    if "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ" in s:
        return "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"

    # 2) ìµœì´ˆì˜ 1~3ìë¦¬ ì •ìˆ˜ ì¶”ì¶œ
    m = re.search(r"(?<!\d)(\d{1,3})(?!\d)", s)
    if not m:
        return None

    raw = m.group(1)
    num = raw.zfill(3)  # 3ìë¦¬ ê°•ì œ

    if not re.fullmatch(r"\d{3}", num):
        return None

    return num

# C-9-C : 041 ì›ì‘ ì–¸ì–´ ê¸°ë°˜ ë¬¸í•™ KDC ì¬ì •ë ¬ê¸°
# =================================================================
# 041 $h ì›ì‘ì–¸ì–´ â†’ 8xx ë¬¸í•™ ì¬ì •ë ¬ê¸° (ì›ë³¸ ë¡œì§ ì™„ì „ ë°˜ì˜)
# =================================================================

def extract_original_lang_from_041(tag_041: str | None) -> Optional[str]:
    if not tag_041:
        return None
    m = re.search(r"\$h([a-z]{3})", tag_041.lower())
    return m.group(1) if m else None


def map_lang3_to_lit_base(lang3: str | None) -> Optional[str]:
    """
    ì–¸ì–´ì½”ë“œ â†’ ë¬¸í•™ê³„ì—´ 8xx ì•ìë¦¬ 2ìë¦¬.
    """
    if not lang3:
        return None
    l = lang3.lower()
    if l == "kor": return "810"
    if l == "chi" or l == "zho": return "820"
    if l == "jpn": return "830"
    if l == "eng": return "840"
    if l == "ger" or l == "deu": return "850"
    if l == "fre": return "860"
    if l == "spa" or l == "por": return "870"
    if l == "ita": return "880"
    return "890"  # ê¸°íƒ€ ë¬¸í•™


def rebase_8xx_kdc(code: str, tag_041: str) -> str:
    """
    code = '823', tag_041 = '041 $akor$heng' â†’ '843'
    """
    if not code or not code[0] == "8":
        return code  # ë¬¸í•™ì´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ìœ ì§€

    orig_lang = extract_original_lang_from_041(tag_041)
    base = map_lang3_to_lit_base(orig_lang)
    if not base:
        return code

    # code = 8xy â†’ genre = y
    import re
    m = re.match(r"^(\d{3})(\..+)?$", code)
    if not m:
        return code

    head3, tail = m.group(1), (m.group(2) or "")
    genre_digit = head3[2]
    new_code = base[:2] + genre_digit

    return new_code + tail

# C-9-D : GPT í”„ë¡¬í”„íŠ¸ (ì›ë³¸ ask_llm_for_kdc 1ì°¨ í”„ë¡¬í”„íŠ¸ ê·¸ëŒ€ë¡œ)
# =================================================================
# GPT System Prompt (1ì°¨) â€” ì›ë³¸ ask_llm_for_kdc ê·¸ëŒ€ë¡œ ì¬í˜„
# =================================================================

KDC_SYSTEM_PROMPT = (
    "ë„ˆëŠ” í•œêµ­ì‹­ì§„ë¶„ë¥˜ë²•(KDC) ì „ë¬¸ê°€ì´ì ê³µê³µë„ì„œê´€ ë¶„ë¥˜ ì‚¬ì„œì´ë‹¤.\n"
    "ì…ë ¥ëœ ë„ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ì±…ì˜ **ì£¼ì œ ì¤‘ì‹¬ ë¶„ë¥˜ê¸°í˜¸(KDC ë²ˆí˜¸)**ë¥¼ í•œ ì¤„ë¡œ íŒë‹¨í•˜ë¼.\n\n"
    "ì°¸ê³ ë¡œ, êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ KOLISNetì˜ ì‹¤ì œ ë¶„ë¥˜ ì‚¬ë¡€ë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ì°¸ì¡°í•˜ë¼. "
    "ë¹„ìŠ·í•œ ì±…ì´ 821(ì¤‘êµ­ì‹œ)Â·823(ì¤‘êµ­ì†Œì„¤)Â·833(ì¼ë³¸ì†Œì„¤)Â·843(ì˜ë¯¸ì†Œì„¤) ë“±ìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ê´€í–‰ì„ ê³ ë ¤í•˜ë˜, "
    "í˜„ì¬ ë„ì„œì˜ ì£¼ì œì™€ ì¼ì¹˜í•˜ëŠ” í•˜ë‚˜ì˜ ë²ˆí˜¸ë§Œ ì„ íƒí•˜ë¼. (ì›¹ì— ì§ì ‘ ì ‘ì†í•˜ì§€ ë§ê³  ì‚¬ê³ ì˜ ê¸°ì¤€ìœ¼ë¡œë§Œ ì‚¼ëŠ”ë‹¤.)\n\n"
    "ê·œì¹™:\n"
    "1. ë°˜ë“œì‹œ **ì†Œìˆ˜ì  ì—†ì´ 3ìë¦¬ ì •ìˆ˜ë§Œ** ì¶œë ¥í•œë‹¤. ì˜ˆ: 813 / 325 / 005 / 181\n"
    "2. ì„¸ëª©(ì†Œìˆ˜ì  ì´í•˜) íŒë‹¨ì€ ë‚´ë¶€ ê²°ì •ì—ë§Œ í™œìš©í•˜ê³ , **ì¶œë ¥ì€ ìƒìœ„ 3ìë¦¬ ì •ìˆ˜**ë¡œ ì œí•œí•œë‹¤.\n"
    "3. ì„¤ëª…, ì´ìœ , ì ‘ë‘ì–´, ë‹¨ìœ„(ì˜ˆ: KDC, ë¶„ë¥˜ë²ˆí˜¸) ë“±ì€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
    "4. í•œ ì±…ì´ ì—¬ëŸ¬ ì£¼ì œë¥¼ ë‹¤ë£¨ë”ë¼ë„ **ê°€ì¥ ì¤‘ì‹¬ë˜ëŠ” ì£¼ì œ**ë¥¼ ì„ íƒí•œë‹¤.\n"
    "5. ë‚´ìš©ì´ í•™ë¬¸ì ì¼ ê²½ìš°, 'í•™ë¬¸ ë¶„ì•¼' ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•œë‹¤. (ì˜ˆ: êµì–‘ì‹¬ë¦¬ì„œ â†’ 181)\n"
    "6. íŠ¹ì • **ì‹œëŒ€Â·ì¥ë¥´** í‘œê¸°ê°€ ë¶„ëª…í•˜ë”ë¼ë„ **ì¶œë ¥ì€ ìƒìœ„ 3ìë¦¬ ì •ìˆ˜**ë¡œ í•œë‹¤. "
    "(ì•„ë™ë¬¸í•™Â·SF ë“± ì¥ë¥´ë¬¸í•™ì€ ë¨¼ì € **ì–¸ì–´/ì§€ì—­** ê³„ì—´ì„ íŒì •í•œ ë’¤ ë¬¸í•™ ë¶„ê¸° ìƒìœ„ 3ìë¦¬ë¡œ ê²°ì •í•œë‹¤. ì˜ˆ: í•œêµ­ì†Œì„¤ â†’ 813)\n"
    "7. ì¶”ìƒ í‘œí˜„(ì‚¬íšŒì ì˜ì˜, í˜„í™©, ì—°êµ¬, ë¬¸ì œ, ë°©ë²•ë¡  ë“±)ì€ ë¶„ë¥˜ ê·¼ê±°ê°€ ì•„ë‹ˆë‹¤.\n"
    "8. ISBNÂ·ì¶œíŒì‚¬Â·ì¹´í…Œê³ ë¦¬ëŠ” ë³´ì¡° ì‹ í˜¸ë¡œë§Œ ì‚¬ìš©í•œë‹¤.\n"
    "8-1. `keywords_hint_653`ê°€ ì œê³µë˜ë©´ ì•½í•œ ë³´ì¡° ì‹ í˜¸ë¡œë§Œ ì°¸ê³ í•˜ê³ , ì„¤ëª…/ëª©ì°¨/ë²”ì£¼ ì¦ê±°ì™€ ì¶©ëŒí•˜ë©´ ë³¸ë¬¸ ê·¼ê±°ë¥¼ ìš°ì„ í•œë‹¤.\n"
    "9. í™•ì‹ ì´ ì—†ìœ¼ë©´ ê°€ì¥ ê´€ë ¨ ë²”ì£¼ì˜ ê¸°ë³¸ ê¸°í˜¸(ì˜ˆ: ì² í•™â†’100, ë¬¸í•™â†’800)ë¥¼ ê³ ë ¤í•˜ë˜, "
    "ê·¸ë˜ë„ í™•ì •ì´ ì–´ë ¤ìš°ë©´ **ì •í™•íˆ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'** ë„¤ ê¸€ìë§Œ ì¶œë ¥í•œë‹¤.\n\n"
    "[KDC ê°•ëª©í‘œ (10ë‹¨ìœ„)]\n"
    "000 ì´ë¥˜\n010 ë„ì„œí•™ ì„œì§€í•™\n020 ë¬¸í—Œì •ë³´í•™\n030 ë°±ê³¼ì‚¬ì „\n040 ê°•ì—°ì§‘ ìˆ˜í•„ì§‘ ì—°ì„¤ë¬¸ì§‘\n050 ì¼ë°˜ ì—°ì†ê°„í–‰ë¬¼\n"
    "060 ì¼ë°˜ í•™íšŒ ë‹¨ì²´ í˜‘íšŒ ê¸°ê´€ ì—°êµ¬ê¸°ê´€\n070 ì‹ ë¬¸ ì €ë„ë¦¬ì¦˜\n080 ì¼ë°˜ ì „ì§‘ ì´ì„œ\n090 í–¥í† ìë£Œ\n100 ì² í•™\n110 í˜•ì´ìƒí•™\n"
    "120 ì¸ì‹ë¡  ì¸ê³¼ë¡  ì¸ê°„í•™\n130 ì² í•™ì˜ ì²´ê³„\n140 ê²½í•™\n150 ë™ì–‘ì² í•™ ë™ì–‘ì‚¬ìƒ\n160 ì„œì–‘ì² í•™\n170 ë…¼ë¦¬í•™\n180 ì‹¬ë¦¬í•™\n"
    "190 ìœ¤ë¦¬í•™ ë„ë•ì² í•™\n200 ì¢…êµ\n210 ë¹„êµì¢…êµ\n220 ë¶ˆêµ\n230 ê¸°ë…êµ\n240 ë„êµ\n250 ì²œë„êµ\n270 íŒë‘êµ ë¸Œë¼ë§Œêµ\n"
    "280 ì´ìŠ¬ëŒêµ íšŒêµ\n290 ê¸°íƒ€ ì œì¢…êµ\n300 ì‚¬íšŒê³¼í•™\n310 í†µê³„ìë£Œ\n320 ê²½ì œí•™\n330 ì‚¬íšŒí•™ ì‚¬íšŒë¬¸ì œ\n340 ì •ì¹˜í•™\n350 í–‰ì •í•™\n"
    "360 ë²•ë¥  ë²•í•™\n370 êµìœ¡í•™\n380 í’ìŠµ ì˜ˆì ˆ ë¯¼ì†í•™\n390 êµ­ë°© êµ°ì‚¬í•™\n400 ìì—°ê³¼í•™\n410 ìˆ˜í•™\n420 ë¬¼ë¦¬í•™\n430 í™”í•™\n440 ì²œë¬¸í•™\n"
    "450 ì§€í•™\n460 ê´‘ë¬¼í•™\n470 ìƒëª…ê³¼í•™\n480 ì‹ë¬¼í•™\n490 ë™ë¬¼í•™\n500 ê¸°ìˆ ê³¼í•™\n510 ì˜í•™\n520 ë†ì—… ë†í•™\n530 ê³µí•™ ê³µì—…ì¼ë°˜ í† ëª©ê³µí•™ í™˜ê²½ê³µí•™\n"
    "540 ê±´ì¶• ê±´ì¶•í•™\n550 ê¸°ê³„ê³µí•™\n560 ì „ê¸°ê³µí•™ í†µì‹ ê³µí•™ ì „ìê³µí•™\n570 í™”í•™ê³µí•™\n580 ì œì¡°ì—…\n590 ìƒí™œê³¼í•™\n600 ì˜ˆìˆ \n620 ì¡°ê° ì¡°í˜•ë¯¸ìˆ \n"
    "630 ê³µì˜ˆ\n640 ì„œì˜ˆ\n650 íšŒí™” ë„í™” ë””ìì¸\n660 ì‚¬ì§„ì˜ˆìˆ \n670 ìŒì•…\n680 ê³µì—°ì˜ˆìˆ  ë§¤ì²´ì˜ˆìˆ \n690 ì˜¤ë½ ìŠ¤í¬ì¸ \n700 ì–¸ì–´\n710 í•œêµ­ì–´\n"
    "720 ì¤‘êµ­ì–´\n730 ì¼ë³¸ì–´ ë° ê¸°íƒ€ ì•„ì‹œì•„ì œì–´\n740 ì˜ì–´\n750 ë…ì¼ì–´\n760 í”„ë‘ìŠ¤ì–´\n770 ìŠ¤í˜ì¸ì–´ ë° í¬ë¥´íˆ¬ê°ˆì–´\n780 ì´íƒˆë¦¬ì•„ì–´\n790 ê¸°íƒ€ ì œì–´\n"
    "800 ë¬¸í•™\n810 í•œêµ­ë¬¸í•™\n820 ì¤‘êµ­ë¬¸í•™\n830 ì¼ë³¸ë¬¸í•™ ë° ê¸°íƒ€ ì•„ì‹œì•„ ì œë¬¸í•™\n840 ì˜ë¯¸ë¬¸í•™\n850 ë…ì¼ë¬¸í•™\n860 í”„ë‘ìŠ¤ë¬¸í•™\n"
    "870 ìŠ¤í˜ì¸ë¬¸í•™ ë° í¬ë¥´íˆ¬ê°ˆë¬¸í•™\n880 ì´íƒˆë¦¬ì•„ë¬¸í•™\n890 ê¸°íƒ€ ì œë¬¸í•™\n900 ì—­ì‚¬\n910 ì•„ì‹œì•„\n920 ìœ ëŸ½\n930 ì•„í”„ë¦¬ì¹´\n"
    "940 ë¶ì•„ë©”ë¦¬ì¹´\n950 ë‚¨ì•„ë©”ë¦¬ì¹´\n960 ì˜¤ì„¸ì•„ë‹ˆì•„ ì–‘ê·¹ì§€ë°©\n980 ì§€ë¦¬\n990 ì „ê¸°\n\n"
)

# C-9-E : GPT í˜¸ì¶œê¸° (1ì°¨ / 2ì°¨ fallback)
# =================================================================
# ì›ë³¸ ask_llm_for_kdc() êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©° ê°€ë…ì„± ê°œì„ 
# =================================================================

def call_gpt_for_kdc(payload: dict, keywords_hint: list[str], api_key: str, model: str) -> Optional[str]:
    """
    1ì°¨: KDC_SYSTEM_PROMPT + payload + keywords_hint
    """
    keyword_str = ", ".join(keywords_hint or [])
    user_prompt = (
        "ì•„ë˜ ë„ì„œ ì •ë³´(JSON)ë¥¼ ì°¸ê³ í•˜ì—¬ **KDC ë¶„ë¥˜ê¸°í˜¸ë¥¼ ì†Œìˆ˜ì  ì—†ì´ 3ìë¦¬ ì •ìˆ˜ë¡œ í•œ ì¤„**ë§Œ ì¶œë ¥í•˜ë¼. "
        "í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ **ì •í™•íˆ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'**ë§Œ ì¶œë ¥.\n\n"
        f"â€» ì°¸ê³ ìš© í‚¤ì›Œë“œ(653): {keyword_str or '(ì—†ìŒ)'}\n\n"
        f"{json.dumps(payload, ensure_ascii=False, indent=2)}\n\n"
        "ì¶œë ¥ ì˜ˆì‹œ: 823 / 813 / 325 / 181 / (í™•ì‹ ì—†ìŒ) ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"
    )

    # ---------------------
    # 1ì°¨ GPT í˜¸ì¶œ
    # ---------------------
    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": KDC_SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": 0.0,
                "max_tokens": 18,
            },
            timeout=45,
        )
        resp.raise_for_status()
        text = resp.json()["choices"][0]["message"]["content"].strip()
        parsed = parse_llm_kdc_response(text)
        if parsed:
            return parsed
    except Exception:
        pass

    # ---------------------
    # 2ì°¨ fallback
    # ---------------------
    fb_sys = (
        "ë„ˆëŠ” KDC ì œ6íŒ ê¸°ì¤€ ë¶„ë¥˜ ì‚¬ì„œë‹¤. "
        "ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ **3ìë¦¬ ì •ìˆ˜**ë§Œ ì¶œë ¥í•˜ë¼. "
        "ì •í™•íˆ íŒë‹¨í•˜ê¸° ì–´ë µë‹¤ë©´ **ì •í™•íˆ 'ì§ì ‘ë¶„ë¥˜ì¶”ì²œ'**ë§Œ ì¶œë ¥."
    )
    fb_user = f"ë„ì„œ ì •ë³´:\n{json.dumps(payload, ensure_ascii=False, indent=2)}"

    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": fb_sys},
                    {"role": "user", "content": fb_user},
                ],
                "temperature": 0.0,
                "max_tokens": 8,
            },
            timeout=45,
        )
        resp.raise_for_status()
        text = resp.json()["choices"][0]["message"]["content"].strip()
        parsed = parse_llm_kdc_response(text)
        if parsed:
            return parsed
    except Exception:
        pass

    # ---------------------
    # 3ì°¨ local fallback
    # ---------------------
    return "ì§ì ‘ë¶„ë¥˜ì¶”ì²œ"

# C-9-F : ìµœì¢… ì™¸ë¶€ API â€” get_kdc_from_isbn()
# =================================================================
# ì™¸ë¶€ì—ì„œ í˜¸ì¶œí•˜ëŠ” ìµœì¢… API â€” ì›ë³¸ get_kdc_from_isbn ì™„ì „ ë³µì›
# =================================================================

def get_kdc_from_isbn(
    isbn13: str,
    ttbkey: Optional[str],
    openai_key: str,
    model: str,
    keywords_hint: list[str] | None = None
) -> Optional[str]:

    # 1) ì•Œë¼ë”˜ API ë˜ëŠ” ìŠ¤í¬ë ˆì´í•‘
    info = aladin_lookup_by_api(isbn13, ttbkey) if ttbkey else None
    if not info:
        info = aladin_lookup_by_web(isbn13)
    if not info:
        return None

    # 2) LLM ì…ë ¥ ì¶•ì•½ (ì›ë³¸ê³¼ ë™ì¼í•œ ê¸¸ì´ ì œí•œ)
    def clip(s: str, n: int) -> str:
        return "" if not s else (s if len(s) <= n else s[:n] + "â€¦")

    payload = {
        "title": clip(info.title, 160),
        "author": clip(info.author, 120),
        "publisher": info.publisher,
        "pub_date": info.pub_date,
        "isbn13": info.isbn13,
        "category": clip(info.category, 160),
        "description": clip(info.description, 1200),
        "toc": clip(info.toc, 1200),
    }

    # 3) GPT í˜¸ì¶œ
    code = call_gpt_for_kdc(
        payload=payload,
        keywords_hint=keywords_hint or [],
        api_key=openai_key,
        model=model,
    )
    if not code:
        return None

    # 4) ë¬¸í•™ 8xx ì¬ì •ë ¬ (041 ì›ì‘ ì–¸ì–´ ê¸°ë°˜)
    marc041 = getattr(info, "marc041", "") or getattr(info, "field_041", "") or getattr(info, "f041", "")
    code = rebase_8xx_kdc(code, marc041)

    return code

# C10. 300(í˜•íƒœì‚¬í•­) í¬ë¡¤ëŸ¬ + íŒŒì„œ ëª¨ë“ˆ
# C-10-A : í…ìŠ¤íŠ¸ ì •ê·œí™” ìœ í‹¸ (ì›ë³¸ _norm, _clean_author_str)
# ==========================================================
# í…ìŠ¤íŠ¸ ì •ê·œí™” â€” ì›ë³¸ ì™„ì „ ì¬í˜„
# ==========================================================

def _norm(text: str) -> str:
    import unicodedata
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text).lower()
    text = re.sub(r"[^\w\s\uac00-\ud7a3]", " ", text)  # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ í—ˆìš©
    return re.sub(r"\s+", " ", text).strip()


def _clean_author_str(s: str) -> str:
    """
    (ì§€ì€ì´), (ì˜®ê¸´ì´) ë“± ê´„í˜¸ ì—­í•  ì œê±° + / ; , Â· ì œê±°
    """
    if not s:
        return ""
    s = re.sub(r"\(.*?\)", " ", s)
    s = re.sub(r"[/;Â·,]", " ", s)
    return re.sub(r"\s+", " ", s).strip()

# C-10-B : ê¸ˆì¹™ì–´(forbidden) ì§‘í•© ë§Œë“¤ê¸° (ì›ë³¸ _build_forbidden_set)
# ==========================================================
# forbidden ë‹¨ì–´ ì§‘í•© â€” ì›ë³¸ ì™„ì „ ì¬í˜„
# ==========================================================

def _build_forbidden_set(title: str, authors: str) -> set:
    t_norm = _norm(title)
    a_norm = _norm(authors)

    forb = set()

    if t_norm:
        forb.update(t_norm.split())
        forb.add(t_norm.replace(" ", ""))

    if a_norm:
        forb.update(a_norm.split())
        forb.add(a_norm.replace(" ", ""))

    # 2ê¸€ì ë¯¸ë§Œ ì œê±°
    return {f for f in forb if f and len(f) >= 2}

# C-10-C : forbidden í•„í„°ë§ ì—¬ë¶€ íŒì •ê¸° (ì›ë³¸ _should_keep_keyword)
# ==========================================================
# keyword í•„í„° í•¨ìˆ˜ â€” ì›ë³¸ ì™„ì „ ì¬í˜„
# ==========================================================

def _should_keep_keyword(kw: str, forbidden: set) -> bool:
    n = _norm(kw)
    if not n or len(n.replace(" ", "")) < 2:
        return False

    for tok in forbidden:
        # forbiddenê³¼ ë™ì¼í•˜ê±°ë‚˜ í¬í•¨ë˜ë©´ ì œì™¸
        if tok in n or n in tok:
            return False

    return True

# C-10-D : ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„° íŒŒì„œ (ì›ë³¸ fetch_aladin_metadata)
# ==========================================================
# ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„° â€” ì›ë³¸ fetch_aladin_metadata ì™„ì „ ì¬í˜„
# ==========================================================

def fetch_aladin_metadata(isbn):
    url = (
        "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        f"?ttbkey={aladin_key}"
        "&ItemIdType=ISBN"
        f"&ItemId={isbn}"
        "&output=js"
        "&Version=20131101"
        "&OptResult=Toc"
    )

    data = requests.get(url).json()
    item = (data.get("item") or [{}])[0]

    raw_author = (
        item.get("author")
        or item.get("authors")
        or item.get("author_t")
        or ""
    )

    authors = _clean_author_str(raw_author)

    return {
        "category": item.get("categoryName", "") or "",
        "title": item.get("title", "") or "",
        "authors": authors,
        "description": item.get("description", "") or "",
        "toc": item.get("toc", "") or "",
    }

# C-10-E : GPT í”„ë¡¬í”„íŠ¸ â€” ì›ë³¸ì˜ ì´ˆì •ë°€ ë²„ì „ 100% ì¬í˜„
# ==========================================================
# 653 GPT ìƒì„± í”„ë¡¬í”„íŠ¸ (system + user) â€” ì›ë³¸ ê·¸ëŒ€ë¡œ
# ==========================================================

def build_653_prompts(category, title, authors, description, toc, forbidden_list, max_keywords):
    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ KORMARC ì‘ì„± ê²½í—˜ì´ í’ë¶€í•œ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ë¶„ë¥˜ ì •ë³´, ì„¤ëª…, ëª©ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'MARC 653 ììœ ì£¼ì œì–´'ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.\n\n"
            "ì›ì¹™\n"
            "- 653ì€ 'ê²€ìƒ‰Â·ë°œê²¬' íš¨ìš©ì„ ë†’ì´ëŠ” ëª…ì‚¬ ì¤‘ì‹¬ ì£¼ì œì–´ë¡œ êµ¬ì„±í•˜ë˜, "
            "**ëª¨ë“  ì£¼ì œì–´ëŠ” ë¶™ì—¬ì“°ê¸° í˜•íƒœ(ê³µë°± ì—†ìŒ)**ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.\n"
            "- ì„œëª…Â·ì €ì(ì—­í•  í¬í•¨)Â·ì‹œë¦¬ì¦ˆëª…Â·ì¶œíŒì‚¬ëª…Â·íŒì°¨Â·ì—°ë„Â·ê°€ê²©Â·í™ë³´ë¬¸êµ¬ë¥¼ ì œì™¸.\n"
            "- ë„ˆë¬´ ì¼ë°˜ì  í‘œí˜„(ì—°êµ¬, ë°©ë²•, ì‚¬ë¡€, ê³ ì°° ë“±)ê³¼ "
            "**ì¶”ìƒÂ·í‰ê°€Â·ë©”íƒ€ í‘œí˜„(ì˜ì˜, ì˜ë¯¸, ë™í–¥, ë°°ê²½ ë“±)**ì€ ê¸ˆì§€.\n"
            "- ë– ì˜¤ë¥¸ ì¶”ìƒ ê°œë…ì€ ë°˜ë“œì‹œ ì±…ì˜ ì‹¤ì œ ì£¼ì œë¥¼ ë“œëŸ¬ë‚´ëŠ” **êµ¬ì²´ í•˜ìœ„ê°œë…**ìœ¼ë¡œ ì¹˜í™˜.\n"
            "- ë¶„ë¥˜ì˜ ë§ˆì§€ë§‰ ìš”ì†Œë¥¼ ì°¸ê³ í•˜ë˜, ê´€ë ¨ì„±Â·êµ¬ì²´ì„±Â·ë¹„ì¤‘ë³µì„±Â·ê· í˜•ì„ ì›ì¹™ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.\n\n"
            "ì¶œë ¥ í˜•ì‹\n"
            f"- `$aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 â€¦` í˜•ì‹ìœ¼ë¡œ, ìµœëŒ€ {max_keywords}ê°œ.\n"
            "- ì„¤ëª… ë¬¸ì¥ ê¸ˆì§€. ê²°ê³¼ë§Œ ì œì‹œ.\n"
        ),
    }

    user_msg = {
        "role": "user",
        "content": (
            f"ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœëŒ€ {max_keywords}ê°œì˜ MARC 653 ì£¼ì œì–´ë¥¼ í•œ ì¤„ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n\n"
            f"- ë¶„ë¥˜ ì „ì²´: \"{category}\"\n"
            f"- ì œëª©: \"{title}\"\n"
            f"- ì €ì: \"{authors}\"\n"
            f"- ì„¤ëª…: \"{description}\"\n"
            f"- ëª©ì°¨: \"{toc}\"\n"
            f"- ì œì™¸ì–´ ëª©ë¡: {forbidden_list}\n\n"
            "ì§€ì‹œì‚¬í•­:\n"
            "1) ì œëª©Â·ì €ìì—ì„œ ìœ ë˜í•œ ë‹¨ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.\n"
            "2) ì„¤ëª…Â·ëª©ì°¨ ê¸°ë°˜ìœ¼ë¡œ **ëª…ì‚¬ ì¤‘ì‹¬(2~6ê¸€ì ë³µí•©ëª…ì‚¬)** ìƒì„±.\n"
            "3) ëª¨ë“  í‚¤ì›Œë“œëŠ” **ë„ì–´ì“°ê¸°ë¥¼ ì œê±°**í•˜ì—¬ ì¶œë ¥.\n"
            "4) ì¤‘ë³µÂ·ë™ì˜ì–´ëŠ” í•˜ë‚˜ë§Œ.\n"
            "5) ì¶œë ¥ì€ `$aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 â€¦` ë‹¨ í•œ ì¤„.\n"
        ),
    }

    return system_msg, user_msg

# C-10-F : GPT í˜¸ì¶œ + í‚¤ì›Œë“œ íŒŒì‹± (ì›ë³¸ generate_653_with_gpt)
# ==========================================================
# GPT í˜¸ì¶œ â†’ $a í‚¤ì›Œë“œ íŒŒì‹± â€” ì›ë³¸ ì™„ì „ ë³µì‚¬ + êµ¬ì¡° ê°œì„ 
# ==========================================================

def generate_653_keywords(category, title, authors, description, toc, max_keywords=7):
    forbidden = _build_forbidden_set(title, authors)
    forbidden_list = ", ".join(sorted(forbidden)) or "(ì—†ìŒ)"

    system_msg, user_msg = build_653_prompts(
        category=category,
        title=title,
        authors=authors,
        description=description,
        toc=toc,
        forbidden_list=forbidden_list,
        max_keywords=max_keywords,
    )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=180,
        )
        raw = (resp.choices[0].message.content or "").strip()

        # $a íŒŒì‹±
        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]

        if not kws:
            tmp = re.split(r"[,\n;|/Â·]", raw)
            kws = [t.strip().lstrip("$a") for t in tmp if t.strip()]

        # ë¶™ì—¬ì“°ê¸° ê°•ì œ
        kws = [kw.replace(" ", "") for kw in kws if kw]

        # forbidden í•„í„°
        kws = [kw for kw in kws if _should_keep_keyword(kw, forbidden)]

        # ì¤‘ë³µ ì œê±°
        seen = set()
        uniq = []
        for kw in kws:
            n = _norm(kw)
            if n not in seen:
                seen.add(n)
                uniq.append(kw)

        uniq = uniq[:max_keywords]

        return [kw for kw in uniq]

    except Exception as e:
        st.warning(f"âš ï¸ 653 ì£¼ì œì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        return []

# C-10-G : 653 MARC ë¼ì¸ ìƒì„±ê¸° (=653 \$aâ€¦)
# ==========================================================
# ìµœì¢… 653 MRK ë¼ì¸ ë¹Œë” â€” ì›ë³¸ _build_653_via_gpt
# ==========================================================

def build_653_mrk_from_item(item: dict) -> Optional[str]:
    title = (item or {}).get("title", "") or ""
    category = (item or {}).get("categoryName", "") or ""
    raw_author = (item or {}).get("author", "") or ""
    desc = (item or {}).get("description", "") or ""
    toc  = ((item or {}).get("subInfo", {}) or {}).get("toc", "") or ""

    authors_clean = _clean_author_str(raw_author)

    kws = generate_653_keywords(
        category=category,
        title=title,
        authors=authors_clean,
        description=desc,
        toc=toc,
        max_keywords=7,
    )
    if not kws:
        return None

    return "=653  \\\\" + "".join(f"$a{kw}" for kw in kws)

# C11. 490/830 ì´ì„œ ëª¨ë“ˆ
# C-11-A : ì‚½í™” ê°ì§€ê¸°(detect_illustrations)
# ==========================================================
# ì‚½í™” ê°ì§€ â€” ì›ë³¸ ì™„ì „ ì¬í˜„ + ì •ê·œì‹ ì•ˆì •í™”
# ==========================================================

def detect_illustrations(text: str):
    """
    ì œëª© + ë¶€ì œ + ì±…ì†Œê°œ ì „ì²´ ë¬¸ìì—´ì—ì„œ ì‚½í™” ì—¬ë¶€ë¥¼ ê°ì§€í•œë‹¤.
    ì›ë³¸ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€.
    """
    if not text:
        return False, None

    # ì›ë³¸ ìŠ¤í‚¤ë§ˆ
    keyword_groups = {
        "ì²œì—°ìƒ‰ì‚½í™”": ["ì‚½í™”", "ì¼ëŸ¬ìŠ¤íŠ¸", "ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜", "illustration", "ê·¸ë¦¼"],
        "ì‚½í™”": ["í‘ë°± ì‚½í™”", "í‘ë°± ì¼ëŸ¬ìŠ¤íŠ¸", "í‘ë°± ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜", "í‘ë°± ê·¸ë¦¼"],
        "ì‚¬ì§„": ["ì‚¬ì§„", "í¬í† ", "photo", "í™”ë³´"],
        "ë„í‘œ": ["ë„í‘œ", "ì°¨íŠ¸", "ê·¸ë˜í”„"],
        "ì§€ë„": ["ì§€ë„", "ì§€ë„ì±…"],
    }

    found_labels = set()
    lower = text.lower()

    for label, keywords in keyword_groups.items():
        for kw in keywords:
            if kw.lower() in lower:
                found_labels.add(label)

    if found_labels:
        return True, ", ".join(sorted(found_labels))

    return False, None

# C-11-B : ì•Œë¼ë”˜ ìƒì„¸ HTML â†’ 300 ìš”ì†Œ íŒŒì„œ
# ==========================================================
# ì•Œë¼ë”˜ ìƒì„¸ í˜ì´ì§€ HTML íŒŒì‹± (í˜ì´ì§€/í¬ê¸°/ì‚½í™”)
# ì›ë³¸ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ë”°ë¥¸ë‹¤.
# ==========================================================

def parse_aladin_physical_book_info(html):
    soup = BeautifulSoup(html, "html.parser")

    # -------------------------------
    # ì œëª© / ë¶€ì œ / ì±…ì†Œê°œ (ì‚½í™” ê°ì§€ìš©)
    # -------------------------------
    title = soup.select_one("span.Ere_bo_title")
    subtitle = soup.select_one("span.Ere_sub1_title")

    title_text = title.get_text(strip=True) if title else ""
    subtitle_text = subtitle.get_text(strip=True) if subtitle else ""

    desc_tag = soup.select_one("div.Ere_prod_mconts_R")
    description = desc_tag.get_text(" ", strip=True) if desc_tag else ""

    # -------------------------------
    # í˜•íƒœì‚¬í•­ (ìª½ìˆ˜, í¬ê¸° mm â†’ cm)
    # -------------------------------
    form_wrap = soup.select_one("div.conts_info_list1")

    a_part = ""
    b_part = ""
    c_part = ""

    page_value = None
    size_value = None

    if form_wrap:
        items = [x.strip() for x in form_wrap.stripped_strings if x.strip()]

        for item in items:
            # --- ìª½ìˆ˜ ---
            if re.search(r"(ìª½|p)\s*$", item):
                m = re.search(r"\d+", item)
                if m:
                    page_value = int(m.group())
                    a_part = f"{page_value} p."

            # --- í¬ê¸°(mm) ---
            elif "mm" in item.lower():
                m = re.search(r"(\d+)\s*[\*xÃ—X]\s*(\d+)", item)
                if m:
                    width = int(m.group(1))
                    height = int(m.group(2))
                    size_value = f"{width}x{height}mm"

                    # ì›ë³¸ ì•Œê³ ë¦¬ì¦˜ ê·¸ëŒ€ë¡œ: ê°€ë¡œ/ì„¸ë¡œ ë¹„ì •ìƒ ë¹„ìœ¨ ì²˜ë¦¬
                    if width == height or width > height or width < height / 2:
                        w_cm = math.ceil(width / 10)
                        h_cm = math.ceil(height / 10)
                        c_part = f"{w_cm}x{h_cm} cm"
                    else:
                        h_cm = math.ceil(height / 10)
                        c_part = f"{h_cm} cm"

    # -------------------------------
    # ì‚½í™” ê°ì§€
    # -------------------------------
    combined_text = " ".join(filter(None, [title_text, subtitle_text, description]))
    has_illus, illus_label = detect_illustrations(combined_text)

    if has_illus:
        b_part = illus_label

    # -------------------------------
    # 300 í•„ë“œ subfields êµ¬ì„±
    # -------------------------------
    subfields_300 = []

    if a_part:
        subfields_300.append(Subfield("a", a_part))
    if b_part:
        subfields_300.append(Subfield("b", b_part))
    if c_part:
        subfields_300.append(Subfield("c", c_part))

    # -------------------------------
    # MRK (ì‚¬ëŒìš© ë¬¸ìì—´) ì¡°í•© ê·œì¹™ â€” ì›ë³¸ ê·¸ëŒ€ë¡œ
    # -------------------------------
    mrk_parts = []

    # 1) $a + :$b
    if a_part:
        temp = f"$a{a_part}"
        if b_part:
            temp += f" :$b{b_part}"
        mrk_parts.append(temp)
    elif b_part:
        mrk_parts.append(f"$b{b_part}")

    # 2) $cëŠ” ; ë¡œ ì—°ê²°
    if c_part:
        if mrk_parts:
            mrk_parts.append(f"; $c {c_part}")
        else:
            mrk_parts.append(f"$c {c_part}")

    # 3) ì•„ë¬´ ê²ƒë„ ì—†ìœ¼ë©´ fallback
    if not mrk_parts:
        mrk_parts = ["$a1ì±…."]
        subfields_300 = [Subfield("a", "1ì±….")]

    field_300 = "=300  \\\\" + " ".join(mrk_parts)

    return {
        "300": field_300,
        "300_subfields": subfields_300,
        "page_value": page_value,
        "size_value": size_value,
        "illustration_possibility": illus_label if illus_label else "ì—†ìŒ",
    }

# C-11-C : ìƒì„¸ í˜ì´ì§€ ìš”ì²­ê¸°(search_aladin_detail_page)
# ==========================================================
# ì•Œë¼ë”˜ ìƒì„¸ í˜ì´ì§€ HTML ìš”ì²­ê¸° â€” ì›ë³¸ + ì˜ˆì™¸ ë³´ê°•
# ==========================================================

def search_aladin_detail_page(link):
    try:
        res = requests.get(link, timeout=15)
        res.raise_for_status()
        return parse_aladin_physical_book_info(res.text), None

    except Exception as e:
        return {
            "300": "=300  \\\\$a1ì±…. [ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜]",
            "300_subfields": [Subfield("a", "1ì±… [íŒŒì‹± ì‹¤íŒ¨]")],
            "page_value": None,
            "size_value": None,
            "illustration_possibility": "ì •ë³´ ì—†ìŒ",
        }, f"Aladin ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì™¸: {e}"

# C-11-D : ìµœì¢… 300 Field Builder (MRK + Field ë‘ ê°€ì§€ ë°˜í™˜)
# ==========================================================
# 300 í•„ë“œ ìµœì¢… ë¹Œë” â€” ì›ë³¸ build_300_from_aladin_detail
# ==========================================================

def build_300_from_aladin_detail(item: dict) -> tuple[str, Field]:
    try:
        aladin_link = (item or {}).get("link", "")

        if not aladin_link:
            # ì›ë³¸ fallback
            fallback = "=300  \\\\$a1ì±…."
            return fallback, Field(
                tag="300",
                indicators=[" ", " "],
                subfields=[Subfield("a", "1ì±….")]
            )

        detail, err = search_aladin_detail_page(aladin_link)

        tag_300 = detail.get("300") or "=300  \\\\$a1ì±…."
        subfields = detail.get("300_subfields") or [Subfield("a", "1ì±….")]

        f_300 = Field(
            tag="300",
            indicators=[" ", " "],
            subfields=subfields
        )

        return tag_300, f_300

    except Exception as e:
        fallback = "=300  \\\\$a1ì±…. [ì˜ˆì™¸]"
        return fallback, Field(
            tag="300",
            indicators=[" ", " "],
            subfields=[Subfield("a", "1ì±…. [ì˜ˆì™¸]")]
        )

# C-11-E : MRK ì „ìš© ë²„ì „(build_300_mrk)
# ==========================================================
# MRKë§Œ í•„ìš”í•  ë•Œ ì“°ëŠ” ì–‡ì€ Wrapper â€” ì›ë³¸ ì¬í˜„
# ==========================================================

def build_300_mrk(item: dict) -> str:
    tag_300, _ = build_300_from_aladin_detail(item)
    return tag_300 or "=300  \\\\$a1ì±…."

# C12. ìµœì¢… MARC Builder ì¡°ë¦½ê¸°
# C-12-A â€” ì´ì„œ(SeriesInfo) ì¶”ì¶œê¸°
# ==========================================================
# 490/830 ì´ì„œ ì •ë³´ ì¶”ì¶œ â€” ì›ë³¸ ë¡œì§ 100% ë™ì¼
# ==========================================================

def extract_series_info(item: dict):
    """
    ì›ë³¸ì˜ seriesInfo ì¶”ì¶œ ê³¼ì •ê³¼ ë™ì¼:
      1) item["seriesInfo"]
      2) item["subInfo"]["seriesInfo"]
      3) ê°€ì¥ ì²« ë²ˆì§¸ ìœ íš¨í•œ ì—”íŠ¸ë¦¬ ì‚¬ìš©
    ë°˜í™˜ê°’: (series_name, volume)
    """
    si = None

    if isinstance(item, dict):
        si = item.get("seriesInfo") or (item.get("subInfo") or {}).get("seriesInfo")

    # ì‹œë¦¬ì¦ˆê°€ ë¦¬ìŠ¤íŠ¸í˜• / dictí˜• ë‘˜ ë‹¤ ëŒ€ì‘
    entries = []
    if isinstance(si, list):
        entries = si
    elif isinstance(si, dict):
        entries = [si]

    series_name = ""
    series_vol = ""

    # ì›ë³¸: ê°€ì¥ ë¨¼ì € ë“±ì¥í•˜ëŠ” ìœ íš¨í•œ ì´ì„œë§Œ ì‚¬ìš©
    for ent in entries or []:
        if not isinstance(ent, dict):
            continue
        name = (ent.get("seriesName") or ent.get("name") or "").strip()
        vol  = (ent.get("volume") or ent.get("vol") or "").strip()
        if name:
            series_name = name
            series_vol = vol
            break

    # ì¶”ê°€ fallback (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    if not series_name:
        series_name = (item.get("seriesName") or "").strip()

    if not series_vol:
        series_vol = (item.get("volume") or "").strip()

    return series_name, series_vol

# C-12-B â€” í‘œì‹œ ë¬¸ìì—´(series_display) êµ¬ì„±ê¸°
# ==========================================================
# ì´ì„œ í‘œì‹œ ë¬¸ìì—´ ìƒì„±ê¸° â€” ì›ë³¸ ë™ì¼
# ==========================================================

def build_series_display(series_name: str, series_vol: str) -> str | None:
    """
    ì›ë³¸ êµ¬ì„± ê·œì¹™:
      - "{name} {vol}".strip()
      - nameì´ ì—†ìœ¼ë©´ ì´ì„œ ì—†ìŒ â†’ None
    """
    if not series_name:
        return None

    if series_vol:
        display = f"{series_name} {series_vol}".strip()
    else:
        display = series_name.strip()

    return display or None

# C-12-C â€” 490 / 830 MRK ë¬¸ìì—´ ìƒì„±ê¸°
# ==========================================================
# MRK ë¬¸ìì—´ ìƒì„± â€” ì›ë³¸ê³¼ ë™ì¼í•œ í¬ë§·
# ==========================================================

def build_mrk_490(series_display: str) -> str:
    return f"=490  10$a{series_display}"

def build_mrk_830(series_display: str) -> str:
    return f"=830  \\0$a{series_display}"

# C-12-D â€” pymarc.Field ìƒì„±ê¸°
# ==========================================================
# pymarc Field ìƒì„± â€” ì›ë³¸ mrk_str_to_field ë™ì‘ê³¼ ë™ì¼
# ==========================================================

def build_field_490(series_display: str) -> Field:
    return Field(
        tag="490",
        indicators=["1", "0"],   # ì›ë³¸ ê·œì¹™ ìœ ì§€
        subfields=[Subfield("a", series_display)]
    )

def build_field_830(series_display: str) -> Field:
    return Field(
        tag="830",
        indicators=[" ", "0"],   # ì›ë³¸ "\0"ì€ MRKì—ì„œì˜ escape â†’ ì‹¤ì œëŠ” [" ", "0"]
        subfields=[Subfield("a", series_display)]
    )

# C-12-E â€” 490/830 ì „ì²´ í†µí•© ë¹Œë”(ì›ë³¸ build_490_830_mrk_from_item ë³µì›)
# ==========================================================
# 490/830 ì „ì²´ ë¹Œë” â€” ì›ë³¸ í•¨ìˆ˜ ì™„ì „ ë³µì›(True Patch)
# ==========================================================

def build_490_830_from_item(item: dict):
    """
    ë°˜í™˜ê°’:
        tag_490 (str or "")
        tag_830 (str or "")
        f_490 (Field or None)
        f_830 (Field or None)
    """

    series_name, series_vol = extract_series_info(item)
    series_display = build_series_display(series_name, series_vol)

    # ì›ë³¸: ì´ì„œ ì •ë³´ ì—†ìœ¼ë©´ ëª¨ë‘ ë¹ˆ ê°’ ë°˜í™˜
    if not series_display:
        return "", "", None, None

    # MRK ë¬¸ìì—´ ìƒì„±
    tag_490 = build_mrk_490(series_display)
    tag_830 = build_mrk_830(series_display)

    # pymarc.Field ìƒì„±
    f_490 = build_field_490(series_display)
    f_830 = build_field_830(series_display)

    return tag_490, tag_830, f_490, f_830

# C-12-F â€” ê¸°ì¡´ generate_all_oneclickì— ì‚½ì…ë˜ëŠ” í˜•íƒœ
# generate_all_oneclick ë‚´ë¶€ì—ì„œ í˜¸ì¶œ í˜•íƒœ
tag_490, tag_830, f_490, f_830 = build_490_830_from_item(item)

if f_490:
    pieces.append((f_490, tag_490))
if f_830:
    pieces.append((f_830, tag_830))

# C13. 049 í•„ë“œ ìƒì„±ê¸°
# ============================================================
# C-13) 049 ìƒì„±ê¸° â€” ì›ë³¸ íŒë‹¨ ë¡œì§ 100% ìœ ì§€ + êµ¬ì¡° ì•ˆì •í™”
# ============================================================

def build_049(reg_mark: str = "", reg_no: str = "", copy_symbol: str = "") -> str:
    """
    ë„¤ ì›ë³¸ ì½”ë“œì˜ 049 ìƒì„± ê·œì¹™ì„ 100% ë™ì¼í•˜ê²Œ êµ¬í˜„í•œ True Patch.
    ê·œì¹™:
      - ì„¸ ê°’ ëª¨ë‘ ë¹„ì–´ ìˆìœ¼ë©´ None ë°˜í™˜
      - ë“±ë¡ê¸°í˜¸(reg_mark) â†’ $a
      - ë“±ë¡ë²ˆí˜¸(reg_no) â†’ $b
      - ë³„ì¹˜ê¸°í˜¸(copy_symbol) â†’ $c
      - ê³µë°±/ìŠ¬ë˜ì‹œ/ê°œí–‰ ì œê±° ë° ì–‘ìª½ trim
      - ê°’ ìˆëŠ” ì„œë¸Œí•„ë“œë§Œ ì¶œë ¥
    """
    reg_mark = (reg_mark or "").strip()
    reg_no = (reg_no or "").strip()
    copy_symbol = (copy_symbol or "").strip()

    if not (reg_mark or reg_no or copy_symbol):
        return None

    parts = ["=049  \\\\"]

    if reg_mark:
        parts.append(f"$a{reg_mark}")
    if reg_no:
        parts.append(f"$b{reg_no}")
    if copy_symbol:
        parts.append(f"$c{copy_symbol}")

    return "".join(parts)


def build_field_049(reg_mark: str = "", reg_no: str = "", copy_symbol: str = "") -> Optional[Field]:
    """
    Field ê°ì²´ ë²„ì „. build_049()ì™€ íŒë‹¨ ë¡œì§ ì™„ì „íˆ ë™ì¼.
    """
    line = build_049(reg_mark, reg_no, copy_symbol)
    if not line:
        return None
    return mrk_str_to_field(line)

# C14. 300 í˜•íƒœì‚¬í•­ ë¸”ë¡
# ============================================================
# C-14) 300 í˜•íƒœì‚¬í•­ (ì›ë³¸ íŒë‹¨ ë¡œì§ 100% ìœ ì§€ + êµ¬ì¡° ì•ˆì •í™”)
# ============================================================

# ---------------------------
# ì‚½í™” ê°ì§€ê¸° (ì›ë³¸ ì½”ë“œ ìœ ì§€)
# ---------------------------
def detect_illustrations(text: str):
    if not text:
        return False, None

    keyword_groups = {
        "ì²œì—°ìƒ‰ì‚½í™”": ["ì‚½í™”", "ì¼ëŸ¬ìŠ¤íŠ¸", "ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜", "illustration", "ê·¸ë¦¼"],
        "ì‚½í™”": ["í‘ë°± ì‚½í™”", "í‘ë°± ì¼ëŸ¬ìŠ¤íŠ¸", "í‘ë°± ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜", "í‘ë°± ê·¸ë¦¼"],
        "ì‚¬ì§„": ["ì‚¬ì§„", "í¬í† ", "photo", "í™”ë³´"],
        "ë„í‘œ": ["ë„í‘œ", "ì°¨íŠ¸", "ê·¸ë˜í”„"],
        "ì§€ë„": ["ì§€ë„", "ì§€ë„ì±…"],
    }

    found = set()
    for label, keywords in keyword_groups.items():
        if any(kw in text for kw in keywords):
            found.add(label)

    if found:
        return True, ", ".join(sorted(found))
    return False, None


# ----------------------------------------------------------
# ì•Œë¼ë”˜ HTML ìƒì„¸ í˜ì´ì§€ ë‚´ë¶€ì—ì„œ 300 ë°ì´í„° íŒŒì‹±í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
# ----------------------------------------------------------
def parse_aladin_physical_book_info(html: str):
    soup = BeautifulSoup(html, "html.parser")

    # -----------------------------
    # ì œëª© / ë¶€ì œ / ì±…ì†Œê°œ
    # -----------------------------
    title = soup.select_one("span.Ere_bo_title")
    subtitle = soup.select_one("span.Ere_sub1_title")
    title_text = title.get_text(strip=True) if title else ""
    subtitle_text = subtitle.get_text(strip=True) if subtitle else ""

    description = None
    desc_tag = soup.select_one("div.Ere_prod_mconts_R")
    if desc_tag:
        description = desc_tag.get_text(" ", strip=True)

    # -----------------------------
    # í˜•íƒœì‚¬í•­ ì˜ì—­ íŒŒì‹± (300 a,b,c)
    # -----------------------------
    form_wrap = soup.select_one("div.conts_info_list1")
    a_part = ""
    b_part = ""
    c_part = ""
    page_value = None
    size_value = None

    if form_wrap:
        items = [x.strip() for x in form_wrap.stripped_strings if x.strip()]
        for item in items:

            # ìª½ìˆ˜ / page
            if re.search(r"(ìª½|p)\s*$", item):
                m = re.search(r"\d+", item)
                if m:
                    page_value = int(m.group())
                    a_part = f"{m.group()} p."

            # íŒí˜•
            elif "mm" in item:
                m = re.search(r"(\d+)\s*[\*xÃ—X]\s*(\d+)", item)
                if m:
                    w = int(m.group(1))
                    h = int(m.group(2))
                    size_value = f"{w}x{h}mm"

                    # cm ë³€í™˜ ê·œì¹™(ì›ë³¸ ìœ ì§€)
                    if w == h or w > h or w < h/2:
                        w_cm = math.ceil(w / 10)
                        h_cm = math.ceil(h / 10)
                        c_part = f"{w_cm}x{h_cm} cm"
                    else:
                        h_cm = math.ceil(h / 10)
                        c_part = f"{h_cm} cm"

    # -----------------------------
    # ì‚½í™”(b) ê°ì§€ (ì›ë³¸ ê·œì¹™ ê·¸ëŒ€ë¡œ)
    # -----------------------------
    combined = " ".join([title_text, subtitle_text, description or ""])
    has_illus, illus_text = detect_illustrations(combined)
    if has_illus:
        b_part = illus_text

    # -----------------------------
    # Subfields + MRK ë¬¸ìì—´ ì¡°ë¦½
    # -----------------------------
    subfields_300 = []
    mrk_parts = []

    if a_part:
        chunk = f"$a{a_part}"
        if b_part:
            chunk += f" :$b{b_part}"
        mrk_parts.append(chunk)
        subfields_300.append(Subfield("a", a_part))
        if b_part:
            subfields_300.append(Subfield("b", b_part))

    elif b_part:
        mrk_parts.append(f"$b{b_part}")
        subfields_300.append(Subfield("b", b_part))

    if c_part:
        if mrk_parts:
            mrk_parts.append(f"; $c {c_part}")
        else:
            mrk_parts.append(f"$c {c_part}")
        subfields_300.append(Subfield("c", c_part))

    # fallback
    if not mrk_parts:
        mrk_parts = ["$a1ì±…."]
        subfields_300 = [Subfield("a", "1ì±….")]

    tag_300 = "=300  \\\\" + " ".join(mrk_parts)

    return {
        "300": tag_300,
        "300_subfields": subfields_300,
        "page_value": page_value,
        "size_value": size_value,
        "illustration_possibility": illus_text or "ì—†ìŒ",
    }


# ----------------------------------------------------------
# ì•Œë¼ë”˜ ë§í¬ë¥¼ ì‹¤ì œ ìš”ì²­í•˜ì—¬ 300 ë§Œë“¤ê¸° (ì›ë³¸ ë¡œì§ ë™ì¼)
# ----------------------------------------------------------
def search_aladin_detail_page(link: str):
    try:
        res = requests.get(link, timeout=15)
        res.raise_for_status()
        result = parse_aladin_physical_book_info(res.text)
        return result, None
    except Exception as e:
        return {
            "300": "=300  \\\\$a1ì±…. [ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜]",
            "300_subfields": [Subfield("a", "1ì±… [íŒŒì‹± ì‹¤íŒ¨]")],
            "page_value": None,
            "size_value": None,
            "illustration_possibility": "ì •ë³´ ì—†ìŒ",
        }, f"Aladin ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì™¸: {e}"


# ----------------------------------------------------------
# ìµœì¢… 300 ìƒì„±ê¸° (MRK + Field) â€” ì›ë³¸ 100% ìœ ì§€
# ----------------------------------------------------------
def build_300_from_aladin_detail(item: dict) -> tuple[str, Field]:
    try:
        link = (item or {}).get("link", "")
        if not link:
            fallback = "=300  \\\\$a1ì±…."
            return fallback, Field(
                tag="300",
                indicators=[" ", " "],
                subfields=[Subfield("a", "1ì±….")]
            )

        # HTML â†’ êµ¬ì¡°í™” ë°ì´í„°
        result, err = search_aladin_detail_page(link)

        # MRK ë¬¸ìì—´
        tag_300 = result.get("300") or "=300  \\\\$a1ì±…."

        # Field ê°ì²´
        subfields = result.get("300_subfields") or [Subfield("a", "1ì±….")]
        f_300 = Field(tag="300", indicators=[" ", " "], subfields=subfields)

        return tag_300, f_300

    except Exception:
        fallback = "=300  \\\\$a1ì±…. [ì˜ˆì™¸]"
        return fallback, Field(
            tag="300",
            indicators=[" ", " "],
            subfields=[Subfield("a", "1ì±…. [ì˜ˆì™¸]")]
        )


# ----------------------------------------------------------
# MRK ì „ìš© ë‹¨ì¶• í•¨ìˆ˜
# ----------------------------------------------------------
def build_300_mrk(item: dict) -> str:
    tag_300, _ = build_300_from_aladin_detail(item)
    return tag_300

# C15: generate_all_oneclick() ì „ì²´ íŒŒì´í”„ë¼ì¸
# ============================================================
# C-15) generate_all_oneclick â€” True Patch
# ì›ë³¸ íŒë‹¨ ë¡œì§ 100% ìœ ì§€ + êµ¬ì¡° ì•ˆì •í™”
# ============================================================

def generate_all_oneclick(
    isbn: str,
    *,
    reg_mark: str = "",
    reg_no: str = "",
    copy_symbol: str = "",
    use_ai_940: bool = True,
):
    """
    ì›ë³¸ generate_all_oneclick()ì˜ ëª¨ë“  íŒë‹¨ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ë©´ì„œ
    ëª¨ë“ˆí™”ëœ field_builders ì˜ ê¸°ëŠ¥ì„ í˜¸ì¶œí•˜ëŠ” True Patch ë²„ì „.
    """

    # ----------------------------------------------------------
    # ì´ˆê¸°í™”
    # ----------------------------------------------------------
    marc_rec = Record(to_unicode=True, force_utf8=True)
    pieces = []              # (Field, MRK str)
    meta = {}
    global CURRENT_DEBUG_LINES
    CURRENT_DEBUG_LINES = []

    # ----------------------------------------------------------
    # 0) NLK author-only ì¡°íšŒ
    # ----------------------------------------------------------
    author_raw, _ = fetch_nlk_author_only(isbn)

    # ----------------------------------------------------------
    # 1) ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„° (API â†’ ì‹¤íŒ¨ ì‹œ Web)
    # ----------------------------------------------------------
    item = fetch_aladin_item(isbn)    # ë„¤ ì›ë³¸ wrapper í•¨ìˆ˜(ìˆë˜ ê·¸ëŒ€ë¡œ)

    # ----------------------------------------------------------
    # 2) 041/546 ìƒì„±
    # ----------------------------------------------------------
    tag_041_text = None
    tag_546_text = None
    original_title = None

    try:
        res = get_kormarc_tags(isbn)   # (041, 546, original)
        if isinstance(res, (list, tuple)) and len(res) == 3:
            tag_041_text, tag_546_text, original_title = res

        # ì˜ˆì™¸ ë¬¸ìì—´ ì²˜ë¦¬
        if isinstance(tag_041_text, str) and tag_041_text.startswith("ğŸ“•"):
            tag_041_text = None
        if isinstance(tag_546_text, str) and tag_546_text.startswith("ğŸ“•"):
            tag_546_text = None
    except Exception:
        tag_041_text = None
        tag_546_text = None

    # 041 ì›ì‘ ì–¸ì–´ì½”ë“œ
    origin_lang = None
    if tag_041_text:
        m = re.search(r"\$h([a-z]{3})", tag_041_text, re.I)
        if m:
            origin_lang = m.group(1).lower()

    # ----------------------------------------------------------
    # 3) 245 / 246 / 700
    # ----------------------------------------------------------
    marc245 = build_245_with_people_from_sources(item, author_raw, prefer="aladin")
    f_245 = mrk_str_to_field(marc245)

    marc246 = build_246_from_aladin_item(item)
    f_246 = mrk_str_to_field(marc246)

    mrk_700_list = build_700_people_pref_aladin(
        author_raw,
        item,
        origin_lang_code=origin_lang
    ) or []

    # ----------------------------------------------------------
    # 4) 90010 (Wikidata ì›ì–´ëª…)
    # ----------------------------------------------------------
    people = extract_people_from_aladin(item) if item else {}
    mrk_90010_list = build_90010_from_wikidata(people, include_translator=False)

    # ----------------------------------------------------------
    # 5) 940 (245 $a ê¸°ë°˜)
    # ----------------------------------------------------------
    a_out, n_flag = parse_245_a_n(marc245)
    mrk_940_list = build_940_from_title_a(
        a_out,
        use_ai=use_ai_940,
        disable_number_reading=bool(n_flag)
    )

    # ----------------------------------------------------------
    # 6) 260 (ë°œí–‰ì§€ + êµ­ê°€ì½”ë“œ)
    # ----------------------------------------------------------
    publisher_raw = (item or {}).get("publisher", "")
    pubdate = (item or {}).get("pubDate", "") or ""
    pubyear = pubdate[:4] if len(pubdate) >= 4 else ""

    bundle = build_pub_location_bundle(isbn, publisher_raw)

    tag_260 = build_260(
        place_display=bundle["place_display"],
        publisher_name=publisher_raw,
        pubyear=pubyear,
    )
    f_260 = mrk_str_to_field(tag_260)

    # ----------------------------------------------------------
    # 7) 008 (country override + lang override)
    # ----------------------------------------------------------
    lang3_override = _lang3_from_tag041(tag_041_text) if tag_041_text else None

    data_008 = build_008_from_isbn(
        isbn,
        aladin_pubdate=(item or {}).get("pubDate", "") or "",
        aladin_title=(item or {}).get("title", "") or "",
        aladin_category=(item or {}).get("categoryName", "") or "",
        aladin_desc=(item or {}).get("description", "") or "",
        aladin_toc=((item or {}).get("subInfo", {}) or {}).get("toc", "") or "",
        override_country3=bundle["country_code"],
        override_lang3=lang3_override,
        cataloging_src="a",
    )
    f_008 = Field(tag="008", data=data_008)

    # ----------------------------------------------------------
    # 8) 007
    # ----------------------------------------------------------
    f_007 = Field(tag="007", data="ta")

    # ----------------------------------------------------------
    # 9) 020 + set ISBN(020-1)
    # ----------------------------------------------------------
    tag_020 = _build_020_from_item_and_nlk(isbn, item)
    f_020 = mrk_str_to_field(tag_020)

    nlk_extra = fetch_additional_code_from_nlk(isbn)
    set_isbn = nlk_extra.get("set_isbn", "").strip()

    # ----------------------------------------------------------
    # 10) 653 (GPT)
    # ----------------------------------------------------------
    tag_653 = _build_653_via_gpt(item)
    f_653 = mrk_str_to_field(tag_653) if tag_653 else None

    # 653 â†’ 056 íŒíŠ¸ë¡œ ì‚¬ìš©
    try:
        kw_hint = sorted(set(_parse_653_keywords(tag_653)))[:7]
    except Exception:
        kw_hint = []

    # ----------------------------------------------------------
    # 11) 056 (KDC)
    # ----------------------------------------------------------
    try:
        kdc_code = get_kdc_from_isbn(
            isbn,
            ttbkey=ALADIN_TTB_KEY,
            openai_key=openai_key,
            model=model,
            keywords_hint=kw_hint,
        )
        if kdc_code and not re.fullmatch(r"\d{1,3}", kdc_code):
            kdc_code = None
    except Exception:
        kdc_code = None

    tag_056 = f"=056  \\\\$a{kdc_code}$26" if kdc_code else None
    f_056 = mrk_str_to_field(tag_056) if tag_056 else None

    # ----------------------------------------------------------
    # 12) 490 / 830
    # ----------------------------------------------------------
    tag_490, tag_830 = build_490_830_mrk_from_item(item)
    f_490 = mrk_str_to_field(tag_490)
    f_830 = mrk_str_to_field(tag_830)

    # ----------------------------------------------------------
    # 13) 300 (í˜•íƒœì‚¬í•­)
    # ----------------------------------------------------------
    tag_300, f_300 = build_300_from_aladin_detail(item)

    # ----------------------------------------------------------
    # 14) 950 (ê°€ê²©)
    # ----------------------------------------------------------
    tag_950 = build_950_from_item_and_price(item, isbn)
    f_950 = mrk_str_to_field(tag_950)

    # ----------------------------------------------------------
    # 15) 049 (ë“±ë¡ë²ˆí˜¸)
    # ----------------------------------------------------------
    tag_049 = build_049(reg_mark, reg_no, copy_symbol)
    f_049 = mrk_str_to_field(tag_049) if tag_049 else None

    # ----------------------------------------------------------
    # 16) í•„ë“œ ìˆœì„œëŒ€ë¡œ ì¡°ë¦½
    # ----------------------------------------------------------
    def add_piece(field, mrk):
        if field and mrk:
            pieces.append((field, mrk))

    add_piece(f_008, f"=008  {data_008}")
    add_piece(f_007, "=007  ta")
    add_piece(f_020, tag_020)

    if set_isbn:
        tag_020_1 = f"=020  1\\$a{set_isbn} (set)"
        add_piece(mrk_str_to_field(tag_020_1), tag_020_1)

    if tag_041_text and "$h" in tag_041_text:
        f_041 = mrk_str_to_field(_as_mrk_041(tag_041_text))
        add_piece(f_041, _as_mrk_041(tag_041_text))

    add_piece(f_056, tag_056)
    add_piece(f_245, marc245)
    add_piece(f_246, marc246)
    add_piece(f_260, tag_260)
    add_piece(f_300, tag_300)
    add_piece(f_490, tag_490)

    if tag_546_text:
        f_546 = mrk_str_to_field(_as_mrk_546(tag_546_text))
        add_piece(f_546, _as_mrk_546(tag_546_text))

    add_piece(f_653, tag_653)

    for m in mrk_700_list:
        ff = mrk_str_to_field(m)
        if ff:
            add_piece(ff, m)

    for m in mrk_90010_list:
        ff = mrk_str_to_field(m)
        if ff:
            add_piece(ff, m)

    for m in mrk_940_list:
        ff = mrk_str_to_field(m)
        if ff:
            add_piece(ff, m)

    add_piece(f_830, tag_830)
    add_piece(f_950, tag_950)
    add_piece(f_049, tag_049)

    # ----------------------------------------------------------
    # MRK ì „ì²´ ë¬¸ìì—´
    # ----------------------------------------------------------
    mrk_text = "\n".join(m for _, m in pieces)

    # ----------------------------------------------------------
    # Record ê°ì²´ êµ¬ì„±
    # ----------------------------------------------------------
    for f, _ in pieces:
        marc_rec.add_field(f)

    # ----------------------------------------------------------
    # meta êµ¬ì„±
    # ----------------------------------------------------------
    meta = {
        "TitleA": a_out,
        "has_n": bool(n_flag),
        "700_count": sum(1 for _, m in pieces if m.startswith("=700")),
        "90010_count": len(mrk_90010_list),
        "940_count": len(mrk_940_list),
        "Candidates": get_candidate_names_for_isbn(isbn),
        "041": tag_041_text,
        "546": tag_546_text,
        "020": tag_020,
        "056": tag_056,
        "653": tag_653,
        "kdc_code": kdc_code,
        "price_for_950": _extract_price_kr(item, isbn),
        "Publisher_raw": publisher_raw,
        "pubyear": pubyear,
        "Place_display": bundle["place_display"],
        "CountryCode_008": bundle["country_code"],
        "Publisher_resolved": bundle["resolved_publisher"],
        "Bundle_source": bundle["source"],
        "debug_lines": list(CURRENT_DEBUG_LINES),
    }

    marc_bytes = marc_rec.as_marc()

    return marc_rec, marc_bytes, mrk_text, meta
